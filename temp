async def process_batch(
    self, 
    texts: List[str], 
    prompt_template: str,
    json_schema: Optional[Dict[str, Any]] = None,
    batch_id: Optional[str] = None,
    custom_ids: Optional[List[str]] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    Processamento com asyncio.wait() para controle correto de tasks
    
    Args:
        texts: Lista de textos para processar
        prompt_template: Template do prompt
        json_schema: Schema JSON opcional para resposta estruturada
        batch_id: ID personalizado para o batch
        custom_ids: Lista de IDs personalizados para cada texto (opcional)
        **kwargs: Argumentos adicionais para o template
    
    Returns:
        Dicion√°rio com resultados, estat√≠sticas e batch_id
    """
    
    # Usar batch_id customizado ou gerar automaticamente
    batch_id = batch_id if batch_id else f"batch_{int(time.time())}"
    
    # ‚úÖ CORRE√á√ÉO: Valida√ß√£o robusta de custom_ids
    if custom_ids is not None:
        # Converter para lista se for pandas Series ou similar
        if hasattr(custom_ids, 'tolist'):
            custom_ids = custom_ids.tolist()
        elif hasattr(custom_ids, 'values'):
            custom_ids = list(custom_ids.values)
        elif not isinstance(custom_ids, list):
            custom_ids = list(custom_ids)
        
        # Validar tamanho ap√≥s convers√£o
        if len(custom_ids) != len(texts):
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(custom_ids)} != {len(texts)}")
        
        # Garantir que todos os IDs s√£o strings
        custom_ids = [str(cid) if cid is not None else None for cid in custom_ids]
    
    # Controle de estado simplificado
    total_successful = 0
    logged_progress = set()
    progress_lock = asyncio.Lock()
    
    # Inicializar rate limiter para o batch
    self.rate_limiter.start_batch(batch_id)
    
    # Iniciar tracking do batch no StatsManager
    self.stats_manager.start_batch(batch_id)
    
    # Log de in√≠cio
    logger.info(
        f"üöÄ Iniciando processamento em lote - {len(texts)} textos",
        extra={
            'batch_id': batch_id,
            'total_texts': len(texts),
            'has_custom_ids': custom_ids is not None,
            'has_json_schema': json_schema is not None,
            'max_concurrent': self.max_concurrent,
            'action': 'batch_start'
        }
    )
    
    # Fun√ß√£o para log de progresso
    async def log_progress(current_successful: int):
        async with progress_lock:
            progress_intervals = self._calculate_progress_intervals(len(texts))
            
            if current_successful in progress_intervals and current_successful not in logged_progress:
                logged_progress.add(current_successful)
                
                elapsed = time.time() - start_time
                rate = current_successful / elapsed if elapsed > 0 else 0
                eta = (len(texts) - current_successful) / rate if rate > 0 else 0
                
                logger.info(
                    f"üìä Progresso: {current_successful}/{len(texts)} ({current_successful/len(texts)*100:.1f}%) | "
                    f"‚úÖ{current_successful} ‚ùå0 | "
                    f"‚è±Ô∏èETA: {eta/60:.1f}min",
                    extra={
                        'batch_id': batch_id,
                        'completed': current_successful,
                        'total': len(texts),
                        'successful_so_far': current_successful,
                        'failed_so_far': 0,
                        'processing_rate': round(rate, 2),
                        'eta_minutes': round(eta / 60, 1),
                        'progress_percent': round((current_successful / len(texts)) * 100, 1),
                        'action': 'batch_progress'
                    }
                )
    
    # Usar asyncio.wait() com FIRST_COMPLETED para controle correto
    all_results = [None] * len(texts)
    pending_items = list(enumerate(texts))
    start_time = time.time()
    
    while pending_items:
        # Criar tasks com mapeamento correto
        task_to_index = {}
        current_tasks = []
        
        for i, text in pending_items:
            # ‚úÖ CORRE√á√ÉO: Tratamento seguro de custom_id
            custom_id = None
            if custom_ids is not None and i < len(custom_ids):
                custom_id = custom_ids[i]
            
            task = asyncio.create_task(
                self.process_single(text, prompt_template, json_schema, custom_id, **kwargs)
            )
            current_tasks.append(task)
            task_to_index[task] = i
        
        # Usar asyncio.wait() com FIRST_COMPLETED para logs incrementais
        rate_limit_indices = []
        rate_limit_detected = False
        
        while current_tasks:
            # Aguardar pelo menos uma task completar
            done, pending = await asyncio.wait(
                current_tasks, 
                return_when=asyncio.FIRST_COMPLETED
            )
            
            # Processar tasks completadas
            for completed_task in done:
                try:
                    result = await completed_task
                    original_index = task_to_index[completed_task]
                    
                    # Verificar rate limit
                    if not result.get('success', False) and is_rate_limit_error(result):
                        rate_limit_indices.append(original_index)
                        
                        # Registrar rate limit apenas uma vez
                        if not rate_limit_detected:
                            rate_limit_detected = True
                            wait_time = self._extract_wait_time_from_error_result(result)
                            
                            # Registrar tempo de pausa coordenada no stats
                            self.stats_manager.record_global_rate_limit_activation(wait_time, len(rate_limit_indices))
                            
                            # Delegar para rate limiter
                            self.rate_limiter.record_api_rate_limit_with_context(
                                wait_time, total_successful, len(texts), total_successful, 0
                            )
                    else:
                        # Resultado definitivo
                        all_results[original_index] = result
                        if result.get('success', False):
                            total_successful += 1
                            
                            # Log de progresso incremental
                            await log_progress(total_successful)
                        
                        # Notificar rate limiter sobre sucesso
                        if result.get('success', False):
                            self.rate_limiter.record_successful_request()
                    
                except Exception as e:
                    logger.error(
                        f"Erro em task do lote: {e}",
                        extra={
                            'batch_id': batch_id,
                            'error': str(e),
                            'error_type': type(e).__name__,
                            'action': 'batch_task_error'
                        }
                    )
                    original_index = task_to_index.get(completed_task, 0)
                    all_results[original_index] = {
                        'content': None,
                        'success': False,
                        'error': str(e),
                        'error_type': type(e).__name__
                    }
            
            # Atualizar lista de tasks pendentes
            current_tasks = list(pending)
        
        # Preparar pr√≥xima rodada
        if rate_limit_indices:
            # Filtrar apenas rate limits para reprocessamento
            pending_items = [(i, texts[i]) for i in rate_limit_indices]
            
            # Aguardar brevemente
            await asyncio.sleep(0.1)
        else:
            # N√£o h√° mais rate limits, finalizar
            pending_items = []
    
    # Filtrar resultados v√°lidos
    final_results = [r for r in all_results if r is not None]
    
    # Finalizar batch
    batch_stats = self.stats_manager.end_batch(batch_id)
    
    # Log de fim
    logger.info(
        f"‚úÖ Batch conclu√≠do - {batch_stats.successful_requests} sucessos, "
        f"{batch_stats.failed_requests} falhas em {batch_stats.processing_time:.1f}s",
        extra={
            'batch_id': batch_id,
            'total_requests': batch_stats.total_requests,
            'successful_requests': batch_stats.successful_requests,
            'failed_requests': batch_stats.failed_requests,
            'processing_time': round(batch_stats.processing_time, 2),
            'total_tokens': batch_stats.total_tokens,
            'total_cost': round(batch_stats.total_cost, 4),
            'avg_rate': round(batch_stats.avg_rate, 2),
            'success_rate': round(batch_stats.success_rate, 1),
            'action': 'batch_complete'
        }
    )
    
    return {
        'results': final_results,
        'batch_stats': batch_stats,
        'batch_id': batch_id
    }
