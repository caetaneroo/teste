# core/ai_processor.py
import asyncio
import time
import json
import logging
import re
import traceback
import sys
import tiktoken
from typing import List, Dict, Any, Optional
from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI

# Importar m√≥dulos locais
from rate_limiter import AdaptiveRateLimiter
from stats_manager import StatsManager

# Logger espec√≠fico do m√≥dulo
logger = logging.getLogger(__name__)

# --------------------------------------------------------------------------- #
# Constantes de configura√ß√£o                                                  #
# --------------------------------------------------------------------------- #
MAX_TPM = 180000  # M√°ximo de tokens por minuto
MAX_RETRY = 2     # M√°ximo de tentativas de retry

# --------------------------------------------------------------------------- #
# Fun√ß√£o global para detec√ß√£o de rate limit                                  #
# --------------------------------------------------------------------------- #

def is_rate_limit_error(result: Dict[str, Any]) -> bool:
    """
    Verifica se o resultado indica rate limit
    
    Args:
        result: Dicion√°rio de resultado da API
        
    Returns:
        True se for rate limit, False caso contr√°rio
    """
    if not isinstance(result, dict):
        return False
    
    error_msg = result.get('error', '').lower()
    return 'token rate limit' in error_msg or 'rate limit' in error_msg


class AIProcessor:
    """
    Processador de IA com AsyncIaraGenAI, rate limiting adaptativo e logs organizados
    
    Caracter√≠sticas:
    - Usa AsyncIaraGenAI da biblioteca iaragenai
    - Rate limiting coordenado com logs organizados por responsabilidade
    - Processamento paralelo com controle de concorr√™ncia
    - Calibra√ß√£o adaptativa de estimativas de tokens
    - M√©tricas detalhadas de performance
    - Suporte a JSON Schema estruturado
    - Retry apenas para rate limits (token rate limit)
    """
    
    def __init__(self, config: Dict[str, Any]):
        # Configura√ß√µes de autentica√ß√£o
        self.client_id = config['client_id']
        self.client_secret = config['client_secret']
        self.environment = config.get('environment', 'dev')
        self.provider = config.get('provider', 'azure-openai')
        self.correlation_id = config.get('correlation_id', 'teste-ia-biatendimento')
        
        # Configura√ß√µes do modelo
        self.model = config.get('model', 'gpt-35-turbo-16k')
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens')
        
        # ‚úÖ CLIENTE AsyncIaraGenAI
        self.client = AsyncIaraGenAI(
            client_id=self.client_id,
            client_secret=self.client_secret,
            environment=self.environment,
            provider=self.provider,
            correlation_id=self.correlation_id
        )
        
        # Controle de concorr√™ncia
        self.max_concurrent = config.get('max_concurrent', 10)
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        
        # Rate limiting adaptativo
        max_tpm = config.get('max_tpm', MAX_TPM)
        calibration_enabled = config.get('adaptive_calibration', True)
        self.rate_limiter = AdaptiveRateLimiter(max_tpm, calibration_enabled)
        
        # Gerenciador de estat√≠sticas
        self.stats_manager = StatsManager()
        
        # Encoder para estimativa de tokens
        try:
            self.encoder = tiktoken.encoding_for_model(self.model)
        except KeyError:
            # Fallback para modelos n√£o reconhecidos
            self.encoder = tiktoken.get_encoding("cl100k_base")
        
        logger.info(
            "AIProcessor inicializado com AsyncIaraGenAI",
            extra={
                'model': self.model,
                'max_concurrent': self.max_concurrent,
                'max_tpm': max_tpm,
                'calibration_enabled': calibration_enabled,
                'provider': self.provider,
                'environment': self.environment,
                'action': 'ai_processor_init'
            }
        )
    
    def _generate_request_id(self) -> str:
        """Gera ID √∫nico para requisi√ß√£o"""
        return f"req_{int(time.time() * 1000)}_{id(self)}"
    
    def estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        """
        Estima tokens necess√°rios para as mensagens usando tiktoken
        """
        try:
            total_tokens = 0
            for message in messages:
                # Formato: role + content + overhead
                role_tokens = len(self.encoder.encode(message.get('role', '')))
                content_tokens = len(self.encoder.encode(message.get('content', '')))
                total_tokens += role_tokens + content_tokens + 4  # Overhead por mensagem
            
            # Overhead adicional para resposta
            total_tokens += 50
            
            return total_tokens
            
        except Exception as e:
            logger.warning(
                f"Erro na estimativa de tokens: {e}. Usando estimativa conservadora.",
                extra={'action': 'token_estimation_error'}
            )
            # Estimativa conservadora baseada em caracteres
            total_chars = sum(len(str(msg.get('content', ''))) for msg in messages)
            return int(total_chars * 0.3)  # ~3.3 chars por token
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """
        Calcula custo estimado baseado nos tokens
        """
        # Pre√ßos estimados para GPT-3.5 Turbo (USD por 1K tokens)
        input_cost_per_1k = 0.0015
        output_cost_per_1k = 0.002
        
        input_cost = (input_tokens / 1000) * input_cost_per_1k
        output_cost = (output_tokens / 1000) * output_cost_per_1k
        
        return input_cost + output_cost
    
    def _prepare_json_schema(self, json_schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        Prepara JSON schema para AsyncIaraGenAI
        """
        if not isinstance(json_schema, dict):
            return None
        
        return {
            "type": "json_schema",
            "json_schema": {
                "name": json_schema.get("name", "response_schema"),
                "description": json_schema.get("description", "Schema for structured response"),
                "schema": json_schema.get("schema", json_schema),
                "strict": json_schema.get("strict", True)
            }
        }
    
    def _extract_wait_time_from_error_result(self, result: Dict[str, Any]) -> float:
        """
        Extrai wait time do resultado de erro
        """
        # Verificar se h√° informa√ß√µes de retry no resultado
        if 'retry_after' in result:
            try:
                return float(result['retry_after'])
            except (ValueError, TypeError):
                pass
        
        # Extrair da mensagem de erro
        error_msg = result.get('error', '')
        
        # Padr√µes espec√≠ficos do Azure OpenAI
        patterns = [
            r'retry after (\d+) seconds',
            r'wait (\d+) seconds', 
            r'retry.*?(\d+)\s*seconds?',
            r'(\d+)s',
            r'retry.*?(\d+)',
            r'wait.*?(\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, error_msg, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except (ValueError, IndexError):
                    continue
        
        # Headers de resposta (se dispon√≠veis no resultado)
        if 'response_headers' in result:
            headers = result['response_headers']
            if 'retry-after' in headers:
                try:
                    return float(headers['retry-after'])
                except (ValueError, TypeError):
                    pass
        
        # Default para Azure OpenAI
        return 60.0
    
    def _validate_api_params(self, api_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida par√¢metros da API antes do envio
        """
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': []
        }
        
        # Validar modelo
        if not api_params.get('model'):
            validation_result['valid'] = False
            validation_result['errors'].append('Modelo n√£o especificado')
        
        # Validar mensagens
        messages = api_params.get('messages', [])
        if not messages:
            validation_result['valid'] = False
            validation_result['errors'].append('Mensagens vazias')
        
        for i, msg in enumerate(messages):
            if not isinstance(msg, dict):
                validation_result['valid'] = False
                validation_result['errors'].append(f'Mensagem {i} n√£o √© um dicion√°rio')
                continue
                
            if 'role' not in msg or 'content' not in msg:
                validation_result['valid'] = False
                validation_result['errors'].append(f'Mensagem {i} sem role ou content')
        
        # Validar temperatura
        temp = api_params.get('temperature', 0.1)
        if not isinstance(temp, (int, float)) or temp < 0 or temp > 2:
            validation_result['warnings'].append(f'Temperatura {temp} pode ser inv√°lida (deve ser 0-2)')
        
        # Validar max_tokens
        max_tokens = api_params.get('max_tokens')
        if max_tokens is not None and (not isinstance(max_tokens, int) or max_tokens <= 0):
            validation_result['warnings'].append(f'max_tokens {max_tokens} pode ser inv√°lido')
        
        return validation_result
    
    def _get_detailed_error_context(self, e: Exception, api_params: Dict[str, Any], request_id: str) -> Dict[str, Any]:
        """
        Obt√©m contexto detalhado do erro para debugging
        """
        # Obter traceback com linha espec√≠fica
        exc_type, exc_value, exc_traceback = sys.exc_info()
        
        # Extrair linha espec√≠fica do erro
        error_line = None
        error_file = None
        error_function = None
        
        if exc_traceback:
            tb = exc_traceback
            # Navegar at√© o √∫ltimo frame do traceback
            while tb.tb_next:
                tb = tb.tb_next
            error_line = tb.tb_lineno
            error_file = tb.tb_frame.f_code.co_filename
            error_function = tb.tb_frame.f_code.co_name
        
        # Contexto completo
        context = {
            'error_type': type(e).__name__,
            'error_message': str(e),
            'error_line': error_line,
            'error_file': error_file.split('/')[-1] if error_file else None,
            'error_function': error_function,
            'request_id': request_id,
            'api_params_summary': {
                'model': api_params.get('model'),
                'messages_count': len(api_params.get('messages', [])),
                'temperature': api_params.get('temperature'),
                'max_tokens': api_params.get('max_tokens'),
                'has_response_format': 'response_format' in api_params
            },
            'full_traceback': traceback.format_exc()
        }
        
        # Adicionar informa√ß√µes espec√≠ficas do erro se dispon√≠vel
        if hasattr(e, 'response'):
            context['response_status'] = getattr(e.response, 'status_code', None)
            context['response_headers'] = dict(getattr(e.response, 'headers', {}))
            
        return context
    
    async def _make_api_call(
        self,
        messages: List[Dict[str, str]],
        json_schema: Optional[Dict[str, Any]] = None,
        request_id: str = None
    ) -> Dict[str, Any]:
        """
        Faz chamada √† API com AsyncIaraGenAI e rate limiting coordenado
        """
        
        if not request_id:
            request_id = self._generate_request_id()
        
        # Validar entrada
        if not messages or not isinstance(messages, list):
            return {
                'content': None,
                'tokens_used': 0,
                'success': False,
                'error': 'Messages inv√°lidas',
                'error_type': 'ValidationError',
                'request_id': request_id
            }
        
        # Estimar tokens necess√°rios
        base_estimate = self.estimate_tokens(messages)
        
        # Rate limiting coordenado inicial
        calibrated_estimate = await self.rate_limiter.wait_for_tokens(base_estimate)
        
        # Registrar in√≠cio de requisi√ß√£o concorrente
        self.stats_manager.record_concurrent_start()
        
        try:
            async with self.semaphore:
                # AsyncRetrying com retry apenas para rate limits
                async for attempt in AsyncRetrying(
                    stop=stop_after_attempt(MAX_RETRY),
                    wait=wait_fixed(0.1),
                    reraise=True
                ):
                    with attempt:
                        # Aguardar rate limiting em todas as tentativas
                        if attempt.retry_state.attempt_number > 1:
                            await self.rate_limiter.wait_for_tokens(base_estimate)
                        
                        start_time = time.time()
                        
                        try:
                            # ‚úÖ PREPARAR E VALIDAR PAR√ÇMETROS DA API
                            api_params = {
                                "model": self.model,
                                "messages": messages,
                                "temperature": self.temperature
                            }
                            
                            if self.max_tokens is not None:
                                api_params["max_tokens"] = self.max_tokens
                            
                            if json_schema and isinstance(json_schema, dict):
                                api_params["response_format"] = self._prepare_json_schema(json_schema)
                            
                            # ‚úÖ VALIDAR PAR√ÇMETROS ANTES DO ENVIO
                            validation = self._validate_api_params(api_params)
                            if not validation['valid']:
                                raise ValueError(f"Par√¢metros inv√°lidos: {', '.join(validation['errors'])}")
                            
                            # Log warnings se houver
                            if validation['warnings']:
                                logger.warning(
                                    f"Avisos nos par√¢metros da API: {', '.join(validation['warnings'])}",
                                    extra={
                                        'request_id': request_id,
                                        'warnings': validation['warnings'],
                                        'action': 'api_params_warning'
                                    }
                                )
                            
                            # Log da tentativa
                            logger.debug(
                                f"Tentativa {attempt.retry_state.attempt_number}/{MAX_RETRY} - Chamada √† API",
                                extra={
                                    'request_id': request_id,
                                    'attempt_number': attempt.retry_state.attempt_number,
                                    'base_estimate': base_estimate,
                                    'calibrated_estimate': calibrated_estimate,
                                    'api_params_summary': api_params.get('model', 'unknown'),
                                    'action': 'api_call_attempt'
                                }
                            )
                            
                            # ‚úÖ FAZER CHAMADA √Ä API COM TRATAMENTO ROBUSTO
                            try:
                                response = await self.client.chat.completions.create(**api_params)
                            except Exception as api_error:
                                # ‚úÖ CONTEXTO DETALHADO DO ERRO DA API
                                error_context = self._get_detailed_error_context(api_error, api_params, request_id)
                                
                                logger.error(
                                    f"Erro na chamada √† API - {error_context['error_file']}:{error_context['error_line']} em {error_context['error_function']}(): {error_context['error_message']}",
                                    extra={
                                        'request_id': request_id,
                                        'error_line': error_context['error_line'],
                                        'error_file': error_context['error_file'],
                                        'error_function': error_context['error_function'],
                                        'error_type': error_context['error_type'],
                                        'api_params_summary': error_context['api_params_summary'],
                                        'response_status': error_context.get('response_status'),
                                        'full_traceback': error_context['full_traceback'],
                                        'action': 'api_call_error'
                                    }
                                )
                                raise api_error
                            
                            # ‚úÖ VALIDA√á√ÉO ROBUSTA DA RESPOSTA
                            if not response:
                                raise ValueError("API retornou resposta None/vazia")
                                
                            if not hasattr(response, 'choices'):
                                raise ValueError(f"API retornou resposta sem atributo 'choices'. Tipo da resposta: {type(response)}")
                                
                            if not response.choices:
                                raise ValueError("API retornou choices vazio (lista vazia)")
                                
                            if len(response.choices) == 0:
                                raise ValueError("API retornou lista choices com 0 elementos")
                                
                            if not hasattr(response.choices[0], 'message'):
                                raise ValueError(f"API retornou choice[0] sem atributo 'message'. Tipo do choice: {type(response.choices[0])}")
                            
                            if not hasattr(response, 'usage'):
                                logger.warning(
                                    "API retornou resposta sem informa√ß√µes de usage - usando valores padr√£o",
                                    extra={
                                        'request_id': request_id,
                                        'action': 'missing_usage_info'
                                    }
                                )
                            
                            end_time = time.time()
                            api_response_time = end_time - start_time
                            
                            # ‚úÖ EXTRAIR DADOS COM VALIDA√á√ÉO
                            try:
                                if hasattr(response, 'usage') and response.usage:
                                    tokens_used = getattr(response.usage, 'total_tokens', 0)
                                    input_tokens = getattr(response.usage, 'prompt_tokens', 0)
                                    output_tokens = getattr(response.usage, 'completion_tokens', 0)
                                else:
                                    # Fallback quando usage n√£o est√° dispon√≠vel
                                    tokens_used = 0
                                    input_tokens = 0
                                    output_tokens = 0
                                    
                            except AttributeError as usage_error:
                                logger.warning(
                                    f"Erro ao extrair tokens da resposta: {usage_error} - usando valores padr√£o",
                                    extra={
                                        'request_id': request_id,
                                        'response_structure': str(type(response.usage)) if hasattr(response, 'usage') else 'N/A',
                                        'action': 'usage_extraction_error'
                                    }
                                )
                                # Valores padr√£o para continuar processamento
                                tokens_used = 0
                                input_tokens = 0
                                output_tokens = 0
                            
                            # Extrair cached tokens se dispon√≠vel
                            cached_tokens = 0
                            if hasattr(response, 'usage') and hasattr(response.usage, 'prompt_tokens_details'):
                                cached_tokens = getattr(response.usage.prompt_tokens_details, 'cached_tokens', 0)
                            
                            # Registrar tokens no rate limiter
                            if tokens_used > 0:
                                self.rate_limiter.record_tokens(tokens_used)
                                self.rate_limiter.record_token_usage(base_estimate, tokens_used)
                            
                            # Calcular custo
                            cost = self.calculate_cost(input_tokens, output_tokens)
                            
                            # ‚úÖ PROCESSAR RESPOSTA COM VALIDA√á√ÉO ROBUSTA
                            try:
                                if not response.choices or len(response.choices) == 0:
                                    raise IndexError("Lista choices est√° vazia")
                                    
                                choice = response.choices[0]
                                if not hasattr(choice, 'message'):
                                    raise AttributeError("Choice n√£o tem atributo 'message'")
                                    
                                message = choice.message
                                if not hasattr(message, 'content'):
                                    raise AttributeError("Message n√£o tem atributo 'content'")
                                    
                                content = message.content
                                
                            except (IndexError, AttributeError) as content_error:
                                error_context = self._get_detailed_error_context(content_error, api_params, request_id)
                                
                                logger.error(
                                    f"Erro ao extrair content da resposta - {error_context['error_file']}:{error_context['error_line']}: {content_error}",
                                    extra={
                                        'request_id': request_id,
                                        'error_line': error_context['error_line'],
                                        'error_file': error_context['error_file'],
                                        'error_function': error_context['error_function'],
                                        'choices_length': len(response.choices) if hasattr(response, 'choices') and response.choices else 'N/A',
                                        'choice_type': str(type(response.choices[0])) if hasattr(response, 'choices') and response.choices else 'N/A',
                                        'full_traceback': error_context['full_traceback'],
                                        'action': 'content_extraction_error'
                                    }
                                )
                                content = None
                            
                            parsed_content = content
                            
                            # Se JSON schema foi usado, tentar parsear
                            if json_schema and isinstance(json_schema, dict) and content:
                                try:
                                    parsed_content = json.loads(content)
                                    logger.debug(
                                        "JSON Schema parseado com sucesso",
                                        extra={
                                            'request_id': request_id,
                                            'action': 'json_parse_success'
                                        }
                                    )
                                except json.JSONDecodeError as e:
                                    logger.warning(
                                        "Erro ao parsear JSON Schema",
                                        extra={
                                            'request_id': request_id,
                                            'error': str(e),
                                            'raw_content': content[:200] if content else 'None',
                                            'action': 'json_parse_error'
                                        }
                                    )
                                    parsed_content = content
                            
                            # Log de sucesso
                            estimation_accuracy = abs(calibrated_estimate - tokens_used) / tokens_used * 100 if tokens_used > 0 else 0
                            
                            logger.debug(
                                "Chamada √† API bem-sucedida",
                                extra={
                                    'request_id': request_id,
                                    'attempt_number': attempt.retry_state.attempt_number,
                                    'tokens_used': tokens_used,
                                    'input_tokens': input_tokens,
                                    'output_tokens': output_tokens,
                                    'cached_tokens': cached_tokens,
                                    'base_estimate': base_estimate,
                                    'calibrated_estimate': calibrated_estimate,
                                    'estimation_accuracy': round(estimation_accuracy, 1),
                                    'api_response_time': round(api_response_time, 3),
                                    'cost': round(cost, 6),
                                    'action': 'api_call_success'
                                }
                            )
                            
                            # Retorno de sucesso
                            return {
                                'content': parsed_content,
                                'raw_content': content,
                                'tokens_used': tokens_used,
                                'input_tokens': input_tokens,
                                'output_tokens': output_tokens,
                                'cached_tokens': cached_tokens,
                                'cost': cost,
                                'api_response_time': api_response_time,
                                'success': True,
                                'is_json': json_schema is not None,
                                'attempts': attempt.retry_state.attempt_number,
                                'request_id': request_id
                            }
                            
                        except Exception as e:
                            end_time = time.time()
                            api_response_time = end_time - start_time
                            
                            # ‚úÖ CONTEXTO DETALHADO DO ERRO
                            error_context = self._get_detailed_error_context(e, api_params, request_id)
                            
                            # Capturar headers se dispon√≠veis
                            response_headers = {}
                            if hasattr(e, 'response') and hasattr(e.response, 'headers'):
                                response_headers = dict(e.response.headers)
                            
                            # Criar resultado de erro
                            error_result = {
                                'content': None,
                                'raw_content': None,
                                'tokens_used': 0,
                                'input_tokens': 0,
                                'output_tokens': 0,
                                'cached_tokens': 0,
                                'cost': 0.0,
                                'api_response_time': api_response_time,
                                'success': False,
                                'error': str(e),
                                'error_type': type(e).__name__,
                                'error_line': error_context['error_line'],
                                'error_file': error_context['error_file'],
                                'error_function': error_context['error_function'],
                                'response_headers': response_headers,
                                'is_json': False,
                                'attempts': attempt.retry_state.attempt_number,
                                'request_id': request_id,
                                'full_traceback': error_context['full_traceback']
                            }
                            
                            # ‚úÖ VERIFICAR se √© rate limit (√∫nica condi√ß√£o para retry)
                            if is_rate_limit_error(error_result):
                                wait_time = self._extract_wait_time_from_error_result(error_result)
                                
                                logger.debug(
                                    f"Rate limit detectado - request {request_id}",
                                    extra={
                                        'request_id': request_id,
                                        'attempt_number': attempt.retry_state.attempt_number,
                                        'wait_time': wait_time,
                                        'action': 'rate_limit_detected'
                                    }
                                )
                                
                                # Raise para que AsyncRetrying fa√ßa nova tentativa
                                raise
                            
                            else:
                                # Qualquer outro erro - n√£o fazer retry, apenas retornar
                                return error_result
                                
            # C√≥digo nunca deveria chegar aqui devido ao AsyncRetrying
            return {
                'content': None,
                'success': False,
                'error': 'Erro inesperado no fluxo de retry',
                'error_type': 'UnexpectedFlowError',
                'request_id': request_id
            }
                                
        except RetryError as retry_error:
            # Todas as tentativas falharam
            logger.error(
                "Todas as tentativas de retry falharam",
                extra={
                    'request_id': request_id,
                    'total_attempts': MAX_RETRY,
                    'final_error': str(retry_error.last_attempt.exception()) if retry_error.last_attempt else 'Desconhecido',
                    'action': 'retry_exhausted'
                }
            )
            
            # Retornar resultado final de falha
            return {
                'content': None,
                'raw_content': None,
                'tokens_used': 0,
                'input_tokens': 0,
                'output_tokens': 0,
                'cached_tokens': 0,
                'cost': 0.0,
                'api_response_time': 0.0,
                'success': False,
                'error': f'M√°ximo de tentativas excedido: {retry_error.last_attempt.exception() if retry_error.last_attempt else "Erro desconhecido"}',
                'error_type': 'RetryError',
                'is_json': False,
                'attempts': MAX_RETRY,
                'request_id': request_id
            }
            
        except Exception as e:
            # ‚úÖ ERRO INESPERADO COM CONTEXTO COMPLETO
            error_context = self._get_detailed_error_context(e, {}, request_id)
            
            logger.error(
                f"Erro inesperado na chamada √† API - {error_context['error_file']}:{error_context['error_line']} em {error_context['error_function']}(): {str(e)}",
                extra={
                    'request_id': request_id,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'error_line': error_context['error_line'],
                    'error_file': error_context['error_file'],
                    'error_function': error_context['error_function'],
                    'full_traceback': error_context['full_traceback'],
                    'action': 'unexpected_api_error'
                }
            )
            
            return {
                'content': None,
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'error_line': error_context['error_line'],
                'error_file': error_context['error_file'],
                'error_function': error_context['error_function'],
                'full_traceback': error_context['full_traceback'],
                'request_id': request_id
            }
                
        finally:
            # Registrar fim de requisi√ß√£o concorrente
            self.stats_manager.record_concurrent_end()
    
    async def process_single(
        self, 
        text: str, 
        prompt_template: str,
        json_schema: Optional[Dict[str, Any]] = None,
        custom_id: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Processa um √∫nico texto com template de prompt
        """
        
        request_id = custom_id if custom_id else self._generate_request_id()
        
        start_time = time.time()
        
        try:
            # Preparar prompt com template
            prompt = prompt_template.format(text=text, **kwargs)
            messages = [{"role": "user", "content": prompt}]
            
            # Fazer chamada √† API
            result = await self._make_api_call(messages, json_schema, request_id)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Adicionar informa√ß√µes de processamento
            result['processing_time'] = processing_time
            result['custom_id'] = custom_id
            
            # Registrar no StatsManager
            api_rate_limit_detected = is_rate_limit_error(result)
            
            self.stats_manager.record_request(
                success=result.get('success', False),
                tokens_input=result.get('input_tokens', 0),
                tokens_output=result.get('output_tokens', 0),
                tokens_cached=result.get('cached_tokens', 0),
                cost=result.get('cost', 0.0),
                api_response_time=result.get('api_response_time', 0.0),
                error_type=result.get('error_type'),
                retry_count=max(0, result.get('attempts', 1) - 1),
                api_rate_limit_detected=api_rate_limit_detected
            )
            
            # ‚úÖ LOG √öNICO E DETALHADO APENAS PARA FALHAS N√ÉO-RATE-LIMIT
            if not result.get('success', False) and not api_rate_limit_detected:
                # Construir informa√ß√µes detalhadas do erro
                error_info = {
                    'request_id': request_id,
                    'processing_time': round(processing_time, 3),
                    'error_type': result.get('error_type'),
                    'error': result.get('error'),
                    'error_line': result.get('error_line'),
                    'error_file': result.get('error_file'),
                    'error_function': result.get('error_function'),
                    'action': 'single_process_failed'
                }
                
                # Adicionar traceback se dispon√≠vel
                if 'full_traceback' in result:
                    error_info['full_traceback'] = result['full_traceback']
                
                logger.error(
                    f"Processamento individual falhou - {result.get('error_file', 'N/A')}:{result.get('error_line', 'N/A')} em {result.get('error_function', 'N/A')}(): {result.get('error')}",
                    extra=error_info
                )
            elif result.get('success', False):
                logger.debug(
                    "Processamento individual conclu√≠do",
                    extra={
                        'request_id': request_id,
                        'processing_time': round(processing_time, 3),
                        'action': 'single_process_success'
                    }
                )
            
            return result
            
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            
            # ‚úÖ ERRO INESPERADO COM CONTEXTO COMPLETO
            exc_type, exc_value, exc_traceback = sys.exc_info()
            error_line = None
            error_file = None
            error_function = None
            
            if exc_traceback:
                tb = exc_traceback
                while tb.tb_next:
                    tb = tb.tb_next
                error_line = tb.tb_lineno
                error_file = tb.tb_frame.f_code.co_filename
                error_function = tb.tb_frame.f_code.co_name
            
            error_traceback = traceback.format_exc()
            
            logger.error(
                f"Erro inesperado no processamento individual - {error_file.split('/')[-1] if error_file else 'N/A'}:{error_line} em {error_function}(): {str(e)}",
                extra={
                    'request_id': request_id,
                    'processing_time': round(processing_time, 3),
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'error_line': error_line,
                    'error_file': error_file.split('/')[-1] if error_file else None,
                    'error_function': error_function,
                    'full_traceback': error_traceback,
                    'action': 'single_process_unexpected_error'
                }
            )
            
            return {
                'content': None,
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'error_line': error_line,
                'error_file': error_file.split('/')[-1] if error_file else None,
                'error_function': error_function,
                'full_traceback': error_traceback,
                'processing_time': processing_time,
                'custom_id': custom_id,
                'request_id': request_id
            }
    
    def _calculate_progress_intervals(self, total: int) -> List[int]:
        """
        Calcula intervalos de progresso a cada 5 itens, independente do tamanho do lote
        """
        if total <= 5:
            # Lotes muito pequenos: log cada item
            return list(range(1, total + 1))
        
        # Sempre a cada 5, independente do tamanho
        intervals = list(range(5, total + 1, 5))
        
        # Garantir que o total sempre est√° inclu√≠do
        if total not in intervals:
            intervals.append(total)
        
        # Sempre incluir o primeiro item para feedback inicial
        if 1 not in intervals:
            intervals.insert(0, 1)
        
        return sorted(intervals)
    
    async def process_batch(
        self, 
        texts: List[str], 
        prompt_template: str,
        json_schema: Optional[Dict[str, Any]] = None,
        batch_id: Optional[str] = None,
        custom_ids: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Processa m√∫ltiplos textos em paralelo com logs organizados por responsabilidade
        """
        
        # Usar batch_id customizado ou gerar automaticamente
        batch_id = batch_id if batch_id else f"batch_{int(time.time())}"
        
        # Validar custom_ids se fornecido
        if custom_ids and len(custom_ids) != len(texts):
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(custom_ids)} != {len(texts)}")
        
        # Inicializar controles de progresso
        progress_intervals = self._calculate_progress_intervals(len(texts))
        self._last_logged_successes = 0
        
        # Inicializar rate limiter para o batch
        self.rate_limiter.start_batch(batch_id)
        
        # Iniciar tracking do batch no StatsManager
        self.stats_manager.start_batch(batch_id)
        
        # Log de in√≠cio
        logger.info(
            f"üöÄ Iniciando processamento em lote - {len(texts)} textos",
            extra={
                'batch_id': batch_id,
                'total_texts': len(texts),
                'has_custom_ids': custom_ids is not None,
                'has_json_schema': json_schema is not None,
                'max_concurrent': self.max_concurrent,
                'action': 'batch_start'
            }
        )
        
        # Criar tasks para processamento paralelo
        tasks = []
        for i, text in enumerate(texts):
            custom_id = custom_ids[i] if custom_ids else None
            task = self.process_single(text, prompt_template, json_schema, custom_id, **kwargs)
            tasks.append(task)
        
        # Executar com logs organizados
        results = []
        completed = 0
        start_time = time.time()
        
        for coro in asyncio.as_completed(tasks):
            try:
                result = await coro
                results.append(result)
                completed += 1
                
                # Detectar rate limit e delegar para rate limiter
                if not result.get('success', False):
                    if is_rate_limit_error(result):
                        # Calcular estat√≠sticas atuais
                        successful_so_far = sum(1 for r in results if r.get('success', False))
                        failed_so_far = completed - successful_so_far
                        wait_time = self._extract_wait_time_from_error_result(result)
                        
                        # Delegar para rate limiter (com contexto completo)
                        self.rate_limiter.record_api_rate_limit_with_context(
                            wait_time, completed, len(texts), successful_so_far, failed_so_far
                        )
                else:
                    # Notificar rate limiter sobre sucesso
                    self.rate_limiter.record_successful_request()
                
                # Log de progresso (responsabilidade do AI Processor - a cada 5)
                if completed in progress_intervals:
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    eta = (len(texts) - completed) / rate if rate > 0 else 0
                    
                    successful_so_far = sum(1 for r in results if r.get('success', False))
                    failed_so_far = completed - successful_so_far
                    recent_successes = successful_so_far - self._last_logged_successes
                    self._last_logged_successes = successful_so_far
                    
                    logger.info(
                        f"üìä Progresso: {completed}/{len(texts)} ({completed/len(texts)*100:.1f}%) | "
                        f"‚úÖ{successful_so_far} ‚ùå{failed_so_far} | "
                        f"üÜï{recent_successes} novos | "
                        f"‚è±Ô∏èETA: {eta/60:.1f}min",
                        extra={
                            'batch_id': batch_id,
                            'completed': completed,
                            'total': len(texts),
                            'successful_so_far': successful_so_far,
                            'failed_so_far': failed_so_far,
                            'recent_successes': recent_successes,
                            'processing_rate': round(rate, 2),
                            'eta_minutes': round(eta / 60, 1),
                            'progress_percent': round((completed / len(texts)) * 100, 1),
                            'action': 'batch_progress'
                        }
                    )
                    
            except Exception as e:
                # ‚úÖ LOG √öNICO DE ERRO DE TASK COM CONTEXTO COMPLETO
                exc_type, exc_value, exc_traceback = sys.exc_info()
                error_line = None
                error_file = None
                error_function = None
                
                if exc_traceback:
                    tb = exc_traceback
                    while tb.tb_next:
                        tb = tb.tb_next
                    error_line = tb.tb_lineno
                    error_file = tb.tb_frame.f_code.co_filename
                    error_function = tb.tb_frame.f_code.co_name
                
                error_traceback = traceback.format_exc()
                
                logger.error(
                    f"Erro em task do lote - {error_file.split('/')[-1] if error_file else 'N/A'}:{error_line} em {error_function}(): {str(e)}",
                    extra={
                        'batch_id': batch_id,
                        'completed': completed,
                        'error': str(e),
                        'error_type': type(e).__name__,
                        'error_line': error_line,
                        'error_file': error_file.split('/')[-1] if error_file else None,
                        'error_function': error_function,
                        'full_traceback': error_traceback,
                        'action': 'batch_task_error'
                    }
                )
                results.append({
                    'content': None,
                    'success': False,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'error_line': error_line,
                    'error_file': error_file.split('/')[-1] if error_file else None,
                    'error_function': error_function,
                    'full_traceback': error_traceback
                })
                completed += 1
        
        # Finalizar batch e obter stats espec√≠ficas do batch
        batch_stats = self.stats_manager.end_batch(batch_id)
        
        # Log de fim
        logger.info(
            f"‚úÖ Batch conclu√≠do - {batch_stats.successful_requests} sucessos, "
            f"{batch_stats.failed_requests} falhas em {batch_stats.processing_time:.1f}s",
            extra={
                'batch_id': batch_id,
                'total_requests': batch_stats.total_requests,
                'successful_requests': batch_stats.successful_requests,
                'failed_requests': batch_stats.failed_requests,
                'processing_time': round(batch_stats.processing_time, 2),
                'total_tokens': batch_stats.total_tokens,
                'total_cost': round(batch_stats.total_cost, 4),
                'avg_rate': round(batch_stats.avg_rate, 2),
                'success_rate': round(batch_stats.success_rate, 1),
                'action': 'batch_complete'
            }
        )
        
        return {
            'results': results,
            'batch_stats': batch_stats,
            'batch_id': batch_id
        }
    
    # M√©todos de utilidade e estat√≠sticas
    def get_global_stats(self):
        """Retorna estat√≠sticas globais atualizadas"""
        return self.stats_manager.get_global_stats()
    
    def show_stats(self, result_or_stats, title: str = "Stats"):
        """Mostra estat√≠sticas formatadas"""
        if isinstance(result_or_stats, dict) and 'batch_stats' in result_or_stats:
            stats = result_or_stats['batch_stats']
        else:
            stats = result_or_stats
        
        formatted_stats = self.stats_manager.format_stats(stats, title)
        print(formatted_stats)
    
    def show_calibration_stats(self):
        """Mostra estat√≠sticas de calibra√ß√£o do rate limiter"""
        calibration_stats = self.rate_limiter.get_calibration_stats()
        
        print("üéØ ESTAT√çSTICAS DE CALIBRA√á√ÉO:")
        print(f"   üìä Precis√£o das estimativas: {calibration_stats['accuracy_percentage']:.1f}%")
        print(f"   üîß Fator de calibra√ß√£o atual: {calibration_stats['current_calibration_factor']:.3f}")
        print(f"   üìà Total de calibra√ß√µes: {calibration_stats['total_calibrations']}")
        print(f"   üõ°Ô∏è Rate limits prevenidos: {calibration_stats['prevented_rate_limits']}")
        print(f"   üö® Rate limits detectados: {calibration_stats['api_rate_limits_detected']}")
    
    def show_rate_limiter_status(self):
        """Mostra status atual do rate limiter"""
        status = self.rate_limiter.get_status()
        
        print("‚ö° STATUS DO RATE LIMITER:")
        print(f"   üî¢ Tokens usados: {status['tokens_used']:,}")
        print(f"   üìä Utiliza√ß√£o: {status['utilization_percent']:.1f}%")
        print(f"   ‚è±Ô∏è Tempo no minuto: {status['time_in_minute']:.1f}s")
        print(f"   üîÑ Total de esperas: {status['total_waits']}")
        print(f"   ‚è≥ Tempo total de espera: {status['total_wait_time']:.1f}s")
