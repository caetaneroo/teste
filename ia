# core/ai_processor.py

import asyncio
import time
import json
import logging
import re
import tiktoken
import os
from typing import List, Dict, Any, Optional
from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI
from adaptive_throughput_controller import AdaptiveThroughputController
from stats_manager import StatsManager

logger = logging.getLogger(__name__)

MAX_RETRY = 2

def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """Carrega configuração dos modelos a partir do arquivo JSON"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Arquivo {config_path} não encontrado. Este arquivo é obrigatório.")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            models_config = json.load(f)
        if not models_config:
            raise ValueError(f"Arquivo {config_path} está vazio ou inválido.")
        return models_config
    except json.JSONDecodeError as e:
        raise ValueError(f"Erro ao decodificar JSON em {config_path}: {e}")
    except Exception as e:
        raise RuntimeError(f"Erro ao carregar {config_path}: {e}")

# Carrega configuração dos modelos
MODELS_CONFIG = load_models_config()
SUPPORTED_MODELS = set(MODELS_CONFIG.keys())

def is_rate_limit_error(result: Dict[str, Any]) -> bool:
    """Detecta se o erro é relacionado a rate limit"""
    if not isinstance(result, dict):
        return False
    error_msg = result.get('error', '').lower()
    return any(phrase in error_msg for phrase in [
        'token rate limit', 'rate limit', 'too many requests',
        'quota exceeded', 'rate_limit_exceeded'
    ])

def supports_json_schema(model: str) -> bool:
    """Verifica se o modelo suporta json_schema no response_format"""
    if model not in MODELS_CONFIG:
        raise ValueError(f"Modelo '{model}' não encontrado na configuração. Modelos disponíveis: {', '.join(sorted(SUPPORTED_MODELS))}")
    return MODELS_CONFIG[model].get('json_schema', False)

def get_model_pricing(model: str) -> Dict[str, float]:
    """Obtém preços do modelo a partir do arquivo models.json"""
    if model not in MODELS_CONFIG:
        raise ValueError(f"Modelo '{model}' não encontrado na configuração. Modelos disponíveis: {', '.join(sorted(SUPPORTED_MODELS))}")
    
    model_data = MODELS_CONFIG[model]
    return {
        'input': model_data['input'],
        'output': model_data['output'],
        'cache': model_data['cache']
    }

class JSONSchemaNotSupportedError(Exception):
    """Exceção levantada quando json_schema é usado com modelo incompatível"""
    pass

class AIProcessor:
    """
    Processador AI otimizado com controle adaptativo de throughput.
    
    Principais características:
    - Auto-ajuste de concorrência baseado em performance real
    - Prevenção inteligente de rate limits
    - Logs contextuais e não-invasivos
    - Stats centralizados e precisos
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.client_id = config['client_id']
        self.client_secret = config['client_secret']
        self.model = config.get('model', 'gpt-4o-mini')
        
        # Validação do modelo
        if self.model not in SUPPORTED_MODELS:
            raise ValueError(
                f"Modelo '{self.model}' não é suportado. "
                f"Modelos suportados: {', '.join(sorted(SUPPORTED_MODELS))}"
            )
        
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens')
        
        # Cliente da API
        self.client = AsyncIaraGenAI(
            client_id=self.client_id,
            client_secret=self.client_secret,
            environment=config.get('environment', 'dev'),
            provider=config.get('provider', 'azure_openai'),
            correlation_id=config.get('correlation_id', 'ai-processor')
        )
        
        # Inicialização do controlador adaptativo
        initial_max_tpm = config.get('max_tpm', 125000)  # OpenAI típico
        initial_concurrency = config.get('max_concurrent', 10)
        
        self.throughput_controller = AdaptiveThroughputController(
            initial_max_tpm=initial_max_tpm,
            initial_concurrency=initial_concurrency,
            model=self.model
        )
        
        # Stats manager centralizado
        self.stats_manager = StatsManager(model=self.model)
        
        # Tokenizer para estimativas
        try:
            self.encoder = tiktoken.encoding_for_model(self.model)
        except KeyError:
            self.encoder = tiktoken.get_encoding("cl100k_base")
        
        # Preços do modelo
        self.model_pricing = get_model_pricing(self.model)
        
        logger.info(
            f"AIProcessor inicializado - Modelo: {self.model} | "
            f"TPM inicial: {initial_max_tpm:,} | "
            f"Concorrência inicial: {initial_concurrency}",
            extra={
                'model': self.model,
                'max_tpm': initial_max_tpm,
                'max_concurrent': initial_concurrency,
                'json_schema_supported': supports_json_schema(self.model),
                'action': 'ai_processor_init'
            }
        )
    
    def _validate_json_schema_compatibility(self, json_schema: Optional[Dict[str, Any]]) -> None:
        """Valida se o modelo atual suporta json_schema"""
        if json_schema is not None and not supports_json_schema(self.model):
            supported_models = [
                model for model in SUPPORTED_MODELS 
                if MODELS_CONFIG[model].get('json_schema', False)
            ]
            raise JSONSchemaNotSupportedError(
                f"O modelo '{self.model}' não suporta json_schema. "
                f"Modelos compatíveis: {', '.join(sorted(supported_models))}"
            )
    
    def _generate_request_id(self) -> str:
        """Gera ID único para requisição"""
        return f"req_{int(time.time() * 1000)}_{id(self)}"
    
    def estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        """Estima tokens de forma mais precisa"""
        try:
            total_tokens = 0
            for message in messages:
                role_tokens = len(self.encoder.encode(message.get('role', '')))
                content_tokens = len(self.encoder.encode(message.get('content', '')))
                total_tokens += role_tokens + content_tokens + 4  # overhead por mensagem
            total_tokens += 50  # overhead da requisição
            return total_tokens
        except Exception as e:
            logger.warning(f"Erro na estimativa de tokens: {e}. Usando estimativa conservadora.")
            # Fallback conservador
            total_chars = sum(len(str(msg.get('content', ''))) for msg in messages)
            return int(total_chars * 0.3)
    
    def calculate_cost(self, input_tokens: int, output_tokens: int, cached_tokens: int = 0) -> float:
        """Calcula custo total considerando todos os tipos de tokens"""
        regular_input_tokens = max(0, input_tokens - cached_tokens)
        
        regular_input_cost = (regular_input_tokens / 1000) * self.model_pricing['input']
        cached_cost = (cached_tokens / 1000) * self.model_pricing['cache']
        output_cost = (output_tokens / 1000) * self.model_pricing['output']
        
        return regular_input_cost + cached_cost + output_cost
    
    def _prepare_json_schema(self, json_schema: Dict[str, Any]) -> Dict[str, Any]:
        """Prepara schema JSON para a API"""
        if not isinstance(json_schema, dict):
            return None
        
        return {
            "type": "json_schema",
            "json_schema": {
                "name": json_schema.get("name", "response_schema"),
                "description": json_schema.get("description", "Schema for structured response"),
                "schema": json_schema.get("schema", json_schema),
                "strict": json_schema.get("strict", True)
            }
        }
    
    def _extract_wait_time_from_error(self, result: Dict[str, Any]) -> float:
        """Extrai tempo de espera do erro de rate limit"""
        if 'retry_after' in result:
            try:
                return float(result['retry_after'])
            except (ValueError, TypeError):
                pass
        
        error_msg = result.get('error', '')
        patterns = [
            r'retry after (\d+) seconds',
            r'wait (\d+) seconds',
            r'retry.*?(\d+)\s*seconds?',
            r'(\d+)s'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, error_msg, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except (ValueError, IndexError):
                    continue
        
        # Verificar headers HTTP
        if 'response_headers' in result:
            headers = result['response_headers']
            if 'retry-after' in headers:
                try:
                    return float(headers['retry-after'])
                except (ValueError, TypeError):
                    pass
        
        return 60.0  # Fallback conservador
    
    async def _make_api_call(
        self,
        messages: List[Dict[str, str]],
        json_schema: Optional[Dict[str, Any]] = None,
        request_id: str = None
    ) -> Dict[str, Any]:
        """
        Faz chamada à API com controle adaptativo de throughput.
        
        O controlador decide quando e como fazer a requisição baseado
        na performance atual e limites aprendidos.
        """
        self._validate_json_schema_compatibility(json_schema)
        
        if not request_id:
            request_id = self._generate_request_id()
        
        if not messages or not isinstance(messages, list):
            return {
                'content': None,
                'tokens_used': 0,
                'success': False,
                'error': 'Messages inválidas',
                'error_type': 'ValidationError',
                'request_id': request_id
            }
        
        # Estimar tokens e aguardar aprovação do controlador
        estimated_tokens = self.estimate_tokens(messages)
        
        # O controlador decide se pode processar agora ou precisa aguardar
        await self.throughput_controller.acquire_permission(estimated_tokens)
        
        try:
            async for attempt in AsyncRetrying(
                stop=stop_after_attempt(MAX_RETRY),
                wait=wait_fixed(0.1),
                reraise=True
            ):
                with attempt:
                    if attempt.retry_state.attempt_number > 1:
                        # Em retry, aguardar nova aprovação
                        await self.throughput_controller.acquire_permission(estimated_tokens)
                    
                    start_time = time.time()
                    
                    try:
                        # Preparar parâmetros da API
                        api_params = {
                            "model": self.model,
                            "messages": messages,
                            "temperature": self.temperature
                        }
                        
                        if self.max_tokens is not None:
                            api_params["max_tokens"] = self.max_tokens
                        
                        if json_schema and isinstance(json_schema, dict):
                            api_params["response_format"] = self._prepare_json_schema(json_schema)
                        
                        # Fazer chamada à API
                        response = await self.client.chat.completions.create(**api_params)
                        
                        end_time = time.time()
                        api_response_time = end_time - start_time
                        
                        # Extrair informações de uso
                        tokens_used = response.usage.total_tokens
                        input_tokens = response.usage.prompt_tokens
                        output_tokens = response.usage.completion_tokens
                        
                        cached_tokens = 0
                        if hasattr(response.usage, 'prompt_tokens_details'):
                            cached_tokens = getattr(response.usage.prompt_tokens_details, 'cached_tokens', 0)
                        
                        # Informar o controlador sobre o sucesso
                        self.throughput_controller.record_success(
                            estimated_tokens=estimated_tokens,
                            actual_tokens=tokens_used,
                            response_time=api_response_time
                        )
                        
                        # Calcular custo
                        cost = self.calculate_cost(input_tokens, output_tokens, cached_tokens)
                        
                        # Processar resposta
                        content = response.choices[0].message.content
                        parsed_content = content
                        
                        if json_schema and isinstance(json_schema, dict) and content:
                            try:
                                parsed_content = json.loads(content)
                            except json.JSONDecodeError:
                                parsed_content = content
                        
                        # Registrar stats
                        self.stats_manager.record_request(
                            success=True,
                            tokens_input=input_tokens,
                            tokens_output=output_tokens,
                            tokens_cached=cached_tokens,
                            cost=cost,
                            api_response_time=api_response_time,
                            estimated_tokens=estimated_tokens,
                            actual_tokens=tokens_used,
                            retry_count=attempt.retry_state.attempt_number - 1
                        )
                        
                        return {
                            'content': parsed_content,
                            'raw_content': content,
                            'tokens_used': tokens_used,
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'cached_tokens': cached_tokens,
                            'cost': cost,
                            'api_response_time': api_response_time,
                            'success': True,
                            'is_json': json_schema is not None,
                            'attempts': attempt.retry_state.attempt_number,
                            'request_id': request_id
                        }
                    
                    except Exception as e:
                        end_time = time.time()
                        api_response_time = end_time - start_time
                        
                        # Extrair headers da resposta se disponível
                        response_headers = {}
                        if hasattr(e, 'response') and hasattr(e.response, 'headers'):
                            response_headers = dict(e.response.headers)
                        
                        error_result = {
                            'content': None,
                            'raw_content': None,
                            'tokens_used': 0,
                            'input_tokens': 0,
                            'output_tokens': 0,
                            'cached_tokens': 0,
                            'cost': 0.0,
                            'api_response_time': api_response_time,
                            'success': False,
                            'error': str(e),
                            'error_type': type(e).__name__,
                            'response_headers': response_headers,
                            'is_json': False,
                            'attempts': attempt.retry_state.attempt_number,
                            'request_id': request_id
                        }
                        
                        # Se for rate limit, informar o controlador
                        if is_rate_limit_error(error_result):
                            wait_time = self._extract_wait_time_from_error(error_result)
                            self.throughput_controller.record_rate_limit(wait_time)
                            raise  # Re-raise para retry
                        else:
                            # Erro não relacionado a rate limit
                            self.stats_manager.record_request(
                                success=False,
                                error_type=error_result['error_type'],
                                api_response_time=api_response_time,
                                retry_count=attempt.retry_state.attempt_number - 1
                            )
                            return error_result
        
        except RetryError as retry_error:
            # Máximo de tentativas excedido
            self.stats_manager.record_request(
                success=False,
                error_type='RetryError',
                retry_count=MAX_RETRY
            )
            
            return {
                'content': None,
                'raw_content': None,
                'tokens_used': 0,
                'input_tokens': 0,
                'output_tokens': 0,
                'cached_tokens': 0,
                'cost': 0.0,
                'api_response_time': 0.0,
                'success': False,
                'error': f'Máximo de tentativas excedido: {retry_error.last_attempt.exception() if retry_error.last_attempt else "Erro desconhecido"}',
                'error_type': 'RetryError',
                'is_json': False,
                'attempts': MAX_RETRY,
                'request_id': request_id
            }
    
    async def process_single(
        self,
        text: str,
        prompt_template: str,
        json_schema: Optional[Dict[str, Any]] = None,
        custom_id: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Processa um único texto com controle adaptativo"""
        self._validate_json_schema_compatibility(json_schema)
        
        request_id = custom_id if custom_id else self._generate_request_id()
        start_time = time.time()
        
        try:
            prompt = prompt_template.format(text=text, **kwargs)
            messages = [{"role": "user", "content": prompt}]
            
            result = await self._make_api_call(messages, json_schema, request_id)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            result['processing_time'] = processing_time
            result['id'] = request_id
            
            # Remover IDs duplicados
            if 'custom_id' in result:
                del result['custom_id']
            if 'request_id' in result:
                del result['request_id']
            
            return result
        
        except JSONSchemaNotSupportedError:
            raise
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            
            return {
                'content': None,
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'processing_time': processing_time,
                'id': request_id
            }
    
    def _calculate_progress_intervals(self, total: int) -> List[int]:
        """Calcula intervalos inteligentes para logs de progresso"""
        if total <= 5:
            return list(range(1, total + 1))
        
        # Intervalos que fazem sentido para o usuário
        intervals = []
        
        # Primeiros marcos
        intervals.extend([1, 5, 10])
        
        # Marcos baseados em percentual
        for percent in [25, 50, 75, 90, 95]:
            milestone = int(total * percent / 100)
            if milestone not in intervals and milestone <= total:
                intervals.append(milestone)
        
        # Sempre incluir o final
        if total not in intervals:
            intervals.append(total)
        
        return sorted([i for i in intervals if i <= total])
    
    async def process_batch(
        self,
        texts: List[str],
        prompt_template: str,
        json_schema: Optional[Dict[str, Any]] = None,
        batch_id: Optional[str] = None,
        custom_ids: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Processa lote com controle adaptativo e logs inteligentes.
        
        O sistema se auto-ajusta durante a execução para maximizar
        throughput sem atingir rate limits.
        """
        self._validate_json_schema_compatibility(json_schema)
        
        batch_id = batch_id if batch_id else f"batch_{int(time.time())}"
        
        # Validar custom_ids se fornecidos
        if custom_ids is not None:
            if hasattr(custom_ids, 'tolist'):
                custom_ids = custom_ids.tolist()
            elif hasattr(custom_ids, 'values'):
                custom_ids = list(custom_ids.values)
            elif not isinstance(custom_ids, list):
                custom_ids = list(custom_ids)
            
            if len(custom_ids) != len(texts):
                raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(custom_ids)} != {len(texts)}")
            
            custom_ids = [str(cid) if cid is not None else None for cid in custom_ids]
        
        # Inicializar batch
        self.throughput_controller.start_batch(batch_id)
        self.stats_manager.start_batch(batch_id)
        
        total_texts = len(texts)
        completed = 0
        failed = 0
        
        # Intervalos de progresso inteligentes
        progress_intervals = self._calculate_progress_intervals(total_texts)
        logged_progress = set()
        progress_lock = asyncio.Lock()
        
        logger.info(
            f"🚀 Iniciando batch {batch_id} - {total_texts} textos | "
            f"Modelo: {self.model} | Controle adaptativo ativo",
            extra={
                'batch_id': batch_id,
                'total_texts': total_texts,
                'model': self.model,
                'has_custom_ids': custom_ids is not None,
                'has_json_schema': json_schema is not None,
                'action': 'batch_start'
            }
        )
        
        start_time = time.time()
        
        async def log_progress(current_completed: int, current_failed: int):
            """Log de progresso inteligente"""
            async with progress_lock:
                current_total = current_completed + current_failed
                
                if current_total in progress_intervals and current_total not in logged_progress:
                    logged_progress.add(current_total)
                    
                    elapsed = time.time() - start_time
                    rate = current_total / elapsed if elapsed > 0 else 0
                    
                    remaining = total_texts - current_total
                    eta_seconds = remaining / rate if rate > 0 else 0
                    eta_minutes = eta_seconds / 60
                    
                    # Stats do controlador
                    controller_stats = self.throughput_controller.get_current_stats()
                    
                    logger.info(
                        f"📊 Progresso: {current_total}/{total_texts} ({current_total/total_texts*100:.1f}%) | "
                        f"✅{current_completed} ❌{current_failed} | "
                        f"🚀{rate:.1f} req/s | ⏱️ETA: {eta_minutes:.1f}min | "
                        f"🔄{controller_stats['current_concurrency']} threads | "
                        f"📈{controller_stats['current_tpm']:,} TPM",
                        extra={
                            'batch_id': batch_id,
                            'completed': current_total,
                            'total': total_texts,
                            'successful': current_completed,
                            'failed': current_failed,
                            'rate': round(rate, 2),
                            'eta_minutes': round(eta_minutes, 1),
                            'progress_percent': round(current_total / total_texts * 100, 1),
                            'current_concurrency': controller_stats['current_concurrency'],
                            'current_tpm': controller_stats['current_tpm'],
                            'action': 'batch_progress'
                        }
                    )
        
        # Processar todos os textos
        all_results = [None] * total_texts
        
        # Criar tarefas para processamento
        async def process_item(index: int, text: str):
            custom_id = None
            if custom_ids is not None and index < len(custom_ids):
                custom_id = custom_ids[index]
            
            return await self.process_single(
                text, prompt_template, json_schema, custom_id, **kwargs
            )
        
        # Executar com controle de concorrência adaptativo
        semaphore = asyncio.Semaphore(self.throughput_controller.get_max_concurrency())
        
        async def process_with_semaphore(index: int, text: str):
            async with semaphore:
                return index, await process_item(index, text)
        
        # Criar todas as tarefas
        tasks = [
            asyncio.create_task(process_with_semaphore(i, text))
            for i, text in enumerate(texts)
        ]
        
        # Processar conforme completam
        for task in asyncio.as_completed(tasks):
            try:
                index, result = await task
                all_results[index] = result
                
                if result.get('success', False):
                    completed += 1
                else:
                    failed += 1
                
                await log_progress(completed, failed)
                
            except Exception as e:
                failed += 1
                logger.error(f"Erro em task do batch: {e}", extra={'batch_id': batch_id})
                await log_progress(completed, failed)
        
        # Finalizar batch
        final_results = [r for r in all_results if r is not None]
        
        # Ajustar IDs nos resultados
        for res in final_results:
            if 'id' not in res:
                if 'custom_id' in res and res['custom_id'] is not None:
                    res['id'] = res['custom_id']
                else:
                    res['id'] = res.get('request_id')
            
            # Limpar IDs duplicados
            if 'custom_id' in res:
                del res['custom_id']
            if 'request_id' in res:
                del res['request_id']
        
        # Finalizar stats
        batch_stats = self.stats_manager.end_batch(batch_id)
        controller_final_stats = self.throughput_controller.end_batch(batch_id)
        
        logger.info(
            f"✅ Batch {batch_id} concluído - {completed} sucessos, {failed} falhas | "
            f"⏱️{batch_stats.processing_time:.1f}s | "
            f"🚀{batch_stats.avg_rate:.1f} req/s | "
            f"💰${batch_stats.total_cost:.4f} | "
            f"📊Eficiência: {batch_stats.efficiency_rate:.1f}%",
            extra={
                'batch_id': batch_id,
                'successful_requests': completed,
                'failed_requests': failed,
                'processing_time': round(batch_stats.processing_time, 2),
                'avg_rate': round(batch_stats.avg_rate, 2),
                'total_cost': round(batch_stats.total_cost, 4),
                'efficiency_rate': round(batch_stats.efficiency_rate, 1),
                'action': 'batch_complete'
            }
        )
        
        return {
            'results': final_results,
            'batch_stats': batch_stats,
            'controller_stats': controller_final_stats,
            'batch_id': batch_id
        }
    
    def get_complete_stats(self, batch_result: Optional[Dict] = None, include_global: bool = True) -> Dict[str, Any]:
        """Retorna stats completos centralizados"""
        complete_stats = {}
        
        if include_global:
            global_stats = self.stats_manager.get_global_stats()
            controller_stats = self.throughput_controller.get_global_stats()
            
            complete_stats['global'] = {
                'processing_stats': self.stats_manager.format_stats(global_stats, "ESTATÍSTICAS GLOBAIS"),
                'controller_stats': controller_stats,
                'summary': f"""🎯 RESUMO GLOBAL:
📊 Requests: {global_stats.total_requests} (✅{global_stats.successful_requests} ❌{global_stats.failed_requests})
🔢 Tokens: {global_stats.total_tokens:,} (💾{global_stats.total_tokens_cached:,} cached)
💰 Custo: ${global_stats.total_cost:.4f}
🚀 Performance: {global_stats.avg_rate:.1f} req/s | {global_stats.efficiency_rate:.1f}% eficiência
🎯 Precisão estimativa: {controller_stats.get('estimation_accuracy', 0):.1f}%
📈 TPM máximo atingido: {controller_stats.get('max_tpm_reached', 0):,}"""
            }
        
        if batch_result and 'batch_stats' in batch_result:
            batch_stats = batch_result['batch_stats']
            batch_id = batch_result.get('batch_id', 'unknown')
            controller_stats = batch_result.get('controller_stats', {})
            
            complete_stats['batch'] = {
                'id': batch_id,
                'processing_stats': self.stats_manager.format_stats(batch_stats, f"BATCH {batch_id}"),
                'controller_stats': controller_stats,
                'summary': f"""🎯 RESUMO BATCH {batch_id}:
📊 Requests: {batch_stats.total_requests} (✅{batch_stats.successful_requests} ❌{batch_stats.failed_requests})
🔢 Tokens: {batch_stats.total_tokens:,} (💾{batch_stats.total_tokens_cached:,} cached)
💰 Custo: ${batch_stats.total_cost:.4f}
🚀 Performance: {batch_stats.avg_rate:.1f} req/s | {batch_stats.efficiency_rate:.1f}% eficiência
🔄 Concorrência máxima: {controller_stats.get('max_concurrency_used', 0)}
⏱️ Tempo total: {batch_stats.processing_time:.1f}s"""
            }
        
        return complete_stats
    
    # Métodos de conveniência para compatibilidade
    def get_global_stats(self):
        """Retorna stats globais"""
        return self.stats_manager.get_global_stats()
    
    def show_stats(self, result_or_stats, title: str = "Stats"):
        """Exibe stats formatados"""
        if isinstance(result_or_stats, dict) and 'batch_stats' in result_or_stats:
            stats = result_or_stats['batch_stats']
        else:
            stats = result_or_stats
        
        formatted_stats = self.stats_manager.format_stats(stats, title)
        print(formatted_stats)
    
    def show_complete_stats(self, batch_result: Optional[Dict] = None, include_global: bool = True):
        """Exibe stats completos"""
        complete_stats = self.get_complete_stats(batch_result, include_global)
        
        print("=" * 80)
        print("🎯 RELATÓRIO COMPLETO DE ESTATÍSTICAS")
        print("=" * 80)
        
        if 'global' in complete_stats:
            print(complete_stats['global']['summary'])
            print()
        
        if 'batch' in complete_stats:
            print(complete_stats['batch']['summary'])
            print()
        
        print("=" * 80)
    
    def show_cost_breakdown(self, result: Dict[str, Any]):
        """Mostra breakdown detalhado dos custos"""
        if not result.get('success', False):
            print("❌ Não é possível mostrar breakdown de custo para requisição sem sucesso")
            return
        
        input_tokens = result.get('input_tokens', 0)
        output_tokens = result.get('output_tokens', 0)
        cached_tokens = result.get('cached_tokens', 0)
        total_cost = result.get('cost', 0.0)
        
        regular_input_tokens = max(0, input_tokens - cached_tokens)
        regular_input_cost = (regular_input_tokens / 1000) * self.model_pricing['input']
        cached_cost = (cached_tokens / 1000) * self.model_pricing['cache']
        output_cost = (output_tokens / 1000) * self.model_pricing['output']
        
        print("💰 BREAKDOWN DETALHADO DE CUSTOS:")
        print(f"  🤖 Modelo: {self.model}")
        print(f"  📊 Total de tokens: {input_tokens + output_tokens:,}")
        print(f"  📥 Input regulares: {regular_input_tokens:,} × ${self.model_pricing['input']:.6f}/1k = ${regular_input_cost:.6f}")
        
        if cached_tokens > 0:
            print(f"  🗄️ Cached tokens: {cached_tokens:,} × ${self.model_pricing['cache']:.6f}/1k = ${cached_cost:.6f}")
            savings = (cached_tokens / 1000) * (self.model_pricing['input'] - self.model_pricing['cache'])
            discount_percent = ((self.model_pricing['input'] - self.model_pricing['cache'])/self.model_pricing['input']*100)
            print(f"  💡 Economia com cache: ${savings:.6f} ({discount_percent:.1f}% desconto)")
        
        print(f"  📤 Output tokens: {output_tokens:,} × ${self.model_pricing['output']:.6f}/1k = ${output_cost:.6f}")
        print(f"  💵 Custo total: ${total_cost:.6f}")
