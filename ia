# core/ai_processor.py
import asyncio
import time
import json
import logging
import re
from typing import List, Dict, Any, Optional, Union
from openai import AsyncOpenAI, RateLimitError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from .rate_limiter import AdaptiveRateLimiter
from .stats_manager import StatsManager

# Logger espec√≠fico do m√≥dulo
logger = logging.getLogger(__name__)

class AIProcessor:
    """
    Processador de IA com rate limiting adaptativo, processamento ass√≠ncrono otimizado,
    suporte a JSON Schema estruturado e estimativa precisa de tokens
    
    Caracter√≠sticas:
    - Rate limiting adaptativo com calibra√ß√£o baseada em hist√≥rico
    - Estimativa precisa usando tiktoken (oficial OpenAI)
    - Processamento ass√≠ncrono com controle de concorr√™ncia
    - M√©tricas detalhadas incluindo cached tokens
    - Suporte a IDs customizados para rastreamento
    - Logging estruturado com progresso otimizado para batches grandes
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.api_key = config.get('openai_api_key')
        self.model = config.get('model', 'gpt-3.5-turbo')
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens', None)  # None = sem limite
        
        # Configurar cliente ass√≠ncrono
        if not self.api_key:
            raise ValueError("openai_api_key √© obrigat√≥rio na configura√ß√£o")
        
        self.client = AsyncOpenAI(api_key=self.api_key)
        
        # Rate limiter adaptativo com calibra√ß√£o
        self.rate_limiter = AdaptiveRateLimiter(
            max_tokens_per_minute=config.get('max_tpm', 180000),
            calibration_enabled=config.get('adaptive_calibration', True)
        )
        
        # Controle de concorr√™ncia
        max_concurrent = config.get('max_concurrent', 15)
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # Stats manager
        self.stats_manager = StatsManager()
        
        # Contador de requisi√ß√µes para IDs √∫nicos
        self._request_counter = 0
        
        # Pre√ßos por modelo (por 1K tokens) - atualizar conforme necess√°rio
        self.pricing = {
            'gpt-3.5-turbo': {'input': 0.0015, 'output': 0.002},
            'gpt-4': {'input': 0.03, 'output': 0.06},
            'gpt-4-turbo': {'input': 0.01, 'output': 0.03},
            'gpt-4o': {'input': 0.005, 'output': 0.015},
            'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006}
        }
        
        # Verificar se tiktoken est√° dispon√≠vel
        self._tiktoken_available = self._check_tiktoken()
        
        # Log de inicializa√ß√£o
        logger.info(
            "AIProcessor inicializado",
            extra={
                'model': self.model,
                'max_tpm': config.get('max_tpm', 180000),
                'max_concurrent': max_concurrent,
                'temperature': self.temperature,
                'max_tokens': self.max_tokens,
                'tiktoken_available': self._tiktoken_available,
                'adaptive_calibration': config.get('adaptive_calibration', True),
                'action': 'initialize'
            }
        )
    
    def _check_tiktoken(self) -> bool:
        """Verifica se tiktoken est√° dispon√≠vel"""
        try:
            import tiktoken
            return True
        except ImportError:
            logger.warning("tiktoken n√£o encontrado. Usando estimativa fallback. Instale com: pip install tiktoken")
            return False
    
    def _generate_request_id(self) -> str:
        """Gera ID √∫nico para cada requisi√ß√£o"""
        self._request_counter += 1
        timestamp = int(time.time() * 1000)
        return f"req_{timestamp}_{self._request_counter}"
    
    def estimate_tokens(self, messages: List[Dict[str, str]], model: str = None) -> int:
        """
        Estima tokens usando tiktoken (biblioteca oficial da OpenAI) ou fallback melhorado
        """
        if self._tiktoken_available:
            return self._estimate_tokens_tiktoken(messages, model)
        else:
            return self._estimate_tokens_fallback(messages)
    
    def _estimate_tokens_tiktoken(self, messages: List[Dict[str, str]], model: str = None) -> int:
        """Estimativa precisa usando tiktoken"""
        import tiktoken
        
        # Validar entrada
        if not messages or not isinstance(messages, list):
            return self.max_tokens if self.max_tokens is not None else 1000
        
        # Usar modelo atual se n√£o especificado
        if model is None:
            model = self.model
        
        # Obter encoding para o modelo
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # Fallback para encoding padr√£o baseado no modelo
            if "gpt-4o" in model:
                encoding = tiktoken.get_encoding("o200k_base")
            else:
                encoding = tiktoken.get_encoding("cl100k_base")
        
        # Configura√ß√µes por modelo (baseado na documenta√ß√£o oficial OpenAI)
        if model in {
            "gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613",
            "gpt-4-0314", "gpt-4-32k-0314", "gpt-4-0613", "gpt-4-32k-0613",
            "gpt-4o-mini-2024-07-18", "gpt-4o-2024-08-06"
        }:
            tokens_per_message = 3
            tokens_per_name = 1
        elif "gpt-3.5-turbo" in model:
            tokens_per_message = 3
            tokens_per_name = 1
        elif "gpt-4" in model:
            tokens_per_message = 3
            tokens_per_name = 1
        else:
            tokens_per_message = 3
            tokens_per_name = 1
        
        # Calcular tokens das mensagens
        num_tokens = 0
        for message in messages:
            if not isinstance(message, dict):
                continue
                
            num_tokens += tokens_per_message
            for key, value in message.items():
                if isinstance(value, str):
                    num_tokens += len(encoding.encode(value))
                    if key == "name":
                        num_tokens += tokens_per_name
        
        # Tokens para priming da resposta
        num_tokens += 3
        
        # Adicionar tokens de resposta estimados se max_tokens definido
        if self.max_tokens is not None:
            num_tokens += self.max_tokens
        else:
            # Estimativa conservadora para rate limiting quando sem limite
            num_tokens += 1000
        
        return num_tokens
    
    def _estimate_tokens_fallback(self, messages: List[Dict[str, str]]) -> int:
        """Estimativa manual melhorada como fallback"""
        total_text = " ".join([msg.get("content", "") for msg in messages if isinstance(msg.get("content"), str)])
        
        if not total_text:
            return self.max_tokens if self.max_tokens is not None else 1000
        
        # M√©todo melhorado: 4 caracteres por token + contagem de palavras
        tokens_by_chars = max(1, len(total_text) // 4)
        words = len(total_text.split())
        tokens_by_words = words
        
        # Usar a maior estimativa para ser conservador
        estimated_tokens = max(tokens_by_chars, tokens_by_words)
        
        # Adicionar overhead para formato de mensagem
        estimated_tokens += len(messages) * 3  # tokens por mensagem
        estimated_tokens += 3  # priming
        
        # Adicionar tokens de resposta
        if self.max_tokens is not None:
            return estimated_tokens + self.max_tokens
        else:
            return estimated_tokens + 1000  # Estimativa padr√£o
    
    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Calcula custo estimado baseado no modelo e tokens"""
        pricing = self.pricing.get(self.model, self.pricing['gpt-3.5-turbo'])
        
        input_cost = (input_tokens / 1000) * pricing['input']
        output_cost = (output_tokens / 1000) * pricing['output']
        
        return input_cost + output_cost
    
    def _prepare_json_schema(self, json_schema: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Prepara o JSON schema para a API OpenAI"""
        if not json_schema or not isinstance(json_schema, dict):
            return None
        
        return {
            "type": "json_schema",
            "json_schema": {
                "name": json_schema.get("name", "response_schema"),
                "description": json_schema.get("description", "Schema for structured response"),
                "schema": json_schema.get("schema", json_schema),
                "strict": json_schema.get("strict", True)
            }
        }
    
    def _extract_wait_time_from_error(self, error: RateLimitError) -> float:
        """Extrai tempo de espera do erro de rate limit"""
        error_msg = str(error)
        
        # Procurar padr√µes como "retry after 20 seconds"
        match = re.search(r'retry.+?(\d+(?:\.\d+)?).+?seconds?', error_msg, re.IGNORECASE)
        if match:
            return float(match.group(1))
        
        # Procurar padr√µes como "20s"
        match = re.search(r'(\d+(?:\.\d+)?)s', error_msg)
        if match:
            return float(match.group(1))
        
        # Default: 60 segundos se n√£o conseguir extrair
        return 60.0
    
    def _get_error_location(self) -> Dict[str, Any]:
        """Captura apenas a localiza√ß√£o essencial do erro"""
        import traceback
        import sys
        
        exc_type, exc_value, exc_traceback = sys.exc_info()
        
        if exc_traceback:
            # Pegar o √∫ltimo frame (onde o erro realmente ocorreu)
            tb = exc_traceback
            while tb.tb_next:
                tb = tb.tb_next
            
            frame = tb.tb_frame
            filename = frame.f_code.co_filename.split('/')[-1]
            
            return {
                'file': filename,
                'function': frame.f_code.co_name,
                'line': tb.tb_lineno,
                'error_context': f"{filename}:{frame.f_code.co_name}():{tb.tb_lineno}"
            }
        
        return {'error_context': 'Localiza√ß√£o n√£o dispon√≠vel'}
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(RateLimitError)
    )
    async def _make_api_call(self, 
                           messages: List[Dict[str, str]], 
                           json_schema: Optional[Dict[str, Any]] = None,
                           request_id: str = None) -> Dict[str, Any]:
        """Faz chamada √† API com rate limiting adaptativo e estimativa precisa de tokens"""
        
        if not request_id:
            request_id = self._generate_request_id()
        
        # Validar entrada
        if not messages or not isinstance(messages, list):
            return {
                'content': None,
                'tokens_used': 0,
                'success': False,
                'error': 'Messages inv√°lidas',
                'error_type': 'ValidationError'
            }
        
        # Estimar tokens necess√°rios
        base_estimate = self.estimate_tokens(messages)
        
        # Aguardar com rate limiting adaptativo (calibrado)
        calibrated_estimate = await self.rate_limiter.wait_for_tokens(base_estimate)
        
        # Registrar in√≠cio de requisi√ß√£o concorrente
        self.stats_manager.record_concurrent_start()
        
        async with self.semaphore:
            start_time = time.time()
            
            try:
                # Preparar par√¢metros da API
                api_params = {
                    "model": self.model,
                    "messages": messages,
                    "temperature": self.temperature
                }
                
                # S√≥ incluir max_tokens se estiver definido
                if self.max_tokens is not None:
                    api_params["max_tokens"] = self.max_tokens
                
                # Adicionar JSON schema se fornecido E for v√°lido
                if json_schema and isinstance(json_schema, dict):
                    prepared_schema = self._prepare_json_schema(json_schema)
                    if prepared_schema:
                        api_params["response_format"] = prepared_schema
                
                # Log da tentativa de chamada
                logger.debug(
                    "Iniciando chamada √† API",
                    extra={
                        'request_id': request_id,
                        'base_estimate': base_estimate,
                        'calibrated_estimate': calibrated_estimate,
                        'has_json_schema': json_schema is not None,
                        'tiktoken_used': self._tiktoken_available,
                        'action': 'api_call_start'
                    }
                )
                
                response = await self.client.chat.completions.create(**api_params)
                
                end_time = time.time()
                api_response_time = end_time - start_time
                
                # Extrair tokens usados
                tokens_used = response.usage.total_tokens
                input_tokens = response.usage.prompt_tokens
                output_tokens = response.usage.completion_tokens
                
                # Extrair cached tokens se dispon√≠vel
                cached_tokens = 0
                if hasattr(response.usage, 'prompt_tokens_details'):
                    cached_tokens = getattr(response.usage.prompt_tokens_details, 'cached_tokens', 0)
                
                # Registrar tokens no rate limiter
                self.rate_limiter.record_tokens(tokens_used)
                
                # Registrar para calibra√ß√£o adaptativa
                self.rate_limiter.record_token_usage(base_estimate, tokens_used)
                
                # Calcular custo
                cost = self.calculate_cost(input_tokens, output_tokens)
                
                # Processar resposta
                content = response.choices[0].message.content
                parsed_content = content
                
                # Se JSON schema foi usado, tentar parsear JSON
                if json_schema and isinstance(json_schema, dict) and content:
                    try:
                        parsed_content = json.loads(content)
                        logger.debug(
                            "JSON Schema parseado com sucesso",
                            extra={
                                'request_id': request_id,
                                'action': 'json_parse_success'
                            }
                        )
                    except json.JSONDecodeError as e:
                        logger.warning(
                            "Erro ao parsear JSON Schema",
                            extra={
                                'request_id': request_id,
                                'error': str(e),
                                'raw_content': content[:200],
                                'action': 'json_parse_error'
                            }
                        )
                        parsed_content = content
                
                # Log de sucesso com m√©tricas de precis√£o
                estimation_accuracy = abs(calibrated_estimate - tokens_used) / tokens_used * 100 if tokens_used > 0 else 0
                
                logger.debug(
                    "Chamada √† API bem-sucedida",
                    extra={
                        'request_id': request_id,
                        'tokens_used': tokens_used,
                        'input_tokens': input_tokens,
                        'output_tokens': output_tokens,
                        'cached_tokens': cached_tokens,
                        'base_estimate': base_estimate,
                        'calibrated_estimate': calibrated_estimate,
                        'estimation_accuracy': round(estimation_accuracy, 1),
                        'api_response_time': round(api_response_time, 3),
                        'cost': round(cost, 6),
                        'action': 'api_call_success'
                    }
                )
                
                return {
                    'content': parsed_content,
                    'raw_content': content,
                    'tokens_used': tokens_used,
                    'input_tokens': input_tokens,
                    'output_tokens': output_tokens,
                    'cached_tokens': cached_tokens,
                    'cost': cost,
                    'api_response_time': api_response_time,
                    'success': True,
                    'is_json': json_schema is not None,
                    'attempts': 1
                }
                
            except RateLimitError as e:
                # Registrar rate limit com tempo no StatsManager
                wait_time = self._extract_wait_time_from_error(e)
                self.stats_manager.record_rate_limit_wait(wait_time)
                
                logger.warning(
                    "Rate limit atingido - retry autom√°tico",
                    extra={
                        'request_id': request_id,
                        'error': str(e),
                        'wait_time': wait_time,
                        'action': 'rate_limit_retry'
                    }
                )
                raise  # Tenacity vai fazer retry
            
            except Exception as e:
                end_time = time.time()
                api_response_time = end_time - start_time
                
                # Capturar localiza√ß√£o essencial do erro
                error_location = self._get_error_location()
                
                logger.error(
                    "Erro na chamada √† API",
                    extra={
                        'request_id': request_id,
                        'error_type': type(e).__name__,
                        'error_message': str(e),
                        'error_location': error_location['error_context'],
                        'api_response_time': round(api_response_time, 3),
                        'action': 'api_call_error'
                    }
                )
                
                return {
                    'content': None,
                    'raw_content': None,
                    'tokens_used': 0,
                    'input_tokens': 0,
                    'output_tokens': 0,
                    'cached_tokens': 0,
                    'cost': 0.0,
                    'api_response_time': api_response_time,
                    'success': False,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'is_json': False,
                    'attempts': 1
                }
            
            finally:
                # Registrar fim de requisi√ß√£o concorrente
                self.stats_manager.record_concurrent_end()
    
    async def process_single(self, 
                           text: str, 
                           prompt_template: str,
                           json_schema: Optional[Dict[str, Any]] = None,
                           custom_id: Optional[str] = None,
                           **kwargs) -> Dict[str, Any]:
        """Processa um √∫nico texto com todas as otimiza√ß√µes"""
        
        # Usar ID customizado ou gerar automaticamente
        request_id = custom_id if custom_id else self._generate_request_id()
        
        # Log do in√≠cio da requisi√ß√£o
        logger.info(
            "Iniciando processamento individual",
            extra={
                'request_id': request_id,
                'custom_id_provided': custom_id is not None,
                'text_length': len(text),
                'text_preview': text[:100] + "..." if len(text) > 100 else text,
                'has_json_schema': json_schema is not None,
                'action': 'single_process_start'
            }
        )
        
        start_time = time.time()
        
        try:
            # Preparar mensagens
            formatted_prompt = prompt_template.format(text=text, **kwargs)
            messages = [{"role": "user", "content": formatted_prompt}]
            
            # Fazer chamada √† API
            result = await self._make_api_call(messages, json_schema, request_id)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Garantir que result √© sempre um dict
            if not isinstance(result, dict):
                result = {
                    'content': None,
                    'tokens_used': 0,
                    'cost': 0.0,
                    'success': False,
                    'error': str(result) if result else 'Erro desconhecido',
                    'error_type': 'UnknownError'
                }
            
            # Registrar no StatsManager com verifica√ß√µes seguras
            self.stats_manager.record_request(
                success=result.get('success', False),
                tokens_input=result.get('input_tokens', 0),
                tokens_output=result.get('output_tokens', 0),
                tokens_cached=result.get('cached_tokens', 0),
                cost=result.get('cost', 0.0),
                api_response_time=result.get('api_response_time', 0.0),
                error_type=result.get('error_type'),
                retry_count=max(0, result.get('attempts', 1) - 1)
            )
            
            # Log simples sem duplicar detalhes
            if result.get('success'):
                logger.info(
                    "Processamento individual conclu√≠do",
                    extra={
                        'request_id': request_id,
                        'processing_time': round(processing_time, 3),
                        'action': 'single_process_success'
                    }
                )
            else:
                # Log simples de falha, detalhes j√° foram logados no _make_api_call
                logger.warning(
                    "Processamento individual falhou",
                    extra={
                        'request_id': request_id,
                        'processing_time': round(processing_time, 3),
                        'action': 'single_process_failed'
                    }
                )
            
            result['processing_time'] = processing_time
            result['request_id'] = request_id
            
            return result
            
        except Exception as e:
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Registrar erro no StatsManager
            self.stats_manager.record_request(
                success=False,
                error_type=type(e).__name__
            )
            
            logger.error(
                "Exce√ß√£o no processamento individual",
                extra={
                    'request_id': request_id,
                    'processing_time': round(processing_time, 3),
                    'exception_type': type(e).__name__,
                    'exception_message': str(e),
                    'action': 'single_process_exception'
                }
            )
            
            return {
                'content': None,
                'tokens_used': 0,
                'cost': 0.0,
                'processing_time': processing_time,
                'success': False,
                'error': str(e),
                'error_type': type(e).__name__,
                'request_id': request_id
            }
    
    def _calculate_progress_intervals(self, total: int) -> List[int]:
        """
        Calcula intervalos inteligentes para log de progresso baseado no tamanho do batch
        """
        if total <= 10:
            # Batches pequenos: log cada item
            return list(range(1, total + 1))
        elif total <= 50:
            # Batches m√©dios: log a cada 5 ou marcos importantes
            intervals = [1, 5, 10, 25]
            intervals.extend(range(10, total + 1, 10))
            intervals.append(total)
        elif total <= 200:
            # Batches grandes: log a cada 10 ou marcos importantes
            intervals = [1, 5, 10, 25, 50]
            intervals.extend(range(25, total + 1, 25))
            intervals.append(total)
        elif total <= 1000:
            # Batches muito grandes: log a cada 50 ou marcos importantes
            intervals = [1, 10, 25, 50, 100, 250]
            intervals.extend(range(100, total + 1, 100))
            intervals.append(total)
        else:
            # Batches enormes: log a cada 100 ou marcos importantes
            intervals = [1, 10, 25, 50, 100, 250, 500]
            intervals.extend(range(250, total + 1, 250))
            intervals.append(total)
        
        # Remover duplicatas e ordenar
        intervals = sorted(list(set(intervals)))
        
        # Garantir que n√£o exceda o total
        intervals = [i for i in intervals if i <= total]
        
        return intervals
    
    async def process_batch(self, 
                          texts: List[str], 
                          prompt_template: str,
                          json_schema: Optional[Dict[str, Any]] = None,
                          batch_id: Optional[str] = None,
                          custom_ids: Optional[List[str]] = None,
                          **kwargs) -> Dict[str, Any]:
        """Processa m√∫ltiplos textos em paralelo com progresso otimizado para batches grandes"""
        
        # Usar batch_id customizado ou gerar automaticamente
        batch_id = batch_id if batch_id else f"batch_{int(time.time())}"
        
        # Validar custom_ids se fornecido
        if custom_ids and len(custom_ids) != len(texts):
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(custom_ids)} != {len(texts)}")
        
        # Iniciar tracking do batch no StatsManager
        self.stats_manager.start_batch(batch_id)
        
        # Estimar tokens total do batch
        if texts:
            sample_messages = [{"role": "user", "content": prompt_template.format(text=texts[0])}]
            estimated_tokens_per_request = self.estimate_tokens(sample_messages)
            estimated_total_tokens = estimated_tokens_per_request * len(texts)
        else:
            estimated_tokens_per_request = 0
            estimated_total_tokens = 0
        
        # Calcular intervalos de progresso inteligentes
        progress_intervals = self._calculate_progress_intervals(len(texts))
        
        # Log do in√≠cio do lote
        logger.info(
            "Iniciando processamento em lote",
            extra={
                'batch_id': batch_id,
                'total_texts': len(texts),
                'has_custom_ids': custom_ids is not None,
                'has_json_schema': json_schema is not None,
                'estimated_total_tokens': estimated_total_tokens,
                'estimated_tokens_per_request': estimated_tokens_per_request,
                'tiktoken_available': self._tiktoken_available,
                'progress_intervals_count': len(progress_intervals),
                'action': 'batch_process_start'
            }
        )
        
        # Criar tasks para processamento paralelo
        tasks = []
        for i, text in enumerate(texts):
            # Usar ID customizado se fornecido, sen√£o None (ser√° gerado automaticamente)
            custom_id = custom_ids[i] if custom_ids else None
            
            task = self.process_single(text, prompt_template, json_schema, custom_id, **kwargs)
            tasks.append(task)
        
        # Executar com monitoramento de progresso otimizado
        results = []
        completed = 0
        start_time = time.time()
        
        for coro in asyncio.as_completed(tasks):
            try:
                result = await coro
                results.append(result)
                completed += 1
                
                # Log de progresso baseado nos intervalos calculados
                if completed in progress_intervals:
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    eta = (len(texts) - completed) / rate if rate > 0 else 0
                    
                    # Calcular estat√≠sticas parciais para progresso
                    successful_so_far = sum(1 for r in results if r.get('success', False))
                    failed_so_far = completed - successful_so_far
                    tokens_so_far = sum(r.get('tokens_used', 0) for r in results)
                    cost_so_far = sum(r.get('cost', 0) for r in results)
                    
                    # Determinar n√≠vel de log baseado no progresso
                    progress_percent = (completed / len(texts)) * 100
                    
                    if progress_percent >= 100:
                        log_level = "info"
                    elif completed in [1, 5, 10] or progress_percent in [25, 50, 75]:
                        log_level = "info"
                    else:
                        log_level = "debug"
                    
                    # Preparar mensagem de progresso
                    if len(texts) <= 50:
                        # Batches pequenos: mais detalhes
                        progress_msg = f"Progresso: {completed}/{len(texts)} ({progress_percent:.1f}%) | Taxa: {rate:.1f} req/s"
                    else:
                        # Batches grandes: informa√ß√£o essencial
                        progress_msg = f"Processando: {completed:,}/{len(texts):,} ({progress_percent:.1f}%) | ‚úÖ{successful_so_far} ‚ùå{failed_so_far} | ETA: {eta/60:.1f}min"
                    
                    # Log com n√≠vel apropriado
                    getattr(logger, log_level)(
                        progress_msg,
                        extra={
                            'batch_id': batch_id,
                            'completed': completed,
                            'total': len(texts),
                            'progress_percent': round(progress_percent, 1),
                            'processing_rate': round(rate, 2),
                            'eta_seconds': round(eta, 1),
                            'eta_minutes': round(eta / 60, 1),
                            'successful_so_far': successful_so_far,
                            'failed_so_far': failed_so_far,
                            'tokens_used_so_far': tokens_so_far,
                            'cost_so_far': round(cost_so_far, 4),
                            'action': 'batch_progress'
                        }
                    )
                    
            except Exception as e:
                logger.error(
                    "Erro em task do lote",
                    extra={
                        'batch_id': batch_id,
                        'completed': completed,
                        'error': str(e),
                        'action': 'batch_task_error'
                    }
                )
                # Adicionar resultado de erro
                results.append({
                    'content': None,
                    'tokens_used': 0,
                    'cost': 0.0,
                    'success': False,
                    'error': str(e),
                    'error_type': type(e).__name__
                })
                completed += 1
        
        # Finalizar batch e obter stats espec√≠ficas do batch
        batch_stats = self.stats_manager.end_batch(batch_id)
        
        # Log final do lote com m√©tricas de calibra√ß√£o
        calibration_stats = self.rate_limiter.get_calibration_stats()
        
        logger.info(
            "Processamento em lote conclu√≠do",
            extra={
                'batch_id': batch_id,
                'batch_total_time': round(batch_stats.processing_time, 2),
                'batch_requests': batch_stats.total_requests,
                'batch_successful': batch_stats.successful_requests,
                'batch_failed': batch_stats.failed_requests,
                'batch_tokens': batch_stats.total_tokens,
                'batch_cost': round(batch_stats.total_cost, 4),
                'batch_rate': round(batch_stats.avg_rate, 2),
                'batch_rate_limit_waits': batch_stats.rate_limit_waits,
                'calibration_accuracy': calibration_stats['accuracy_percentage'],
                'calibration_factor': calibration_stats['current_calibration_factor'],
                'action': 'batch_process_complete'
            }
        )
        
        return {
            'results': results,
            'batch_stats': batch_stats,
            'batch_id': batch_id
        }
    
    # M√©todos para acessar stats e controle de calibra√ß√£o
    def get_global_stats(self):
        """Retorna estat√≠sticas globais acumuladas de todos os batches"""
        return self.stats_manager.get_global_stats()
    
    def get_calibration_stats(self):
        """Retorna estat√≠sticas de calibra√ß√£o do rate limiter"""
        return self.rate_limiter.get_calibration_stats()
    
    def show_stats(self, stats_or_result, title="Stats"):
        """Mostra qualquer tipo de estat√≠stica de forma formatada"""
        if isinstance(stats_or_result, dict) and 'batch_stats' in stats_or_result:
            stats = stats_or_result['batch_stats']
        else:
            stats = stats_or_result
        
        print(self.stats_manager.format_stats(stats, title))
    
    def show_calibration_stats(self):
        """Mostra estat√≠sticas de calibra√ß√£o do rate limiter"""
        calibration_stats = self.get_calibration_stats()
        
        print("üéØ ESTAT√çSTICAS DE CALIBRA√á√ÉO:")
        print(f"   üìä Precis√£o geral: {calibration_stats['accuracy_percentage']}%")
        print(f"   üîß Fator de calibra√ß√£o atual: {calibration_stats['current_calibration_factor']}")
        print(f"   üìà Total de calibra√ß√µes: {calibration_stats['total_calibrations']}")
        print(f"   üõ°Ô∏è Rate limits prevenidos: {calibration_stats['prevented_rate_limits']}")
        print(f"   ‚è±Ô∏è Tempo m√©dio de espera: {calibration_stats['efficiency_metrics']['avg_wait_time']}s")
        print(f"   üìä Frequ√™ncia de esperas: {calibration_stats['efficiency_metrics']['wait_frequency']:.3f}")
        
        if calibration_stats['total_calibrations'] > 0:
            print(f"   üìà Hist√≥rico: {calibration_stats['history_size']} amostras")
    
    def reset_stats(self) -> None:
        """Reseta estat√≠sticas globais"""
        logger.info(
            "Resetando estat√≠sticas do AIProcessor",
            extra={'action': 'stats_reset'}
        )
        self.stats_manager.reset_global()
    
    def reset_calibration(self) -> None:
        """Reseta sistema de calibra√ß√£o do rate limiter"""
        logger.info(
            "Resetando calibra√ß√£o do rate limiter",
            extra={'action': 'calibration_reset'}
        )
        self.rate_limiter.reset_calibration()
    
    def disable_calibration(self) -> None:
        """Desabilita o sistema de calibra√ß√£o"""
        logger.info("Desabilitando calibra√ß√£o adaptativa")
        self.rate_limiter.disable_calibration()

    def enable_calibration(self) -> None:
        """Habilita o sistema de calibra√ß√£o"""
        logger.info("Habilitando calibra√ß√£o adaptativa")
        self.rate_limiter.enable_calibration()

    def is_calibration_enabled(self) -> bool:
        """Verifica se a calibra√ß√£o est√° habilitada"""
        return self.rate_limiter.calibration_enabled
    
    def get_rate_limiter_status(self) -> Dict[str, Any]:
        """Retorna status completo do rate limiter"""
        return self.rate_limiter.get_status()

    def show_rate_limiter_status(self):
        """Mostra status atual do rate limiter"""
        status = self.get_rate_limiter_status()
        
        print("‚ö° STATUS DO RATE LIMITER:")
        print(f"   üî¢ Tokens usados: {status['tokens_used']:,}/{status['tokens_limit']:,}")
        print(f"   üìä Utiliza√ß√£o: {status['utilization_percent']:.1f}%")
        print(f"   ‚è±Ô∏è Tempo no minuto: {status['time_in_minute']:.1f}s")
        print(f"   üõ°Ô∏è Rate limits prevenidos: {status['prevented_rate_limits']}")
        print(f"   üéØ Fator de calibra√ß√£o: {status['calibration_factor']}")
    
    def log_final_summary(self) -> None:
        """Log de resumo final para an√°lise"""
        global_stats = self.get_global_stats()
        calibration_stats = self.get_calibration_stats()
        
        if global_stats.total_requests > 0:
            logger.info(
                "Resumo final do AIProcessor",
                extra={
                    'total_requests': global_stats.total_requests,
                    'successful_requests': global_stats.successful_requests,
                    'failed_requests': global_stats.failed_requests,
                    'success_rate_percent': round(global_stats.success_rate, 2),
                    'total_tokens_input': global_stats.total_tokens_input,
                    'total_tokens_output': global_stats.total_tokens_output,
                    'total_tokens_cached': global_stats.total_tokens_cached,
                    'total_tokens': global_stats.total_tokens,
                    'total_cost': round(global_stats.total_cost, 4),
                    'average_cost_per_request': round(global_stats.total_cost / global_stats.successful_requests, 6) if global_stats.successful_requests > 0 else 0,
                    'processing_time': round(global_stats.processing_time, 2),
                    'average_response_time': round(global_stats.avg_response_time, 3),
                    'rate_limit_waits': global_stats.rate_limit_waits,
                    'retry_attempts': global_stats.retry_attempts,
                    'concurrent_peak': global_stats.concurrent_peak,
                    'efficiency_rate': round(global_stats.efficiency_rate, 1),
                    'cache_hit_rate': round(global_stats.cache_hit_rate, 1),
                    
                    # CALIBRATION STATS ADICIONADAS:
                    'calibration_accuracy': calibration_stats['accuracy_percentage'],
                    'calibration_factor': calibration_stats['current_calibration_factor'],
                    'rate_limits_prevented': calibration_stats['prevented_rate_limits'],
                    'calibration_samples': calibration_stats['total_calibrations'],
                    
                    'model_used': self.model,
                    'tiktoken_available': self._tiktoken_available,
                    'action': 'final_summary'
                }
            )
    
    def __del__(self):
        """Log final quando objeto √© destru√≠do"""
        try:
            self.log_final_summary()
        except:
            pass  # Evitar erros durante destrui√ß√£o
