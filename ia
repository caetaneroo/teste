# core/ai_processor.py
import asyncio
import time
import json
import logging
import re
import tiktoken
from typing import List, Dict, Any, Optional
from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI

from rate_limiter import AdaptiveRateLimiter
from stats_manager import StatsManager

logger = logging.getLogger(__name__)

MAX_TPM = 180000
MAX_RETRY = 2

def is_rate_limit_error(result: Dict[str, Any]) -> bool:
    if not isinstance(result, dict):
        return False
    error_msg = result.get('error', '').lower()
    return 'token rate limit' in error_msg

class AIProcessor:
    def __init__(self, config: Dict[str, Any]):
        self.client_id = config['client_id']
        self.client_secret = config['client_secret']
        self.model = config.get('model', 'gpt-35-turbo-16k')
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens')
        self.client = AsyncIaraGenAI(
            client_id=self.client_id,
            client_secret=self.client_secret,
            environment=config.get('environment', 'dev'),
            provider=config.get('provider', 'azure_openai'),
            correlation_id=config.get('correlation_id', 'teste-ia-biatendimento')
        )
        self.max_concurrent = config.get('max_concurrent', 10)
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        max_tpm = config.get('max_tpm', MAX_TPM)
        calibration_enabled = config.get('adaptive_calibration', True)
        self.rate_limiter = AdaptiveRateLimiter(max_tpm, calibration_enabled)
        self.stats_manager = StatsManager()
        self.stats_manager.set_max_concurrent(self.max_concurrent)
        try:
            self.encoder = tiktoken.encoding_for_model(self.model)
        except KeyError:
            self.encoder = tiktoken.get_encoding("cl100k_base")
        logger.info(
            "AIProcessor inicializado com AsyncIaraGenAI",
            extra={
                'model': self.model,
                'max_concurrent': self.max_concurrent,
                'max_tpm': max_tpm,
                'calibration_enabled': calibration_enabled,
                'provider': config.get('provider', 'azure_openai'),
                'environment': config.get('environment', 'dev'),
                'action': 'ai_processor_init'
            }
        )

    def _generate_request_id(self) -> str:
        return f"req_{int(time.time() * 1000)}_{id(self)}"

    def estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        try:
            total_tokens = 0
            for message in messages:
                role_tokens = len(self.encoder.encode(message.get('role', '')))
                content_tokens = len(self.encoder.encode(message.get('content', '')))
                total_tokens += role_tokens + content_tokens + 4
            total_tokens += 50
            return total_tokens
        except Exception as e:
            logger.warning(
                f"Erro na estimativa de tokens: {e}. Usando estimativa conservadora.",
                extra={'action': 'token_estimation_error'}
            )
            total_chars = sum(len(str(msg.get('content', ''))) for msg in messages)
            return int(total_chars * 0.3)

    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        input_cost_per_1k = 0.0015
        output_cost_per_1k = 0.002
        input_cost = (input_tokens / 1000) * input_cost_per_1k
        output_cost = (output_tokens / 1000) * output_cost_per_1k
        return input_cost + output_cost

    def _prepare_json_schema(self, json_schema: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(json_schema, dict):
            return None
        return {
            "type": "json_schema",
            "json_schema": {
                "name": json_schema.get("name", "response_schema"),
                "description": json_schema.get("description", "Schema for structured response"),
                "schema": json_schema.get("schema", json_schema),
                "strict": json_schema.get("strict", True)
            }
        }

    def _extract_wait_time_from_error_result(self, result: Dict[str, Any]) -> float:
        if 'retry_after' in result:
            try:
                return float(result['retry_after'])
            except (ValueError, TypeError):
                pass
        error_msg = result.get('error', '')
        patterns = [
            r'retry after (\d+) seconds',
            r'wait (\d+) seconds',
            r'retry.*?(\d+)\s*seconds?',
            r'(\d+)s',
            r'retry.*?(\d+)',
            r'wait.*?(\d+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, error_msg, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except (ValueError, IndexError):
                    continue
        if 'response_headers' in result:
            headers = result['response_headers']
            if 'retry-after' in headers:
                try:
                    return float(headers['retry-after'])
                except (ValueError, TypeError):
                    pass
        return 60.0

    async def _make_api_call(
        self,
        messages: List[Dict[str, str]],
        json_schema: Optional[Dict[str, Any]] = None,
        request_id: str = None
    ) -> Dict[str, Any]:
        if not request_id:
            request_id = self._generate_request_id()
        if not messages or not isinstance(messages, list):
            return {
                'content': None,
                'tokens_used': 0,
                'success': False,
                'error': 'Messages inválidas',
                'error_type': 'ValidationError',
                'request_id': request_id
            }
        base_estimate = self.estimate_tokens(messages)
        # ✅ Só aguarda se houver rate limit global ativo (não há mais pausa proativa!)
        calibrated_estimate = await self.rate_limiter.wait_for_tokens(base_estimate)
        self.stats_manager.record_concurrent_start()
        try:
            async with self.semaphore:
                async for attempt in AsyncRetrying(
                    stop=stop_after_attempt(MAX_RETRY),
                    wait=wait_fixed(0.1),
                    reraise=True
                ):
                    with attempt:
                        if attempt.retry_state.attempt_number > 1:
                            await self.rate_limiter.wait_for_tokens(base_estimate)
                        start_time = time.time()
                        try:
                            api_params = {
                                "model": self.model,
                                "messages": messages,
                                "temperature": self.temperature
                            }
                            if self.max_tokens is not None:
                                api_params["max_tokens"] = self.max_tokens
                            if json_schema and isinstance(json_schema, dict):
                                api_params["response_format"] = self._prepare_json_schema(json_schema)
                            response = await self.client.chat.completions.create(**api_params)
                            end_time = time.time()
                            api_response_time = end_time - start_time
                            tokens_used = response.usage.total_tokens
                            input_tokens = response.usage.prompt_tokens
                            output_tokens = response.usage.completion_tokens
                            cached_tokens = 0
                            if hasattr(response.usage, 'prompt_tokens_details'):
                                cached_tokens = getattr(response.usage.prompt_tokens_details, 'cached_tokens', 0)
                            # ✅ Não há mais controle local de tokens/minuto
                            self.rate_limiter.record_token_usage(base_estimate, tokens_used)
                            cost = self.calculate_cost(input_tokens, output_tokens)
                            content = response.choices[0].message.content
                            parsed_content = content
                            if json_schema and isinstance(json_schema, dict) and content:
                                try:
                                    parsed_content = json.loads(content)
                                except json.JSONDecodeError:
                                    parsed_content = content
                            return {
                                'content': parsed_content,
                                'raw_content': content,
                                'tokens_used': tokens_used,
                                'input_tokens': input_tokens,
                                'output_tokens': output_tokens,
                                'cached_tokens': cached_tokens,
                                'cost': cost,
                                'api_response_time': api_response_time,
                                'success': True,
                                'is_json': json_schema is not None,
                                'attempts': attempt.retry_state.attempt_number,
                                'request_id': request_id
                            }
                        except Exception as e:
                            end_time = time.time()
                            api_response_time = end_time - start_time
                            response_headers = {}
                            if hasattr(e, 'response') and hasattr(e.response, 'headers'):
                                response_headers = dict(e.response.headers)
                            error_result = {
                                'content': None,
                                'raw_content': None,
                                'tokens_used': 0,
                                'input_tokens': 0,
                                'output_tokens': 0,
                                'cached_tokens': 0,
                                'cost': 0.0,
                                'api_response_time': api_response_time,
                                'success': False,
                                'error': str(e),
                                'error_type': type(e).__name__,
                                'response_headers': response_headers,
                                'is_json': False,
                                'attempts': attempt.retry_state.attempt_number,
                                'request_id': request_id
                            }
                            if is_rate_limit_error(error_result):
                                wait_time = self._extract_wait_time_from_error_result(error_result)
                                raise
                            else:
                                return error_result
        except RetryError as retry_error:
            return {
                'content': None,
                'raw_content': None,
                'tokens_used': 0,
                'input_tokens': 0,
                'output_tokens': 0,
                'cached_tokens': 0,
                'cost': 0.0,
                'api_response_time': 0.0,
                'success': False,
                'error': f'Máximo de tentativas excedido: {retry_error.last_attempt.exception() if retry_error.last_attempt else "Erro desconhecido"}',
                'error_type': 'RetryError',
                'is_json': False,
                'attempts': MAX_RETRY,
                'request_id': request_id
            }
        finally:
            self.stats_manager.record_concurrent_end()

    # ... (restante do código permanece igual, pois não há mais lógica de tokens/minuto local)

