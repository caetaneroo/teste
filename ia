# ai_processor.py
import asyncio
import time
import json
import logging
import re
from typing import List, Dict, Any, Optional
from rate_limiter import AdaptiveRateLimiter
from stats_manager import StatsManager
from openai import AsyncOpenAI
from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed

logger = logging.getLogger(__name__)

def _is_rate_limit_error_from_result(result: Any) -> bool:
    if not isinstance(result, dict) or result.get('success', False):
        return False
    msg = result.get('error', '').lower()
    return any(ind in msg for ind in ['429', 'rate limit', 'ratelimit', 'too many requests', 'azure-openai error', 'quota exceeded'])

def _should_retry_result(result: Any) -> bool:
    if not isinstance(result, dict) or result.get('success', False):
        return False
    msg = result.get('error', '').lower()
    return (
        any(ind in msg for ind in ['429', 'rate limit', 'ratelimit', 'too many requests', 'azure-openai error', 'quota exceeded']) or
        any(ind in msg for ind in ['connection', 'timeout', 'network', '500', '502', '503', '504', 'internal server error'])
    )

def _is_internal_error_from_result(result: Any) -> bool:
    if not isinstance(result, dict) or result.get('success', False):
        return False
    msg = result.get('error', '').lower()
    return (
        'internalerror' in msg or
        'internal error' in msg or
        'service unavailable' in msg or
        'api is down' in msg or
        'server error' in msg or
        'temporarily unavailable' in msg
    )

class AIProcessor:
    def __init__(self, config: Dict[str, Any]):
        self.api_key = config['openai_api_key']
        self.model = config.get('model', 'gpt-3.5-turbo')
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens')
        self.semaphore = asyncio.Semaphore(config.get('max_concurrent', 10))
        self.client = AsyncOpenAI(api_key=self.api_key)
        self.rate_limiter = AdaptiveRateLimiter(config.get('max_tpm', 180000))
        self.stats_manager = StatsManager()
        self._global_error_event = asyncio.Event()

    def estimate_tokens(self, messages: List[Dict[str, str]]) -> int:
        # Dummy implementation; replace with actual token estimation
        return sum(len(m.get('content', '')) // 4 for m in messages) + 10

    def calculate_cost(self, input_tokens, output_tokens):
        # Dummy implementation; replace with actual cost calculation
        return 0.0

    def _prepare_json_schema(self, json_schema):
        return {
            "type": "json_schema",
            "json_schema": {
                "name": json_schema.get("name", "response_schema"),
                "description": json_schema.get("description", "Schema for structured response"),
                "schema": json_schema.get("schema", json_schema),
                "strict": json_schema.get("strict", True)
            }
        }

    def _extract_wait_time_from_error_result(self, result: Dict[str, Any]) -> float:
        error_msg = result.get('error', '')
        # Try to extract from headers if available
        if 'response' in result and hasattr(result['response'], 'headers'):
            retry_after = result['response'].headers.get('Retry-After')
            if retry_after:
                try:
                    return float(retry_after)
                except ValueError:
                    pass
        # Fallback to regex
        patterns = [
            r'retry after (\d+) seconds',
            r'wait (\d+) seconds',
            r'(\d+)s',
            r'retry.*?(\d+)',
            r'wait.*?(\d+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, error_msg, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue
        return 60.0

    async def _make_api_call(
        self,
        messages: List[Dict[str, str]],
        json_schema: Optional[Dict[str, Any]] = None,
        request_id: str = None
    ) -> Dict[str, Any]:
        if not request_id:
            request_id = f"req_{int(time.time() * 1000)}"
        if not messages or not isinstance(messages, list):
            return {
                'content': None,
                'tokens_used': 0,
                'success': False,
                'error': 'Messages inválidas',
                'error_type': 'ValidationError',
                'request_id': request_id
            }
        base_estimate = self.estimate_tokens(messages)
        calibrated_estimate = await self.rate_limiter.wait_for_tokens(base_estimate)
        self.stats_manager.record_concurrent_start()
        try:
            async for attempt in AsyncRetrying(
                stop=stop_after_attempt(5),
                wait=wait_fixed(0.1),
                reraise=True
            ):
                with attempt:
                    if attempt.retry_state.attempt_number > 1:
                        await self.rate_limiter.wait_for_tokens(base_estimate)
                    start_time = time.time()
                    try:
                        api_params = {
                            "model": self.model,
                            "messages": messages,
                            "temperature": self.temperature
                        }
                        if self.max_tokens is not None:
                            api_params["max_tokens"] = self.max_tokens
                        if json_schema and isinstance(json_schema, dict):
                            api_params["response_format"] = self._prepare_json_schema(json_schema)
                        response = await self.client.chat.completions.create(**api_params)
                        end_time = time.time()
                        api_response_time = end_time - start_time
                        tokens_used = response.usage.total_tokens
                        input_tokens = response.usage.prompt_tokens
                        output_tokens = response.usage.completion_tokens
                        cached_tokens = 0
                        if hasattr(response.usage, 'prompt_tokens_details'):
                            cached_tokens = getattr(response.usage.prompt_tokens_details, 'cached_tokens', 0)
                        self.rate_limiter.record_tokens(tokens_used)
                        self.rate_limiter.record_token_usage(base_estimate, tokens_used)
                        cost = self.calculate_cost(input_tokens, output_tokens)
                        content = response.choices[0].message.content
                        parsed_content = content
                        if json_schema and isinstance(json_schema, dict) and content:
                            try:
                                parsed_content = json.loads(content)
                            except Exception:
                                parsed_content = content
                        return {
                            'content': parsed_content,
                            'raw_content': content,
                            'tokens_used': tokens_used,
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'cached_tokens': cached_tokens,
                            'cost': cost,
                            'api_response_time': api_response_time,
                            'success': True,
                            'is_json': json_schema is not None,
                            'attempts': attempt.retry_state.attempt_number,
                            'request_id': request_id
                        }
                    except Exception as e:
                        end_time = time.time()
                        api_response_time = end_time - start_time
                        error_result = {
                            'content': None,
                            'raw_content': None,
                            'tokens_used': 0,
                            'input_tokens': 0,
                            'output_tokens': 0,
                            'cached_tokens': 0,
                            'cost': 0.0,
                            'api_response_time': api_response_time,
                            'success': False,
                            'error': str(e),
                            'error_type': type(e).__name__,
                            'is_json': False,
                            'attempts': attempt.retry_state.attempt_number,
                            'request_id': request_id
                        }
                        # Rate limit and internal errors are handled at batch level for logging
                        if _should_retry_result(error_result):
                            if _is_rate_limit_error_from_result(error_result):
                                logger.debug(
                                    f"Rate limit detectado - request {request_id} - será tratado no batch",
                                    extra={
                                        'request_id': request_id,
                                        'wait_time': self._extract_wait_time_from_error_result(error_result),
                                        'action': 'rate_limit_detected_individual'
                                    }
                                )
                            elif _is_internal_error_from_result(error_result):
                                logger.debug(
                                    f"Erro interno detectado - request {request_id} - será tratado no batch",
                                    extra={
                                        'request_id': request_id,
                                        'error': error_result['error'],
                                        'action': 'internal_error_detected_individual'
                                    }
                                )
                                self._global_error_event.set()
                            raise
                        else:
                            # Only log non-rate limit, non-internal errors at warning level
                            if not _is_rate_limit_error_from_result(error_result) and not _is_internal_error_from_result(error_result):
                                logger.warning(
                                    f"Processamento individual falhou",
                                    extra={
                                        'request_id': request_id,
                                        'error': error_result['error'],
                                        'error_type': error_result['error_type'],
                                        'action': 'single_process_failed'
                                    }
                                )
                            return error_result
        finally:
            self.stats_manager.record_concurrent_end()

    async def process_single(self, text, prompt_template, json_schema=None, custom_id=None, **kwargs):
        prompt = prompt_template.format(text=text, **kwargs)
        messages = [{"role": "user", "content": prompt}]
        request_id = custom_id or f"single_{int(time.time() * 1000)}"
        start = time.time()
        result = await self._make_api_call(messages, json_schema=json_schema, request_id=request_id)
        processing_time = time.time() - start
        # Only log warnings for real failures (not rate limit/internal)
        if result.get('success'):
            logger.info(
                "Processamento individual concluído",
                extra={
                    'request_id': request_id,
                    'processing_time': round(processing_time, 3),
                    'action': 'single_process_success'
                }
            )
        else:
            error_msg = result.get('error', '').lower()
            is_rate_limit = any(indicator in error_msg for indicator in [
                '429', 'rate limit', 'azure-openai error', 'too many requests'
            ])
            is_internal = _is_internal_error_from_result(result)
            if not is_rate_limit and not is_internal:
                logger.warning(
                    "Processamento individual falhou",
                    extra={
                        'request_id': request_id,
                        'processing_time': round(processing_time, 3),
                        'error_type': result.get('error_type'),
                        'action': 'single_process_failed'
                    }
                )
            else:
                logger.debug(
                    f"Request com rate limit ou erro interno - será tratado no batch",
                    extra={
                        'request_id': request_id,
                        'action': 'single_process_rate_limit_or_internal'
                    }
                )
        return result

    async def process_batch(self, texts, prompt_template, json_schema=None, batch_id=None, custom_ids=None, **kwargs):
        batch_id = batch_id if batch_id else f"batch_{int(time.time())}"
        if custom_ids and len(custom_ids) != len(texts):
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(custom_ids)} != {len(texts)}")
        progress_intervals = self._calculate_progress_intervals(len(texts))
        self._last_logged_successes = 0
        self._global_error_event.clear()
        self.rate_limiter.start_batch(batch_id)
        self.stats_manager.start_batch(batch_id)
        logger.info(
            f"🚀 Iniciando processamento em lote - {len(texts)} textos",
            extra={
                'batch_id': batch_id,
                'total_texts': len(texts),
                'has_custom_ids': custom_ids is not None,
                'has_json_schema': json_schema is not None,
                'max_concurrent': self.semaphore._initial_value,
                'action': 'batch_start'
            }
        )
        tasks = []
        for i, text in enumerate(texts):
            custom_id = custom_ids[i] if custom_ids else None
            task = self.process_single(text, prompt_template, json_schema, custom_id, **kwargs)
            tasks.append(task)
        results = []
        completed = 0
        start_time = time.time()
        global_error_logged = False
        for coro in asyncio.as_completed(tasks):
            try:
                result = await coro
                results.append(result)
                completed += 1
                # Rate limit: delegar logs para o rate limiter
                if not result.get('success', False):
                    error_msg = result.get('error', '').lower()
                    is_rate_limit = any(indicator in error_msg for indicator in [
                        '429', 'rate limit', 'azure-openai error', 'too many requests'
                    ])
                    is_internal = _is_internal_error_from_result(result)
                    if is_rate_limit:
                        successful_so_far = sum(1 for r in results if r.get('success', False))
                        failed_so_far = completed - successful_so_far
                        wait_time = self._extract_wait_time_from_error_result(result)
                        self.rate_limiter.record_api_rate_limit_with_context(
                            wait_time, completed, len(texts), successful_so_far, failed_so_far
                        )
                    elif is_internal and not global_error_logged:
                        # Logar erro global apenas uma vez
                        global_error_logged = True
                        successful_so_far = sum(1 for r in results if r.get('success', False))
                        failed_so_far = completed - successful_so_far
                        remaining = len(texts) - completed
                        logger.error(
                            f"❌ Erro crítico detectado em lote - API possivelmente fora do ar. "
                            f"Status: {completed}/{len(texts)} | "
                            f"✅{successful_so_far} ❌{failed_so_far} | "
                            f"🔄{remaining} restantes",
                            extra={
                                'batch_id': batch_id,
                                'completed': completed,
                                'total': len(texts),
                                'successful_so_far': successful_so_far,
                                'failed_so_far': failed_so_far,
                                'remaining': remaining,
                                'error': result.get('error'),
                                'error_type': result.get('error_type'),
                                'action': 'batch_internal_error'
                            }
                        )
                else:
                    self.rate_limiter.record_successful_request()
                # Log de progresso (a cada 5)
                if completed in progress_intervals:
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    eta = (len(texts) - completed) / rate if rate > 0 else 0
                    successful_so_far = sum(1 for r in results if r.get('success', False))
                    failed_so_far = completed - successful_so_far
                    recent_successes = successful_so_far - self._last_logged_successes
                    self._last_logged_successes = successful_so_far
                    logger.info(
                        f"📊 Progresso: {completed}/{len(texts)} ({completed/len(texts)*100:.1f}%) | "
                        f"✅{successful_so_far} ❌{failed_so_far} | "
                        f"🆕{recent_successes} novos | "
                        f"⏱️ETA: {eta/60:.1f}min",
                        extra={
                            'batch_id': batch_id,
                            'completed': completed,
                            'total': len(texts),
                            'successful_so_far': successful_so_far,
                            'failed_so_far': failed_so_far,
                            'recent_successes': recent_successes,
                            'processing_rate': round(rate, 2),
                            'eta_minutes': round(eta / 60, 1),
                            'progress_percent': round((completed / len(texts)) * 100, 1),
                            'action': 'batch_progress'
                        }
                    )
            except Exception as e:
                logger.error(
                    f"Erro em task do lote - continuando...",
                    extra={
                        'batch_id': batch_id,
                        'completed': completed,
                        'error': str(e),
                        'error_type': type(e).__name__,
                        'action': 'batch_task_error'
                    }
                )
                results.append({
                    'content': None,
                    'success': False,
                    'error': str(e),
                    'error_type': type(e).__name__
                })
                completed += 1
        batch_stats = self.stats_manager.end_batch(batch_id)
        logger.info(
            f"✅ Batch concluído - {batch_stats.successful_requests} sucessos, "
            f"{batch_stats.failed_requests} falhas em {batch_stats.processing_time:.1f}s",
            extra={
                'batch_id': batch_id,
                'total_requests': batch_stats.total_requests,
                'successful_requests': batch_stats.successful_requests,
                'failed_requests': batch_stats.failed_requests,
                'processing_time': round(batch_stats.processing_time, 2),
                'total_tokens': batch_stats.total_tokens,
                'total_cost': round(batch_stats.total_cost, 4),
                'avg_rate': round(batch_stats.avg_rate, 2),
                'success_rate': round(batch_stats.success_rate, 1),
                'action': 'batch_complete'
            }
        )
        return {
            'results': results,
            'batch_stats': batch_stats,
            'batch_id': batch_id
        }

    def _calculate_progress_intervals(self, total: int) -> List[int]:
        if total <= 5:
            return list(range(1, total + 1))
        intervals = list(range(5, total + 1, 5))
        if total not in intervals:
            intervals.append(total)
        if 1 not in intervals:
            intervals.insert(0, 1)
        return sorted(intervals)
