# core/stats_manager.py

import time
import logging
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)

# Configuração de preços centralizada
MODEL_PRICING = {
    'gpt-35-turbo-16k': {'input': 0.0015, 'output': 0.002, 'cache': 0.00075},
    'gpt-4': {'input': 0.03, 'output': 0.06, 'cache': 0.015},
    'gpt-4-turbo': {'input': 0.01, 'output': 0.03, 'cache': 0.005},
    'gpt-4o': {'input': 0.005, 'output': 0.015, 'cache': 0.0025},
    'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006, 'cache': 0.000075},
    'o1-mini': {'input': 0.003, 'output': 0.012, 'cache': 0.0015},
    'o1': {'input': 0.015, 'output': 0.06, 'cache': 0.0075},
    'text-embedding-ada-002': {'input': 0.0001, 'output': 0.0, 'cache': 0.00005},
    'o3-mini': {'input': 0.0025, 'output': 0.01, 'cache': 0.00125},
    'gpt-4.5': {'input': 0.008, 'output': 0.024, 'cache': 0.004},
    'gpt-4.1': {'input': 0.007, 'output': 0.021, 'cache': 0.0035},
    'gpt-4.1-mini': {'input': 0.0002, 'output': 0.0008, 'cache': 0.0001},
    'gpt-4.1-nano': {'input': 0.00005, 'output': 0.0002, 'cache': 0.000025},
    'o4-mini': {'input': 0.002, 'output': 0.008, 'cache': 0.001},
    'o3': {'input': 0.01, 'output': 0.04, 'cache': 0.005},
    'text-embedding-3-large': {'input': 0.00013, 'output': 0.0, 'cache': 0.000065},
    'text-embedding-3-small': {'input': 0.00002, 'output': 0.0, 'cache': 0.00001}
}

@dataclass
class Stats:
    """
    Classe unificada para todas as estatísticas (batch e global).
    
    Centraliza todas as métricas para evitar duplicação e inconsistências.
    """
    # Métricas básicas
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    
    # Métricas de tokens
    total_tokens_input: int = 0
    total_tokens_output: int = 0
    total_tokens_cached: int = 0
    
    # Métricas de custo
    total_cost: float = 0.0
    cost_breakdown: Dict[str, float] = field(default_factory=dict)
    
    # Métricas de tempo
    processing_time: float = 0.0
    start_time: float = field(default_factory=time.time)
    total_api_response_time: float = 0.0
    min_response_time: float = float('inf')
    max_response_time: float = 0.0
    
    # Métricas de estimativas (nova)
    total_estimated_tokens: int = 0
    estimation_errors: List[float] = field(default_factory=list)
    
    # Métricas de retry e erro
    retry_attempts: int = 0
    errors_by_type: Dict[str, int] = field(default_factory=dict)
    
    # Métricas de concorrência
    concurrent_peak: int = 0
    
    # Metadados
    model_used: str = ""
    
    @property
    def total_tokens(self) -> int:
        """Total de tokens processados"""
        return self.total_tokens_input + self.total_tokens_output + self.total_tokens_cached
    
    @property
    def success_rate(self) -> float:
        """Taxa de sucesso em percentual"""
        return (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0.0
    
    @property
    def avg_rate(self) -> float:
        """Taxa média de requisições por segundo"""
        return (self.successful_requests / self.processing_time) if self.processing_time > 0 else 0.0
    
    @property
    def avg_response_time(self) -> float:
        """Tempo médio de resposta da API"""
        return (self.total_api_response_time / self.successful_requests) if self.successful_requests > 0 else 0.0
    
    @property
    def efficiency_rate(self) -> float:
        """Taxa de eficiência baseada em throughput vs tempo total"""
        if self.processing_time <= 0:
            return 0.0
        
        # Calcular tempo "útil" baseado em respostas da API
        useful_time = self.total_api_response_time
        
        # Eficiência = tempo útil / tempo total
        return (useful_time / self.processing_time * 100) if self.processing_time > 0 else 0.0
    
    @property
    def retry_rate(self) -> float:
        """Taxa de retry por requisição"""
        return (self.retry_attempts / self.total_requests) if self.total_requests > 0 else 0.0
    
    @property
    def cost_per_token(self) -> float:
        """Custo médio por token"""
        return (self.total_cost / self.total_tokens) if self.total_tokens > 0 else 0.0
    
    @property
    def cost_per_request(self) -> float:
        """Custo médio por requisição"""
        return (self.total_cost / self.successful_requests) if self.successful_requests > 0 else 0.0
    
    @property
    def cache_hit_rate(self) -> float:
        """Taxa de cache hit em percentual"""
        total_input_and_cached = self.total_tokens_input + self.total_tokens_cached
        return (self.total_tokens_cached / total_input_and_cached * 100) if total_input_and_cached > 0 else 0.0
    
    @property
    def estimation_accuracy(self) -> float:
        """Precisão das estimativas de tokens"""
        if not self.estimation_errors:
            return 0.0
        
        # Calcular quantas estimativas estão dentro de 20% do real
        accurate_count = sum(1 for error in self.estimation_errors if abs(error) <= 0.2)
        return (accurate_count / len(self.estimation_errors)) * 100
    
    @property
    def cost_savings_from_cache(self) -> float:
        """Economia real com cache baseada no modelo"""
        if not self.model_used or self.model_used not in MODEL_PRICING:
            return 0.0
        
        pricing = MODEL_PRICING[self.model_used]
        savings_per_token = pricing['input'] - pricing['cache']
        return (self.total_tokens_cached / 1000) * savings_per_token

class StatsManager:
    """
    Gerenciador centralizado de todas as estatísticas.
    
    Responsável por:
    - Coletar e organizar todas as métricas
    - Calcular estatísticas derivadas
    - Gerar relatórios formatados
    - Manter consistência entre batch e global
    """
    
    def __init__(self, model: str = "gpt-4o-mini"):
        self.model = model
        self.global_stats = Stats(model_used=model)
        self._batch_snapshots = {}
        
        # Controle de concorrência
        self._current_concurrent = 0
        self._max_concurrent_ever = 0
        
        logger.debug(f"StatsManager inicializado para modelo: {model}")
    
    def start_batch(self, batch_id: str) -> None:
        """
        Inicia tracking de um batch específico.
        
        Cria um snapshot do estado global atual para depois calcular
        as estatísticas apenas deste batch.
        """
        self._batch_snapshots[batch_id] = {
            'start_time': time.time(),
            'start_stats': Stats(
                total_requests=self.global_stats.total_requests,
                successful_requests=self.global_stats.successful_requests,
                failed_requests=self.global_stats.failed_requests,
                total_tokens_input=self.global_stats.total_tokens_input,
                total_tokens_output=self.global_stats.total_tokens_output,
                total_tokens_cached=self.global_stats.total_tokens_cached,
                total_cost=self.global_stats.total_cost,
                processing_time=0.0,
                start_time=time.time(),
                total_api_response_time=self.global_stats.total_api_response_time,
                min_response_time=self.global_stats.min_response_time,
                max_response_time=self.global_stats.max_response_time,
                total_estimated_tokens=self.global_stats.total_estimated_tokens,
                estimation_errors=self.global_stats.estimation_errors.copy(),
                retry_attempts=self.global_stats.retry_attempts,
                concurrent_peak=0,
                errors_by_type=self.global_stats.errors_by_type.copy(),
                cost_breakdown=self.global_stats.cost_breakdown.copy(),
                model_used=self.model
            )
        }
        
        logger.debug(f"Batch {batch_id} iniciado - snapshot criado")
    
    def end_batch(self, batch_id: str) -> Stats:
        """
        Finaliza batch e retorna estatísticas específicas dele.
        
        Calcula a diferença entre o estado atual e o snapshot inicial
        para obter métricas apenas deste batch.
        """
        if batch_id not in self._batch_snapshots:
            raise ValueError(f"Batch {batch_id} não foi iniciado")
        
        snapshot = self._batch_snapshots[batch_id]
        start_stats = snapshot['start_stats']
        end_time = time.time()
        
        # Calcular estatísticas apenas deste batch
        batch_stats = Stats(
            # Métricas básicas (diferença)
            total_requests=self.global_stats.total_requests - start_stats.total_requests,
            successful_requests=self.global_stats.successful_requests - start_stats.successful_requests,
            failed_requests=self.global_stats.failed_requests - start_stats.failed_requests,
            
            # Métricas de tokens (diferença)
            total_tokens_input=self.global_stats.total_tokens_input - start_stats.total_tokens_input,
            total_tokens_output=self.global_stats.total_tokens_output - start_stats.total_tokens_output,
            total_tokens_cached=self.global_stats.total_tokens_cached - start_stats.total_tokens_cached,
            
            # Métricas de custo (diferença)
            total_cost=self.global_stats.total_cost - start_stats.total_cost,
            
            # Métricas de tempo (específicas do batch)
            processing_time=end_time - snapshot['start_time'],
            start_time=snapshot['start_time'],
            total_api_response_time=self.global_stats.total_api_response_time - start_stats.total_api_response_time,
            min_response_time=self.global_stats.min_response_time if self.global_stats.min_response_time != float('inf') else 0.0,
            max_response_time=self.global_stats.max_response_time,
            
            # Métricas de estimativas (diferença)
            total_estimated_tokens=self.global_stats.total_estimated_tokens - start_stats.total_estimated_tokens,
            estimation_errors=self.global_stats.estimation_errors[len(start_stats.estimation_errors):],
            
            # Métricas de retry (diferença)
            retry_attempts=self.global_stats.retry_attempts - start_stats.retry_attempts,
            
            # Métricas de concorrência (máximo do batch)
            concurrent_peak=self._max_concurrent_ever,
            
            # Erros (diferença)
            errors_by_type={
                error_type: self.global_stats.errors_by_type.get(error_type, 0) - start_stats.errors_by_type.get(error_type, 0)
                for error_type in set(list(self.global_stats.errors_by_type.keys()) + list(start_stats.errors_by_type.keys()))
                if (self.global_stats.errors_by_type.get(error_type, 0) - start_stats.errors_by_type.get(error_type, 0)) > 0
            },
            
            # Breakdown de custos (diferença)
            cost_breakdown={
                model: self.global_stats.cost_breakdown.get(model, 0) - start_stats.cost_breakdown.get(model, 0)
                for model in set(list(self.global_stats.cost_breakdown.keys()) + list(start_stats.cost_breakdown.keys()))
                if (self.global_stats.cost_breakdown.get(model, 0) - start_stats.cost_breakdown.get(model, 0)) > 0
            },
            
            # Metadados
            model_used=self.model
        )
        
        # Limpar snapshot
        del self._batch_snapshots[batch_id]
        
        logger.debug(f"Batch {batch_id} finalizado - estatísticas calculadas")
        
        return batch_stats
    
    def record_request(
        self,
        success: bool,
        tokens_input: int = 0,
        tokens_output: int = 0,
        tokens_cached: int = 0,
        cost: float = 0.0,
        api_response_time: float = 0.0,
        error_type: Optional[str] = None,
        retry_count: int = 0,
        estimated_tokens: int = 0,
        actual_tokens: int = 0
    ) -> None:
        """
        Registra uma requisição com todas as métricas relevantes.
        
        Este é o ponto central de coleta de dados - todas as métricas
        passam por aqui para garantir consistência.
        """
        # Métricas básicas
        self.global_stats.total_requests += 1
        
        if success:
            self.global_stats.successful_requests += 1
            
            # Métricas de tempo (só para sucessos)
            if api_response_time > 0:
                self.global_stats.total_api_response_time += api_response_time
                self.global_stats.min_response_time = min(self.global_stats.min_response_time, api_response_time)
                self.global_stats.max_response_time = max(self.global_stats.max_response_time, api_response_time)
        else:
            self.global_stats.failed_requests += 1
            
            # Registrar tipo de erro
            if error_type:
                if error_type not in self.global_stats.errors_by_type:
                    self.global_stats.errors_by_type[error_type] = 0
                self.global_stats.errors_by_type[error_type] += 1
        
        # Métricas de tokens
        self.global_stats.total_tokens_input += tokens_input
        self.global_stats.total_tokens_output += tokens_output
        self.global_stats.total_tokens_cached += tokens_cached
        
        # Métricas de estimativas
        if estimated_tokens > 0 and actual_tokens > 0:
            self.global_stats.total_estimated_tokens += estimated_tokens
            
            # Calcular erro relativo da estimativa
            error_ratio = (actual_tokens - estimated_tokens) / estimated_tokens
            self.global_stats.estimation_errors.append(error_ratio)
            
            # Manter apenas os últimos 1000 erros para eficiência
            if len(self.global_stats.estimation_errors) > 1000:
                self.global_stats.estimation_errors = self.global_stats.estimation_errors[-1000:]
        
        # Métricas de retry
        self.global_stats.retry_attempts += retry_count
        
        # Métricas de custo
        if cost > 0:
            self.global_stats.total_cost += cost
            
            # Breakdown por modelo
            if self.model not in self.global_stats.cost_breakdown:
                self.global_stats.cost_breakdown[self.model] = 0
            self.global_stats.cost_breakdown[self.model] += cost
        elif tokens_input > 0 or tokens_output > 0:  # Calcular custo se não fornecido
            calculated_cost = self._calculate_cost(tokens_input, tokens_output, tokens_cached)
            self.global_stats.total_cost += calculated_cost
            
            if self.model not in self.global_stats.cost_breakdown:
                self.global_stats.cost_breakdown[self.model] = 0
            self.global_stats.cost_breakdown[self.model] += calculated_cost
        
        # Atualizar pico de concorrência
        self.global_stats.concurrent_peak = max(self.global_stats.concurrent_peak, self._current_concurrent)
    
    def _calculate_cost(self, input_tokens: int, output_tokens: int, cached_tokens: int) -> float:
        """Calcula custo baseado no modelo atual"""
        if self.model not in MODEL_PRICING:
            return 0.0
        
        pricing = MODEL_PRICING[self.model]
        
        regular_input_tokens = max(0, input_tokens - cached_tokens)
        regular_input_cost = (regular_input_tokens / 1000) * pricing['input']
        cached_cost = (cached_tokens / 1000) * pricing['cache']
        output_cost = (output_tokens / 1000) * pricing['output']
        
        return regular_input_cost + cached_cost + output_cost
    
    def record_concurrent_start(self) -> None:
        """Registra início de requisição concorrente"""
        self._current_concurrent += 1
        self._max_concurrent_ever = max(self._max_concurrent_ever, self._current_concurrent)
        self.global_stats.concurrent_peak = self._max_concurrent_ever
    
    def record_concurrent_end(self) -> None:
        """Registra fim de requisição concorrente"""
        self._current_concurrent = max(0, self._current_concurrent - 1)
    
    def get_global_stats(self) -> Stats:
        """Retorna estatísticas globais atualizadas"""
        # Atualizar tempo de processamento
        self.global_stats.processing_time = time.time() - self.global_stats.start_time
        return self.global_stats
    
    def format_stats(self, stats: Stats, title: str = "Estatísticas") -> str:
        """
        Formata estatísticas de forma consistente e legível.
        
        Inclui todas as métricas relevantes organizadas por categoria.
        """
        # Seção básica
        basic_section = f"""📊 {title.upper()}:
✅ Sucessos: {stats.successful_requests:,}
❌ Falhas: {stats.failed_requests:,}
📊 Taxa de sucesso: {stats.success_rate:.1f}%
⏱️ Tempo total: {stats.processing_time:.2f}s
🚀 Taxa: {stats.avg
