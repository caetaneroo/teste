# core/stats_manager.py

import time
import logging
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)

# Configura√ß√£o de pre√ßos centralizada
MODEL_PRICING = {
    'gpt-35-turbo-16k': {'input': 0.0015, 'output': 0.002, 'cache': 0.00075},
    'gpt-4': {'input': 0.03, 'output': 0.06, 'cache': 0.015},
    'gpt-4-turbo': {'input': 0.01, 'output': 0.03, 'cache': 0.005},
    'gpt-4o': {'input': 0.005, 'output': 0.015, 'cache': 0.0025},
    'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006, 'cache': 0.000075},
    'o1-mini': {'input': 0.003, 'output': 0.012, 'cache': 0.0015},
    'o1': {'input': 0.015, 'output': 0.06, 'cache': 0.0075},
    'text-embedding-ada-002': {'input': 0.0001, 'output': 0.0, 'cache': 0.00005},
    'o3-mini': {'input': 0.0025, 'output': 0.01, 'cache': 0.00125},
    'gpt-4.5': {'input': 0.008, 'output': 0.024, 'cache': 0.004},
    'gpt-4.1': {'input': 0.007, 'output': 0.021, 'cache': 0.0035},
    'gpt-4.1-mini': {'input': 0.0002, 'output': 0.0008, 'cache': 0.0001},
    'gpt-4.1-nano': {'input': 0.00005, 'output': 0.0002, 'cache': 0.000025},
    'o4-mini': {'input': 0.002, 'output': 0.008, 'cache': 0.001},
    'o3': {'input': 0.01, 'output': 0.04, 'cache': 0.005},
    'text-embedding-3-large': {'input': 0.00013, 'output': 0.0, 'cache': 0.000065},
    'text-embedding-3-small': {'input': 0.00002, 'output': 0.0, 'cache': 0.00001}
}

@dataclass
class Stats:
    """
    Classe unificada para todas as estat√≠sticas (batch e global).
    
    Centraliza todas as m√©tricas para evitar duplica√ß√£o e inconsist√™ncias.
    """
    # M√©tricas b√°sicas
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    
    # M√©tricas de tokens
    total_tokens_input: int = 0
    total_tokens_output: int = 0
    total_tokens_cached: int = 0
    
    # M√©tricas de custo
    total_cost: float = 0.0
    cost_breakdown: Dict[str, float] = field(default_factory=dict)
    
    # M√©tricas de tempo
    processing_time: float = 0.0
    start_time: float = field(default_factory=time.time)
    total_api_response_time: float = 0.0
    min_response_time: float = float('inf')
    max_response_time: float = 0.0
    
    # M√©tricas de estimativas (nova)
    total_estimated_tokens: int = 0
    estimation_errors: List[float] = field(default_factory=list)
    
    # M√©tricas de retry e erro
    retry_attempts: int = 0
    errors_by_type: Dict[str, int] = field(default_factory=dict)
    
    # M√©tricas de concorr√™ncia
    concurrent_peak: int = 0
    
    # Metadados
    model_used: str = ""
    
    @property
    def total_tokens(self) -> int:
        """Total de tokens processados"""
        return self.total_tokens_input + self.total_tokens_output + self.total_tokens_cached
    
    @property
    def success_rate(self) -> float:
        """Taxa de sucesso em percentual"""
        return (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0.0
    
    @property
    def avg_rate(self) -> float:
        """Taxa m√©dia de requisi√ß√µes por segundo"""
        return (self.successful_requests / self.processing_time) if self.processing_time > 0 else 0.0
    
    @property
    def avg_response_time(self) -> float:
        """Tempo m√©dio de resposta da API"""
        return (self.total_api_response_time / self.successful_requests) if self.successful_requests > 0 else 0.0
    
    @property
    def efficiency_rate(self) -> float:
        """Taxa de efici√™ncia baseada em throughput vs tempo total"""
        if self.processing_time <= 0:
            return 0.0
        
        # Calcular tempo "√∫til" baseado em respostas da API
        useful_time = self.total_api_response_time
        
        # Efici√™ncia = tempo √∫til / tempo total
        return (useful_time / self.processing_time * 100) if self.processing_time > 0 else 0.0
    
    @property
    def retry_rate(self) -> float:
        """Taxa de retry por requisi√ß√£o"""
        return (self.retry_attempts / self.total_requests) if self.total_requests > 0 else 0.0
    
    @property
    def cost_per_token(self) -> float:
        """Custo m√©dio por token"""
        return (self.total_cost / self.total_tokens) if self.total_tokens > 0 else 0.0
    
    @property
    def cost_per_request(self) -> float:
        """Custo m√©dio por requisi√ß√£o"""
        return (self.total_cost / self.successful_requests) if self.successful_requests > 0 else 0.0
    
    @property
    def cache_hit_rate(self) -> float:
        """Taxa de cache hit em percentual"""
        total_input_and_cached = self.total_tokens_input + self.total_tokens_cached
        return (self.total_tokens_cached / total_input_and_cached * 100) if total_input_and_cached > 0 else 0.0
    
    @property
    def estimation_accuracy(self) -> float:
        """Precis√£o das estimativas de tokens"""
        if not self.estimation_errors:
            return 0.0
        
        # Calcular quantas estimativas est√£o dentro de 20% do real
        accurate_count = sum(1 for error in self.estimation_errors if abs(error) <= 0.2)
        return (accurate_count / len(self.estimation_errors)) * 100
    
    @property
    def cost_savings_from_cache(self) -> float:
        """Economia real com cache baseada no modelo"""
        if not self.model_used or self.model_used not in MODEL_PRICING:
            return 0.0
        
        pricing = MODEL_PRICING[self.model_used]
        savings_per_token = pricing['input'] - pricing['cache']
        return (self.total_tokens_cached / 1000) * savings_per_token

class StatsManager:
    """
    Gerenciador centralizado de todas as estat√≠sticas.
    
    Respons√°vel por:
    - Coletar e organizar todas as m√©tricas
    - Calcular estat√≠sticas derivadas
    - Gerar relat√≥rios formatados
    - Manter consist√™ncia entre batch e global
    """
    
    def __init__(self, model: str = "gpt-4o-mini"):
        self.model = model
        self.global_stats = Stats(model_used=model)
        self._batch_snapshots = {}
        
        # Controle de concorr√™ncia
        self._current_concurrent = 0
        self._max_concurrent_ever = 0
        
        logger.debug(f"StatsManager inicializado para modelo: {model}")
    
    def start_batch(self, batch_id: str) -> None:
        """
        Inicia tracking de um batch espec√≠fico.
        
        Cria um snapshot do estado global atual para depois calcular
        as estat√≠sticas apenas deste batch.
        """
        self._batch_snapshots[batch_id] = {
            'start_time': time.time(),
            'start_stats': Stats(
                total_requests=self.global_stats.total_requests,
                successful_requests=self.global_stats.successful_requests,
                failed_requests=self.global_stats.failed_requests,
                total_tokens_input=self.global_stats.total_tokens_input,
                total_tokens_output=self.global_stats.total_tokens_output,
                total_tokens_cached=self.global_stats.total_tokens_cached,
                total_cost=self.global_stats.total_cost,
                processing_time=0.0,
                start_time=time.time(),
                total_api_response_time=self.global_stats.total_api_response_time,
                min_response_time=self.global_stats.min_response_time,
                max_response_time=self.global_stats.max_response_time,
                total_estimated_tokens=self.global_stats.total_estimated_tokens,
                estimation_errors=self.global_stats.estimation_errors.copy(),
                retry_attempts=self.global_stats.retry_attempts,
                concurrent_peak=0,
                errors_by_type=self.global_stats.errors_by_type.copy(),
                cost_breakdown=self.global_stats.cost_breakdown.copy(),
                model_used=self.model
            )
        }
        
        logger.debug(f"Batch {batch_id} iniciado - snapshot criado")
    
    def end_batch(self, batch_id: str) -> Stats:
        """
        Finaliza batch e retorna estat√≠sticas espec√≠ficas dele.
        
        Calcula a diferen√ßa entre o estado atual e o snapshot inicial
        para obter m√©tricas apenas deste batch.
        """
        if batch_id not in self._batch_snapshots:
            raise ValueError(f"Batch {batch_id} n√£o foi iniciado")
        
        snapshot = self._batch_snapshots[batch_id]
        start_stats = snapshot['start_stats']
        end_time = time.time()
        
        # Calcular estat√≠sticas apenas deste batch
        batch_stats = Stats(
            # M√©tricas b√°sicas (diferen√ßa)
            total_requests=self.global_stats.total_requests - start_stats.total_requests,
            successful_requests=self.global_stats.successful_requests - start_stats.successful_requests,
            failed_requests=self.global_stats.failed_requests - start_stats.failed_requests,
            
            # M√©tricas de tokens (diferen√ßa)
            total_tokens_input=self.global_stats.total_tokens_input - start_stats.total_tokens_input,
            total_tokens_output=self.global_stats.total_tokens_output - start_stats.total_tokens_output,
            total_tokens_cached=self.global_stats.total_tokens_cached - start_stats.total_tokens_cached,
            
            # M√©tricas de custo (diferen√ßa)
            total_cost=self.global_stats.total_cost - start_stats.total_cost,
            
            # M√©tricas de tempo (espec√≠ficas do batch)
            processing_time=end_time - snapshot['start_time'],
            start_time=snapshot['start_time'],
            total_api_response_time=self.global_stats.total_api_response_time - start_stats.total_api_response_time,
            min_response_time=self.global_stats.min_response_time if self.global_stats.min_response_time != float('inf') else 0.0,
            max_response_time=self.global_stats.max_response_time,
            
            # M√©tricas de estimativas (diferen√ßa)
            total_estimated_tokens=self.global_stats.total_estimated_tokens - start_stats.total_estimated_tokens,
            estimation_errors=self.global_stats.estimation_errors[len(start_stats.estimation_errors):],
            
            # M√©tricas de retry (diferen√ßa)
            retry_attempts=self.global_stats.retry_attempts - start_stats.retry_attempts,
            
            # M√©tricas de concorr√™ncia (m√°ximo do batch)
            concurrent_peak=self._max_concurrent_ever,
            
            # Erros (diferen√ßa)
            errors_by_type={
                error_type: self.global_stats.errors_by_type.get(error_type, 0) - start_stats.errors_by_type.get(error_type, 0)
                for error_type in set(list(self.global_stats.errors_by_type.keys()) + list(start_stats.errors_by_type.keys()))
                if (self.global_stats.errors_by_type.get(error_type, 0) - start_stats.errors_by_type.get(error_type, 0)) > 0
            },
            
            # Breakdown de custos (diferen√ßa)
            cost_breakdown={
                model: self.global_stats.cost_breakdown.get(model, 0) - start_stats.cost_breakdown.get(model, 0)
                for model in set(list(self.global_stats.cost_breakdown.keys()) + list(start_stats.cost_breakdown.keys()))
                if (self.global_stats.cost_breakdown.get(model, 0) - start_stats.cost_breakdown.get(model, 0)) > 0
            },
            
            # Metadados
            model_used=self.model
        )
        
        # Limpar snapshot
        del self._batch_snapshots[batch_id]
        
        logger.debug(f"Batch {batch_id} finalizado - estat√≠sticas calculadas")
        
        return batch_stats
    
    def record_request(
        self,
        success: bool,
        tokens_input: int = 0,
        tokens_output: int = 0,
        tokens_cached: int = 0,
        cost: float = 0.0,
        api_response_time: float = 0.0,
        error_type: Optional[str] = None,
        retry_count: int = 0,
        estimated_tokens: int = 0,
        actual_tokens: int = 0
    ) -> None:
        """
        Registra uma requisi√ß√£o com todas as m√©tricas relevantes.
        
        Este √© o ponto central de coleta de dados - todas as m√©tricas
        passam por aqui para garantir consist√™ncia.
        """
        # M√©tricas b√°sicas
        self.global_stats.total_requests += 1
        
        if success:
            self.global_stats.successful_requests += 1
            
            # M√©tricas de tempo (s√≥ para sucessos)
            if api_response_time > 0:
                self.global_stats.total_api_response_time += api_response_time
                self.global_stats.min_response_time = min(self.global_stats.min_response_time, api_response_time)
                self.global_stats.max_response_time = max(self.global_stats.max_response_time, api_response_time)
        else:
            self.global_stats.failed_requests += 1
            
            # Registrar tipo de erro
            if error_type:
                if error_type not in self.global_stats.errors_by_type:
                    self.global_stats.errors_by_type[error_type] = 0
                self.global_stats.errors_by_type[error_type] += 1
        
        # M√©tricas de tokens
        self.global_stats.total_tokens_input += tokens_input
        self.global_stats.total_tokens_output += tokens_output
        self.global_stats.total_tokens_cached += tokens_cached
        
        # M√©tricas de estimativas
        if estimated_tokens > 0 and actual_tokens > 0:
            self.global_stats.total_estimated_tokens += estimated_tokens
            
            # Calcular erro relativo da estimativa
            error_ratio = (actual_tokens - estimated_tokens) / estimated_tokens
            self.global_stats.estimation_errors.append(error_ratio)
            
            # Manter apenas os √∫ltimos 1000 erros para efici√™ncia
            if len(self.global_stats.estimation_errors) > 1000:
                self.global_stats.estimation_errors = self.global_stats.estimation_errors[-1000:]
        
        # M√©tricas de retry
        self.global_stats.retry_attempts += retry_count
        
        # M√©tricas de custo
        if cost > 0:
            self.global_stats.total_cost += cost
            
            # Breakdown por modelo
            if self.model not in self.global_stats.cost_breakdown:
                self.global_stats.cost_breakdown[self.model] = 0
            self.global_stats.cost_breakdown[self.model] += cost
        elif tokens_input > 0 or tokens_output > 0:  # Calcular custo se n√£o fornecido
            calculated_cost = self._calculate_cost(tokens_input, tokens_output, tokens_cached)
            self.global_stats.total_cost += calculated_cost
            
            if self.model not in self.global_stats.cost_breakdown:
                self.global_stats.cost_breakdown[self.model] = 0
            self.global_stats.cost_breakdown[self.model] += calculated_cost
        
        # Atualizar pico de concorr√™ncia
        self.global_stats.concurrent_peak = max(self.global_stats.concurrent_peak, self._current_concurrent)
    
    def _calculate_cost(self, input_tokens: int, output_tokens: int, cached_tokens: int) -> float:
        """Calcula custo baseado no modelo atual"""
        if self.model not in MODEL_PRICING:
            return 0.0
        
        pricing = MODEL_PRICING[self.model]
        
        regular_input_tokens = max(0, input_tokens - cached_tokens)
        regular_input_cost = (regular_input_tokens / 1000) * pricing['input']
        cached_cost = (cached_tokens / 1000) * pricing['cache']
        output_cost = (output_tokens / 1000) * pricing['output']
        
        return regular_input_cost + cached_cost + output_cost
    
    def record_concurrent_start(self) -> None:
        """Registra in√≠cio de requisi√ß√£o concorrente"""
        self._current_concurrent += 1
        self._max_concurrent_ever = max(self._max_concurrent_ever, self._current_concurrent)
        self.global_stats.concurrent_peak = self._max_concurrent_ever
    
    def record_concurrent_end(self) -> None:
        """Registra fim de requisi√ß√£o concorrente"""
        self._current_concurrent = max(0, self._current_concurrent - 1)
    
    def get_global_stats(self) -> Stats:
        """Retorna estat√≠sticas globais atualizadas"""
        # Atualizar tempo de processamento
        self.global_stats.processing_time = time.time() - self.global_stats.start_time
        return self.global_stats
    
    def format_stats(self, stats: Stats, title: str = "Estat√≠sticas") -> str:
        """
        Formata estat√≠sticas de forma consistente e leg√≠vel.
        
        Inclui todas as m√©tricas relevantes organizadas por categoria.
        """
        # Se√ß√£o b√°sica
        basic_section = f"""üìä {title.upper()}:
‚úÖ Sucessos: {stats.successful_requests:,}
‚ùå Falhas: {stats.failed_requests:,}
üìä Taxa de sucesso: {stats.success_rate:.1f}%
‚è±Ô∏è Tempo total: {stats.processing_time:.2f}s
üöÄ Taxa: {stats.avg_rate:.1f} req/s
üìà Efici√™ncia: {stats.efficiency_rate:.1f}%"""
        
        # Se√ß√£o de tokens
        tokens_section = f"""
üî¢ TOKENS:
üì• Input: {stats.total_tokens_input:,}
üì§ Output: {stats.total_tokens_output:,}
üíæ Cached: {stats.total_tokens_cached:,} ({stats.cache_hit_rate:.1f}%)
üìä Total: {stats.total_tokens:,}"""
        
        # Se√ß√£o de custos
        cost_section = f"""
üí∞ CUSTOS:
üíµ Total: ${stats.total_cost:.4f}
üìä Por token: ${stats.cost_per_token:.6f}
üìä Por request: ${stats.cost_per_request:.6f}"""
        
        # Adicionar economia com cache se houver
        if stats.total_tokens_cached > 0:
            cost_section += f"\nüí° Economia c/ cache: ${stats.cost_savings_from_cache:.6f}"
        
        # Se√ß√£o de performance
        performance_section = f"""
‚ö° PERFORMANCE:
‚è±Ô∏è Tempo m√©dio API: {stats.avg_response_time:.2f}s
‚è±Ô∏è Tempo m√≠n API: {stats.min_response_time:.2f}s
‚è±Ô∏è Tempo m√°x API: {stats.max_response_time:.2f}s
üîÑ Pico concorr√™ncia: {stats.concurrent_peak}"""
        
        # Se√ß√£o de precis√£o (se houver dados)
        precision_section = ""
        if stats.estimation_errors:
            precision_section = f"""
üéØ PRECIS√ÉO:
üìä Estimativas: {len(stats.estimation_errors):,}
üéØ Precis√£o: {stats.estimation_accuracy:.1f}%"""
        
        # Se√ß√£o de erros (se houver)
        errors_section = ""
        if stats.errors_by_type:
            errors_section = "\n‚ùå ERROS POR TIPO:"
            for error_type, count in stats.errors_by_type.items():
                errors_section += f"\n   {error_type}: {count:,}"
        
        # Se√ß√£o de retry (se houver)
        retry_section = ""
        if stats.retry_attempts > 0:
            retry_section = f"""
üîÑ RETRY:
üîÑ Tentativas: {stats.retry_attempts:,}
üìä Taxa: {stats.retry_rate:.2f} por request"""
        
        # Montar relat√≥rio completo
        report = basic_section + tokens_section + cost_section + performance_section
        
        if precision_section:
            report += precision_section
        
        if retry_section:
            report += retry_section
        
        if errors_section:
            report += errors_section
        
        # Adicionar informa√ß√µes do modelo
        if stats.model_used:
            report += f"\nü§ñ Modelo: {stats.model_used}"
        
        return report
    
    def get_summary_dict(self, stats: Stats) -> Dict[str, Any]:
        """
        Retorna resumo das estat√≠sticas como dicion√°rio.
        
        √ötil para integra√ß√£o com sistemas externos ou an√°lises program√°ticas.
        """
        summary = {
            # B√°sico
            'total_requests': stats.total_requests,
            'successful_requests': stats.successful_requests,
            'failed_requests': stats.failed_requests,
            'success_rate': round(stats.success_rate, 2),
            
            # Tokens
            'tokens_input': stats.total_tokens_input,
            'tokens_output': stats.total_tokens_output,
            'tokens_cached': stats.total_tokens_cached,
            'total_tokens': stats.total_tokens,
            'cache_hit_rate': round(stats.cache_hit_rate, 2),
            
            # Custos
            'total_cost': round(stats.total_cost, 6),
            'cost_per_token': round(stats.cost_per_token, 8),
            'cost_per_request': round(stats.cost_per_request, 6),
            
            # Performance
            'processing_time': round(stats.processing_time, 2),
            'avg_rate': round(stats.avg_rate, 2),
            'efficiency_rate': round(stats.efficiency_rate, 2),
            'avg_response_time': round(stats.avg_response_time, 3),
            'min_response_time': round(stats.min_response_time, 3),
            'max_response_time': round(stats.max_response_time, 3),
            'concurrent_peak': stats.concurrent_peak,
            
            # Precis√£o
            'estimation_accuracy': round(stats.estimation_accuracy, 2),
            'total_estimations': len(stats.estimation_errors),
            
            # Retry
            'retry_attempts': stats.retry_attempts,
            'retry_rate': round(stats.retry_rate, 3),
            
            # Metadados
            'model_used': stats.model_used,
            'errors_by_type': stats.errors_by_type.copy(),
            'cost_breakdown': stats.cost_breakdown.copy()
        }
        
        # Adicionar economia com cache se houver
        if stats.total_tokens_cached > 0:
            summary['cost_savings_from_cache'] = round(stats.cost_savings_from_cache, 6)
        
        return summary
    
    def export_stats_to_csv(self, stats: Stats, filename: str) -> None:
        """
        Exporta estat√≠sticas para CSV.
        
        √ötil para an√°lise em ferramentas externas como Excel ou BI.
        """
        import csv
        
        summary = self.get_summary_dict(stats)
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # Cabe√ßalho
            writer.writerow(['M√©trica', 'Valor'])
            
            # Dados b√°sicos
            writer.writerow(['Total Requests', summary['total_requests']])
            writer.writerow(['Successful Requests', summary['successful_requests']])
            writer.writerow(['Failed Requests', summary['failed_requests']])
            writer.writerow(['Success Rate (%)', summary['success_rate']])
            
            # Tokens
            writer.writerow(['Input Tokens', summary['tokens_input']])
            writer.writerow(['Output Tokens', summary['tokens_output']])
            writer.writerow(['Cached Tokens', summary['tokens_cached']])
            writer.writerow(['Total Tokens', summary['total_tokens']])
            writer.writerow(['Cache Hit Rate (%)', summary['cache_hit_rate']])
            
            # Custos
            writer.writerow(['Total Cost ($)', summary['total_cost']])
            writer.writerow(['Cost per Token ($)', summary['cost_per_token']])
            writer.writerow(['Cost per Request ($)', summary['cost_per_request']])
            
            if 'cost_savings_from_cache' in summary:
                writer.writerow(['Cost Savings from Cache ($)', summary['cost_savings_from_cache']])
            
            # Performance
            writer.writerow(['Processing Time (s)', summary['processing_time']])
            writer.writerow(['Average Rate (req/s)', summary['avg_rate']])
            writer.writerow(['Efficiency Rate (%)', summary['efficiency_rate']])
            writer.writerow(['Avg Response Time (s)', summary['avg_response_time']])
            writer.writerow(['Min Response Time (s)', summary['min_response_time']])
            writer.writerow(['Max Response Time (s)', summary['max_response_time']])
            writer.writerow(['Concurrent Peak', summary['concurrent_peak']])
            
            # Precis√£o
            writer.writerow(['Estimation Accuracy (%)', summary['estimation_accuracy']])
            writer.writerow(['Total Estimations', summary['total_estimations']])
            
            # Retry
            writer.writerow(['Retry Attempts', summary['retry_attempts']])
            writer.writerow(['Retry Rate', summary['retry_rate']])
            
            # Metadados
            writer.writerow(['Model Used', summary['model_used']])
            
            # Erros por tipo
            if summary['errors_by_type']:
                writer.writerow(['--- Errors by Type ---', ''])
                for error_type, count in summary['errors_by_type'].items():
                    writer.writerow([f'Error: {error_type}', count])
            
            # Breakdown de custos
            if summary['cost_breakdown']:
                writer.writerow(['--- Cost Breakdown ---', ''])
                for model, cost in summary['cost_breakdown'].items():
                    writer.writerow([f'Model: {model}', cost])
        
        logger.info(f"Estat√≠sticas exportadas para: {filename}")
    
    def compare_stats(self, stats1: Stats, stats2: Stats, title1: str = "Stats 1", title2: str = "Stats 2") -> str:
        """
        Compara duas estat√≠sticas e retorna relat√≥rio de diferen√ßas.
        
        √ötil para comparar performance entre diferentes batches ou configura√ß√µes.
        """
        def calc_diff(val1, val2):
            if val2 == 0:
                return float('inf') if val1 > 0 else 0
            return ((val1 - val2) / val2) * 100
        
        comparison = f"""üìä COMPARA√á√ÉO: {title1} vs {title2}
        
üìà PERFORMANCE:
üöÄ Taxa (req/s): {stats1.avg_rate:.1f} vs {stats2.avg_rate:.1f} ({calc_diff(stats1.avg_rate, stats2.avg_rate):+.1f}%)
üìà Efici√™ncia (%): {stats1.efficiency_rate:.1f} vs {stats2.efficiency_rate:.1f} ({calc_diff(stats1.efficiency_rate, stats2.efficiency_rate):+.1f}%)
‚úÖ Taxa sucesso (%): {stats1.success_rate:.1f} vs {stats2.success_rate:.1f} ({calc_diff(stats1.success_rate, stats2.success_rate):+.1f}%)

üí∞ CUSTOS:
üíµ Total: ${stats1.total_cost:.4f} vs ${stats2.total_cost:.4f} ({calc_diff(stats1.total_cost, stats2.total_cost):+.1f}%)
üìä Por token: ${stats1.cost_per_token:.6f} vs ${stats2.cost_per_token:.6f} ({calc_diff(stats1.cost_per_token, stats2.cost_per_token):+.1f}%)
üìä Por request: ${stats1.cost_per_request:.6f} vs ${stats2.cost_per_request:.6f} ({calc_diff(stats1.cost_per_request, stats2.cost_per_request):+.1f}%)

üî¢ TOKENS:
üìä Total: {stats1.total_tokens:,} vs {stats2.total_tokens:,} ({calc_diff(stats1.total_tokens, stats2.total_tokens):+.1f}%)
üíæ Cache hit: {stats1.cache_hit_rate:.1f}% vs {stats2.cache_hit_rate:.1f}% ({calc_diff(stats1.cache_hit_rate, stats2.cache_hit_rate):+.1f}%)

‚ö° TEMPOS:
‚è±Ô∏è Processamento: {stats1.processing_time:.2f}s vs {stats2.processing_time:.2f}s ({calc_diff(stats1.processing_time, stats2.processing_time):+.1f}%)
‚è±Ô∏è Resposta m√©dia: {stats1.avg_response_time:.2f}s vs {stats2.avg_response_time:.2f}s ({calc_diff(stats1.avg_response_time, stats2.avg_response_time):+.1f}%)

üéØ PRECIS√ÉO:
üìä Estimativas: {stats1.estimation_accuracy:.1f}% vs {stats2.estimation_accuracy:.1f}% ({calc_diff(stats1.estimation_accuracy, stats2.estimation_accuracy):+.1f}%)"""
        
        return comparison
    
    def reset_stats(self) -> None:
        """
        Reseta todas as estat√≠sticas globais.
        
        √ötil para come√ßar uma nova sess√£o de medi√ß√µes.
        """
        self.global_stats = Stats(model_used=self.model)
        self._current_concurrent = 0
        self._max_concurrent_ever = 0
        self._batch_snapshots.clear()
        
        logger.info("Estat√≠sticas resetadas")
    
    def get_model_pricing(self, model: str) -> Dict[str, float]:
        """
        Retorna pre√ßos do modelo especificado.
        
        √ötil para c√°lculos de custo customizados.
        """
        return MODEL_PRICING.get(model, {'input': 0.0, 'output': 0.0, 'cache': 0.0})
    
    def estimate_cost(self, model: str, input_tokens: int, output_tokens: int, cached_tokens: int = 0) -> float:
        """
        Estima custo para um modelo espec√≠fico.
        
        √ötil para planejamento de or√ßamento.
        """
        if model not in MODEL_PRICING:
            return 0.0
        
        pricing = MODEL_PRICING[model]
        regular_input_tokens = max(0, input_tokens - cached_tokens)
        
        regular_input_cost = (regular_input_tokens / 1000) * pricing['input']
        cached_cost = (cached_tokens / 1000) * pricing['cache']
        output_cost = (output_tokens / 1000) * pricing['output']
        
        return regular_input_cost + cached_cost + output_cost
    
    def get_supported_models(self) -> List[str]:
        """Retorna lista de modelos suportados"""
        return list(MODEL_PRICING.keys())
    
    def __str__(self) -> str:
        """Representa√ß√£o string do StatsManager"""
        return f"StatsManager(model='{self.model}', total_requests={self.global_stats.total_requests})"
    
    def __repr__(self) -> str:
        """Representa√ß√£o detalhada do StatsManager"""
        return (f"StatsManager(model='{self.model}', "
                f"total_requests={self.global_stats.total_requests}, "
                f"successful_requests={self.global_stats.successful_requests}, "
                f"total_cost={self.global_stats.total_cost:.4f})")
