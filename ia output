# core/output_manager.py
import time
import json
import logging
import pandas as pd
import boto3
from typing import List, Dict, Any, Optional, Union
from botocore.exceptions import ClientError
import re
from datetime import datetime

logger = logging.getLogger(__name__)

DEFAULT_DATABASE = 'workspace_db'
DEFAULT_WORKGROUP = 'analytics-workgroup-v3'
DEFAULT_REGION = 'sa-east-1'
MAX_RETRY_ATHENA = 3

class OutputManager:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Inicializa o OutputManager para processar resultados do AIProcessor
        e gerenciar outputs em tabelas AWS.
        
        Args:
            config: Configura√ß√µes opcionais contendo:
                - database: Nome do database (default: workspace_db)
                - workgroup: Nome do workgroup (default: analytics-workgroup-v3)
                - region: Regi√£o AWS (default: sa-east-1)
        """
        if config is None:
            config = {}
            
        self.database = config.get('database', DEFAULT_DATABASE)
        self.workgroup = config.get('workgroup', DEFAULT_WORKGROUP)
        self.region = config.get('region', DEFAULT_REGION)
        
        # Inicializar clientes AWS
        try:
            self.athena_client = boto3.client('athena', region_name=self.region)
            self.s3_client = boto3.client('s3', region_name=self.region)
            self.glue_client = boto3.client('glue', region_name=self.region)
        except Exception as e:
            logger.error(
                f"Erro ao inicializar clientes AWS: {e}",
                extra={'action': 'aws_client_init_error', 'error': str(e)}
            )
            raise
            
        logger.info(
            "OutputManager inicializado",
            extra={
                'database': self.database,
                'workgroup': self.workgroup,
                'region': self.region,
                'action': 'output_manager_init'
            }
        )

    def _generate_operation_id(self) -> str:
        """Gera um ID √∫nico para opera√ß√µes"""
        return f"op_{int(time.time() * 1000)}_{id(self)}"

    def _validate_table_name(self, table_name: str) -> str:
        """Valida e sanitiza nome da tabela"""
        if not table_name or not isinstance(table_name, str):
            raise ValueError("Nome da tabela deve ser uma string n√£o vazia")
        
        # Remove caracteres especiais e converte para lowercase
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', table_name.lower())
        
        # Garante que come√ßa com letra ou underscore
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"table_{sanitized}"
            
        return sanitized

    def _validate_s3_uri(self, s3_uri: str) -> str:
        """Valida URI S3 e garante que termina com /"""
        if not s3_uri or not isinstance(s3_uri, str):
            raise ValueError("URI S3 deve ser uma string n√£o vazia")
        
        if not s3_uri.startswith('s3://'):
            raise ValueError("URI S3 deve come√ßar com 's3://'")
        
        # Garante que termina com /
        if not s3_uri.endswith('/'):
            s3_uri += '/'
            
        return s3_uri

    def _validate_column_name(self, column_name: str) -> str:
        """Valida e sanitiza nome da coluna"""
        if not column_name or not isinstance(column_name, str):
            raise ValueError("Nome da coluna deve ser uma string n√£o vazia")
        
        # Remove caracteres especiais e converte para lowercase
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', column_name.lower())
        
        # Garante que come√ßa com letra ou underscore
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"col_{sanitized}"
            
        return sanitized

    def process_ai_results(
        self, 
        ai_results: Union[Dict[str, Any], List[Dict[str, Any]]],
        custom_id_column_name: str = 'custom_id'
    ) -> pd.DataFrame:
        """
        Processa resultados do AIProcessor e converte para DataFrame
        
        Args:
            ai_results: Resultado(s) do AIProcessor - pode ser:
                - Resultado √∫nico (dict)
                - Lista de resultados (list)
                - Resultado de batch com 'results' (dict com key 'results')
            custom_id_column_name: Nome personalizado para a coluna de ID (default: 'custom_id')
        
        Returns:
            DataFrame com custom_id (ou nome personalizado) e colunas extra√≠das do content JSON
        """
        operation_id = self._generate_operation_id()
        start_time = time.time()
        
        # Valida e sanitiza nome da coluna personalizada
        custom_id_column_name = self._validate_column_name(custom_id_column_name)
        
        try:
            # Normaliza input para lista de resultados
            if isinstance(ai_results, dict):
                if 'results' in ai_results:
                    # Resultado de batch
                    results_list = ai_results['results']
                else:
                    # Resultado √∫nico
                    results_list = [ai_results]
            elif isinstance(ai_results, list):
                results_list = ai_results
            else:
                raise ValueError(f"Tipo de input n√£o suportado: {type(ai_results)}")
            
            processed_data = []
            json_content_count = 0
            text_content_count = 0
            failed_count = 0
            
            for i, result in enumerate(results_list):
                if not isinstance(result, dict):
                    continue
                
                # Extrai informa√ß√µes b√°sicas
                custom_id = result.get('custom_id')
                content = result.get('content')
                success = result.get('success', False)
                
                # Usa o nome personalizado para a coluna de ID
                row_data = {
                    custom_id_column_name: custom_id,
                    'success': success,
                    'processing_time': result.get('processing_time'),
                    'tokens_used': result.get('tokens_used'),
                    'cost': result.get('cost'),
                    'error': result.get('error'),
                    'error_type': result.get('error_type')
                }
                
                if not success:
                    failed_count += 1
                    processed_data.append(row_data)
                    continue
                
                # Processa content baseado no tipo
                if content is None:
                    processed_data.append(row_data)
                elif isinstance(content, dict):
                    # Content √© JSON - expande as chaves como colunas
                    json_content_count += 1
                    for key, value in content.items():
                        # Serializa objetos complexos como JSON string
                        if isinstance(value, (dict, list)):
                            row_data[key] = json.dumps(value, ensure_ascii=False)
                        else:
                            row_data[key] = value
                    processed_data.append(row_data)
                elif isinstance(content, str):
                    # Tenta fazer parse como JSON
                    try:
                        parsed_content = json.loads(content)
                        if isinstance(parsed_content, dict):
                            json_content_count += 1
                            for key, value in parsed_content.items():
                                if isinstance(value, (dict, list)):
                                    row_data[key] = json.dumps(value, ensure_ascii=False)
                                else:
                                    row_data[key] = value
                        else:
                            text_content_count += 1
                            row_data['content_text'] = content
                    except json.JSONDecodeError:
                        # Content √© texto simples
                        text_content_count += 1
                        row_data['content_text'] = content
                    processed_data.append(row_data)
                else:
                    # Outros tipos - converte para string
                    text_content_count += 1
                    row_data['content_text'] = str(content)
                    processed_data.append(row_data)
            
            # Cria DataFrame
            df = pd.DataFrame(processed_data)
            
            # Reordena colunas colocando a coluna de ID primeiro
            if custom_id_column_name in df.columns:
                cols = [custom_id_column_name] + [col for col in df.columns if col != custom_id_column_name]
                df = df[cols]
            
            processing_time = time.time() - start_time
            
            logger.info(
                f"‚úÖ Processamento conclu√≠do - {len(df)} registros, {json_content_count} JSON, {text_content_count} texto, {failed_count} falhas",
                extra={
                    'operation_id': operation_id,
                    'total_records': len(df),
                    'json_content_count': json_content_count,
                    'text_content_count': text_content_count,
                    'failed_count': failed_count,
                    'processing_time': round(processing_time, 2),
                    'columns_count': len(df.columns),
                    'custom_id_column_name': custom_id_column_name,
                    'action': 'process_ai_results_complete'
                }
            )
            
            return df
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(
                f"Erro no processamento: {e}",
                extra={
                    'operation_id': operation_id,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'processing_time': round(processing_time, 2),
                    'action': 'process_ai_results_error'
                }
            )
            raise

    def _execute_athena_query(
        self, 
        query: str, 
        wait_for_completion: bool = True,
        max_wait_time: int = 300
    ) -> Dict[str, Any]:
        """Executa query no Athena"""
        try:
            # Configura par√¢metros da query
            query_params = {
                'QueryString': query,
                'WorkGroup': self.workgroup
            }
            
            # Inicia execu√ß√£o
            response = self.athena_client.start_query_execution(**query_params)
            execution_id = response['QueryExecutionId']
            
            if not wait_for_completion:
                return {
                    'execution_id': execution_id,
                    'status': 'RUNNING',
                    'query': query
                }
            
            # Aguarda conclus√£o
            start_time = time.time()
            while time.time() - start_time < max_wait_time:
                response = self.athena_client.get_query_execution(QueryExecutionId=execution_id)
                status = response['QueryExecution']['Status']['State']
                
                if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                    execution_time = time.time() - start_time
                    
                    if status == 'SUCCEEDED':
                        logger.info(
                            f"‚úÖ Query conclu√≠da em {execution_time:.1f}s",
                            extra={
                                'execution_id': execution_id,
                                'execution_time': round(execution_time, 2),
                                'action': 'athena_query_succeeded'
                            }
                        )
                    else:
                        error_reason = response['QueryExecution']['Status'].get('StateChangeReason', 'Erro desconhecido')
                        logger.error(
                            f"‚ùå Query falhou: {error_reason}",
                            extra={
                                'execution_id': execution_id,
                                'status': status,
                                'error_reason': error_reason,
                                'action': 'athena_query_failed'
                            }
                        )
                    
                    return {
                        'execution_id': execution_id,
                        'status': status,
                        'execution_time': execution_time,
                        'query': query,
                        'response': response
                    }
                
                time.sleep(2)
            
            # Timeout
            return {
                'execution_id': execution_id,
                'status': 'TIMEOUT',
                'execution_time': max_wait_time,
                'query': query
            }
            
        except Exception as e:
            logger.error(
                f"Erro na query Athena: {e}",
                extra={
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'athena_query_error'
                }
            )
            raise

    def _table_exists(self, table_name: str) -> bool:
        """Verifica se a tabela existe no Glue Catalog"""
        try:
            self.glue_client.get_table(
                DatabaseName=self.database,
                Name=table_name
            )
            return True
        except ClientError as e:
            if e.response['Error']['Code'] == 'EntityNotFoundException':
                return False
            raise

    def create_table_from_dataframe(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_location: str,
        file_format: str = 'PARQUET',
        overwrite: bool = False
    ) -> Dict[str, Any]:
        """
        Cria nova tabela a partir de DataFrame
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela a ser criada
            s3_location: URI S3 completa onde os dados da tabela ficar√£o armazenados
            file_format: Formato do arquivo (PARQUET, JSON, CSV)
            overwrite: Se True, sobrescreve tabela existente
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_location = self._validate_s3_uri(s3_location)
        
        logger.info(
            f"üèóÔ∏è Criando tabela {table_name} - {len(df)} registros, {len(df.columns)} colunas",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'rows_count': len(df),
                'columns_count': len(df.columns),
                'file_format': file_format,
                'overwrite': overwrite,
                's3_location': s3_location,
                'action': 'create_table_start'
            }
        )
        
        try:
            # Verifica se tabela existe
            if self._table_exists(table_name):
                if not overwrite:
                    raise ValueError(f"Tabela {table_name} j√° existe. Use overwrite=True para sobrescrever.")
            
            # Gera schema baseado no DataFrame
            columns_ddl = []
            for col_name, dtype in df.dtypes.items():
                if pd.api.types.is_integer_dtype(dtype):
                    sql_type = 'bigint'
                elif pd.api.types.is_float_dtype(dtype):
                    sql_type = 'double'
                elif pd.api.types.is_bool_dtype(dtype):
                    sql_type = 'boolean'
                else:
                    sql_type = 'string'
                
                columns_ddl.append(f"`{col_name}` {sql_type}")
            
            # Monta DDL
            ddl = f"""
            CREATE TABLE IF NOT EXISTS `{self.database}`.`{table_name}` (
                {', '.join(columns_ddl)}
            )
            STORED AS {file_format}
            LOCATION '{s3_location}'
            """
            
            if overwrite:
                # Drop table se existe
                drop_ddl = f"DROP TABLE IF EXISTS `{self.database}`.`{table_name}`"
                self._execute_athena_query(drop_ddl)
            
            # Cria tabela
            result = self._execute_athena_query(ddl)
            
            logger.info(
                f"‚úÖ Tabela {table_name} criada",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'execution_id': result.get('execution_id'),
                    's3_location': s3_location,
                    'action': 'create_table_complete'
                }
            )
            
            return {
                'table_name': table_name,
                'database': self.database,
                's3_location': s3_location,
                'columns_count': len(df.columns),
                'athena_execution': result,
                'operation_id': operation_id
            }
            
        except Exception as e:
            logger.error(
                f"Erro ao criar tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'create_table_error'
                }
            )
            raise

    def append_to_table(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_staging_location: str
    ) -> Dict[str, Any]:
        """
        Adiciona dados do DataFrame a uma tabela existente
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela existente
            s3_staging_location: URI S3 completa para staging dos dados
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_staging_location = self._validate_s3_uri(s3_staging_location)
        
        logger.info(
            f"üìù Adicionando {len(df)} registros √† tabela {table_name}",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'rows_count': len(df),
                's3_staging_location': s3_staging_location,
                'action': 'append_to_table_start'
            }
        )
        
        try:
            # Verifica se tabela existe
            if not self._table_exists(table_name):
                raise ValueError(f"Tabela {table_name} n√£o existe. Crie a tabela primeiro.")
            
            # TODO: Implementar upload do DataFrame para S3 e INSERT INTO
            # Por enquanto, retorna estrutura b√°sica
            
            logger.info(
                f"‚úÖ Dados adicionados √† tabela {table_name}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'rows_added': len(df),
                    'action': 'append_to_table_complete'
                }
            )
            
            return {
                'table_name': table_name,
                'database': self.database,
                'rows_added': len(df),
                'staging_location': s3_staging_location,
                'operation_id': operation_id
            }
            
        except Exception as e:
            logger.error(
                f"Erro ao adicionar dados √† tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'append_to_table_error'
                }
            )
            raise

    def merge_overwrite_by_custom_id(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_staging_location: str,
        custom_id_column: str = 'custom_id'
    ) -> Dict[str, Any]:
        """
        Faz merge/overwrite na tabela baseado no custom_id
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela existente
            s3_staging_location: URI S3 completa para staging dos dados
            custom_id_column: Nome da coluna para fazer o merge (deve existir no DataFrame)
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_staging_location = self._validate_s3_uri(s3_staging_location)
        
        logger.info(
            f"üîÑ Merge overwrite {table_name} por {custom_id_column} - {len(df)} registros",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'merge_column': custom_id_column,
                'rows_count': len(df),
                's3_staging_location': s3_staging_location,
                'action': 'merge_overwrite_start'
            }
        )
        
        try:
            # Verifica se tabela existe
            if not self._table_exists(table_name):
                raise ValueError(f"Tabela {table_name} n√£o existe. Crie a tabela primeiro.")
            
            # Verifica se coluna de merge existe no DataFrame
            if custom_id_column not in df.columns:
                raise ValueError(f"Coluna {custom_id_column} n√£o encontrada no DataFrame")
            
            # TODO: Implementar l√≥gica de merge com ICEBERG ou estrat√©gia de DELETE + INSERT
            
            logger.info(
                f"‚úÖ Merge overwrite conclu√≠do na tabela {table_name}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'rows_processed': len(df),
                    'action': 'merge_overwrite_complete'
                }
            )
            
            return {
                'table_name': table_name,
                'database': self.database,
                'merge_column': custom_id_column,
                'rows_processed': len(df),
                'staging_location': s3_staging_location,
                'operation_id': operation_id
            }
            
        except Exception as e:
            logger.error(
                f"Erro no merge overwrite da tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'merge_overwrite_error'
                }
            )
            raise

    def upsert_table(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_location: str,
        custom_id_column: str = 'custom_id'
    ) -> Dict[str, Any]:
        """
        Faz upsert: merge se tabela existe, sen√£o cria nova tabela
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela
            s3_location: URI S3 completa para os dados da tabela
            custom_id_column: Nome da coluna para fazer o merge (deve existir no DataFrame se tabela existir)
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_location = self._validate_s3_uri(s3_location)
        
        logger.info(
            f"üîÄ Upsert tabela {table_name} - {len(df)} registros",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'rows_count': len(df),
                's3_location': s3_location,
                'action': 'upsert_table_start'
            }
        )
        
        try:
            if self._table_exists(table_name):
                # Tabela existe - faz merge
                return self.merge_overwrite_by_custom_id(
                    df, table_name, s3_location, custom_id_column
                )
            else:
                # Tabela n√£o existe - cria nova
                return self.create_table_from_dataframe(df, table_name, s3_location)
                
        except Exception as e:
            logger.error(
                f"Erro no upsert da tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'upsert_table_error'
                }
            )
            raise
