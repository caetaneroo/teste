import json
import logging
import pandas as pd
import awswrangler as wr
import boto3
from typing import List, Dict, Any, Optional, Union
import re

logger = logging.getLogger(__name__)

DEFAULT_DATABASE = 'workspace_db'
DEFAULT_WORKGROUP = 'analytics-workgroup-v3'
DEFAULT_REGION = 'sa-east-1'

class OutputManager:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        if config is None:
            config = {}
        self.database = config.get('database', DEFAULT_DATABASE)
        self.workgroup = config.get('workgroup', DEFAULT_WORKGROUP)
        self.region = config.get('region', DEFAULT_REGION)
        self.session = boto3.Session(region_name=self.region)

    def _validate_table_name(self, table_name: str) -> str:
        if not table_name or not isinstance(table_name, str):
            raise ValueError("Nome da tabela deve ser uma string não vazia")
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', table_name.lower())
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"table_{sanitized}"
        return sanitized

    def _validate_column_name(self, column_name: str) -> str:
        if not column_name or not isinstance(column_name, str):
            raise ValueError("Nome da coluna deve ser uma string não vazia")
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', column_name.lower())
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"col_{sanitized}"
        return sanitized

    def _validate_s3_path(self, s3_path: str) -> str:
        """Valida e normaliza o caminho S3"""
        if not s3_path or not isinstance(s3_path, str):
            raise ValueError("Caminho S3 deve ser uma string não vazia")
        if not s3_path.startswith('s3://'):
            raise ValueError("Caminho S3 deve começar com 's3://'")
        return s3_path.rstrip('/') + '/'

    def process_ai_results(
        self, 
        ai_results: Union[Dict[str, Any], List[Dict[str, Any]]],
        id_column_name: str = 'id',
        include_content_text: bool = True
    ) -> pd.DataFrame:
        """
        Processa resultados do AIProcessor e retorna DataFrame.
        Apenas sucessos são processados.
        """
        id_column_name = self._validate_column_name(id_column_name)
        
        if isinstance(ai_results, dict):
            if 'results' in ai_results:
                results = ai_results['results']
            else:
                results = [ai_results]
        elif isinstance(ai_results, list):
            results = ai_results
        else:
            raise ValueError(f"Tipo de input não suportado: {type(ai_results)}")

        processed = []
        success_count = 0
        result_columns = set()
        
        for r in results:
            if not r.get('success', False):
                continue
                
            success_count += 1
            row = {
                id_column_name: r.get('id'),
                'processing_time': r.get('processing_time'),
                'input_tokens': r.get('input_tokens'),
                'output_tokens': r.get('output_tokens'),
                'cached_tokens': r.get('cached_tokens'),
                'tokens_used': r.get('tokens_used'),
                'cost': r.get('cost')
            }
            
            content = r.get('content')
            if content is None:
                if include_content_text:
                    processed.append(row)
                continue
                
            if isinstance(content, dict):
                for k, v in content.items():
                    result_columns.add(k)
                    key = f'result_{k}'
                    if isinstance(v, (dict, list)):
                        row[key] = json.dumps(v, ensure_ascii=False)
                    else:
                        row[key] = v
                processed.append(row)
            elif isinstance(content, str):
                try:
                    parsed = json.loads(content)
                    if isinstance(parsed, dict):
                        for k, v in parsed.items():
                            result_columns.add(k)
                            key = f'result_{k}'
                            if isinstance(v, (dict, list)):
                                row[key] = json.dumps(v, ensure_ascii=False)
                            else:
                                row[key] = v
                        processed.append(row)
                    else:
                        if include_content_text:
                            row['content_text'] = content
                            processed.append(row)
                except json.JSONDecodeError:
                    if include_content_text:
                        row['content_text'] = content
                        processed.append(row)
            else:
                if include_content_text:
                    row['content_text'] = str(content)
                    processed.append(row)

        df = pd.DataFrame(processed)
        if id_column_name in df.columns:
            cols = [id_column_name] + [c for c in df.columns if c != id_column_name]
            df = df[cols]
        
        # Log único com informações detalhadas
        result_cols_str = ', '.join(sorted(result_columns)) if result_columns else 'nenhuma'
        logger.info(f"AI Results processados: {success_count} sucessos, DataFrame com {len(df)} linhas, colunas de resultado: {result_cols_str}")
        
        return df

    def create_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Cria tabela Iceberg usando funções nativas do awswrangler.
        """
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        
        # Garante que o database existe
        wr.catalog.create_database(database, exist_ok=True, boto3_session=self.session)
        
        # Cria tabela Iceberg usando função específica
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            partition_cols=partition_cols,
            mode='overwrite',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        logger.info(f"Tabela Iceberg {database}.{table_name} criada com {len(df)} registros em {s3_path}")

    def append_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Adiciona dados em tabela Iceberg existente.
        """
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} não existe. Use create_table primeiro.")
        
        # Append usando função Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            partition_cols=partition_cols,
            mode='append',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        logger.info(f"Adicionados {len(df)} registros à tabela Iceberg {database}.{table_name}")

    def upsert_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        merge_keys: Optional[List[str]] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Upsert usando merge nativo do Iceberg.
        """
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        
        # Se a tabela não existe, cria
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            self.create_table(df, table_name, s3_path, database, partition_cols)
            return
        
        # Define chaves de merge (usa 'id' como padrão se não especificado)
        if merge_keys is None:
            merge_keys = ['id']
        
        # Verifica se as chaves de merge existem
        for key in merge_keys:
            if key not in df.columns:
                raise ValueError(f"Chave de merge '{key}' não encontrada no DataFrame")
        
        # Upsert usando merge do Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            merge_cols=merge_keys,
            partition_cols=partition_cols,
            mode='upsert',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        logger.info(f"Upsert realizado em tabela Iceberg {database}.{table_name} com {len(df)} registros usando chaves: {merge_keys}")

    def merge_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        merge_keys: List[str],
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Merge usando capacidades nativas do Iceberg.
        """
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} não existe. Use create_table primeiro.")
        
        # Verifica se as chaves de merge existem
        for key in merge_keys:
            if key not in df.columns:
                raise ValueError(f"Chave de merge '{key}' não encontrada no DataFrame")
        
        # Merge usando função Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            merge_cols=merge_keys,
            partition_cols=partition_cols,
            mode='upsert',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        logger.info(f"Merge realizado em tabela Iceberg {database}.{table_name} com {len(df)} registros usando chaves: {merge_keys}")

    def overwrite_partitions(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Sobrescreve partições específicas da tabela Iceberg.
        """
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} não existe. Use create_table primeiro.")
        
        # Sobrescreve partições usando função Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            partition_cols=partition_cols,
            mode='overwrite_partitions',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        logger.info(f"Partições sobrescritas na tabela Iceberg {database}.{table_name} com {len(df)} registros")

    def delete_table(
        self,
        table_name: str,
        database: Optional[str] = None,
        delete_s3_data: bool = False
    ) -> None:
        """
        Remove tabela Iceberg do catálogo e opcionalmente os dados do S3.
        """
        table_name = self._validate_table_name(table_name)
        database = database or self.database
        
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            logger.warning(f"Tabela {database}.{table_name} não existe")
            return
        
        # Remove a tabela do catálogo
        wr.catalog.delete_table_if_exists(
            database=database,
            table=table_name,
            boto3_session=self.session
        )
        
        # Opcionalmente remove os dados do S3
        if delete_s3_data:
            try:
                table_location = wr.catalog.get_table_location(
                    database=database,
                    table=table_name,
                    boto3_session=self.session
                )
                wr.s3.delete_objects(path=table_location, boto3_session=self.session)
                logger.info(f"Tabela Iceberg {database}.{table_name} e dados S3 removidos")
            except Exception:
                logger.info(f"Tabela Iceberg {database}.{table_name} removida do catálogo")
        else:
            logger.info(f"Tabela Iceberg {database}.{table_name} removida do catálogo")

    def read_table(
        self,
        table_name: str,
        database: Optional[str] = None,
        columns: Optional[List[str]] = None,
        filters: Optional[List[List[str]]] = None
    ) -> pd.DataFrame:
        """
        Lê dados de uma tabela Iceberg.
        """
        table_name = self._validate_table_name(table_name)
        database = database or self.database
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} não existe")
        
        # Lê a tabela usando função Iceberg
        df = wr.athena.read_sql_table(
            table=table_name,
            database=database,
            columns=columns,
            filters=filters,
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        logger.info(f"Lidos {len(df)} registros da tabela Iceberg {database}.{table_name}")
        return df
