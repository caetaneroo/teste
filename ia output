import time
import json
import logging
import pandas as pd
import boto3
import pyarrow as pa
import pyarrow.parquet as pq
from typing import List, Dict, Any, Optional, Union
from botocore.exceptions import ClientError
import re
import os
from datetime import datetime
from uuid import uuid4

logger = logging.getLogger(__name__)

DEFAULT_DATABASE = 'workspace_db'
DEFAULT_WORKGROUP = 'analytics-workgroup-v3'
DEFAULT_REGION = 'sa-east-1'

class OutputManager:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        if config is None:
            config = {}
        self.database = config.get('database', DEFAULT_DATABASE)
        self.workgroup = config.get('workgroup', DEFAULT_WORKGROUP)
        self.region = config.get('region', DEFAULT_REGION)
        try:
            self.athena_client = boto3.client('athena', region_name=self.region)
            self.s3_client = boto3.client('s3', region_name=self.region)
            self.glue_client = boto3.client('glue', region_name=self.region)
        except Exception as e:
            logger.error(f"Erro ao inicializar clientes AWS: {e}")
            raise

    def _generate_operation_id(self) -> str:
        return f"op_{int(time.time() * 1000)}_{id(self)}"

    def _validate_table_name(self, table_name: str) -> str:
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', table_name.lower())
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"table_{sanitized}"
        return sanitized

    def _validate_s3_uri(self, s3_uri: str) -> str:
        if not s3_uri.startswith('s3://'):
            raise ValueError("URI S3 deve começar com 's3://'")
        if not s3_uri.endswith('/'):
            s3_uri += '/'
        return s3_uri

    def _table_exists(self, table_name: str) -> bool:
        try:
            self.glue_client.get_table(DatabaseName=self.database, Name=table_name)
            return True
        except ClientError as e:
            if e.response['Error']['Code'] == 'EntityNotFoundException':
                return False
            raise

    def _upload_parquet_to_s3(self, df: pd.DataFrame, s3_uri: str, filename_prefix: str = None) -> str:
        # Gera arquivo Parquet local temporário
        if filename_prefix is None:
            filename_prefix = f"part-{uuid4().hex}"
        local_file = f"/tmp/{filename_prefix}.parquet"
        table = pa.Table.from_pandas(df)
        pq.write_table(table, local_file)
        # Extrai bucket e prefixo
        s3_uri = self._validate_s3_uri(s3_uri)
        bucket = s3_uri[5:].split('/')[0]
        prefix = '/'.join(s3_uri[5:].split('/')[1:])
        s3_key = f"{prefix}{filename_prefix}.parquet"
        self.s3_client.upload_file(local_file, bucket, s3_key)
        os.remove(local_file)
        return f"s3://{bucket}/{s3_key}"

    def create_table_from_dataframe(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_location: str,
        file_format: str = 'ICEBERG',
        overwrite: bool = False,
        partition_cols: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Cria tabela e grava os dados no S3. Suporta ICEBERG (preferencial), fallback para PARQUET.
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_location = self._validate_s3_uri(s3_location)
        if file_format.upper() not in ['ICEBERG', 'PARQUET']:
            raise ValueError("Formato suportado: ICEBERG ou PARQUET")
        # Apaga tabela se overwrite
        if overwrite and self._table_exists(table_name):
            drop_ddl = f"DROP TABLE IF EXISTS `{self.database}`.`{table_name}`"
            self._execute_athena_query(drop_ddl)
        # Grava arquivo Parquet no S3 (mesmo para Iceberg, para ingestão inicial)
        self._upload_parquet_to_s3(df, s3_location)
        # Monta DDL
        columns_ddl = []
        for col_name, dtype in df.dtypes.items():
            if pd.api.types.is_integer_dtype(dtype):
                sql_type = 'bigint'
            elif pd.api.types.is_float_dtype(dtype):
                sql_type = 'double'
            elif pd.api.types.is_bool_dtype(dtype):
                sql_type = 'boolean'
            else:
                sql_type = 'string'
            columns_ddl.append(f"`{col_name}` {sql_type}")
        if file_format.upper() == 'ICEBERG':
            tbl_stmt = "CREATE TABLE"
            tbl_type = "ICEBERG"
        else:
            tbl_stmt = "CREATE EXTERNAL TABLE"
            tbl_type = "PARQUET"
        partition_stmt = ""
        if partition_cols:
            partition_stmt = f"PARTITIONED BY ({', '.join([f'`{col}` string' for col in partition_cols])})"
        ddl = f"""
        {tbl_stmt} IF NOT EXISTS `{self.database}`.`{table_name}` (
            {', '.join(columns_ddl)}
        )
        {partition_stmt}
        STORED AS {tbl_type}
        LOCATION '{s3_location}'
        """
        self._execute_athena_query(ddl)
        logger.info(f"✅ Tabela {table_name} criada/atualizada em {s3_location}")
        return {
            'table_name': table_name,
            'database': self.database,
            's3_location': s3_location,
            'columns_count': len(df.columns),
            'operation_id': operation_id
        }

    def append_to_table(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_location: str
    ) -> Dict[str, Any]:
        """
        Faz append dos dados no S3 da tabela. Para Iceberg, Athena detecta automaticamente.
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_location = self._validate_s3_uri(s3_location)
        if not self._table_exists(table_name):
            raise ValueError(f"Tabela {table_name} não existe.")
        self._upload_parquet_to_s3(df, s3_location)
        # Para Iceberg, não precisa MSCK REPAIR TABLE; para Parquet sim.
        logger.info(f"✅ Append realizado em {table_name}")
        return {
            'table_name': table_name,
            'database': self.database,
            'rows_added': len(df),
            's3_location': s3_location,
            'operation_id': operation_id
        }

    def upsert_table(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_location: str,
        custom_id_column: str = 'id',
        file_format: str = 'ICEBERG'
    ) -> Dict[str, Any]:
        """
        Upsert real: cria se não existe, senão faz merge (apaga linhas antigas com mesmo id e adiciona novas).
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_location = self._validate_s3_uri(s3_location)
        if not self._table_exists(table_name):
            return self.create_table_from_dataframe(
                df, table_name, s3_location, file_format=file_format, overwrite=True
            )
        # Para Iceberg: faz merge overwrite pelo id
        temp_prefix = f"tmp_upsert_{uuid4().hex}/"
        temp_s3 = s3_location + temp_prefix
        parquet_uri = self._upload_parquet_to_s3(df, temp_s3)
        merge_sql = f"""
        MERGE INTO `{self.database}`.`{table_name}` t
        USING (SELECT * FROM '{parquet_uri}') s
        ON t.{custom_id_column} = s.{custom_id_column}
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
        """
        self._execute_athena_query(merge_sql)
        logger.info(f"✅ Upsert realizado em {table_name}")
        return {
            'table_name': table_name,
            'database': self.database,
            'rows_upserted': len(df),
            's3_location': s3_location,
            'operation_id': operation_id
        }

    def _execute_athena_query(
        self, 
        query: str, 
        wait_for_completion: bool = True,
        max_wait_time: int = 300
    ) -> Dict[str, Any]:
        try:
            query_params = {
                'QueryString': query,
                'WorkGroup': self.workgroup
            }
            response = self.athena_client.start_query_execution(**query_params)
            execution_id = response['QueryExecutionId']
            if not wait_for_completion:
                return {
                    'execution_id': execution_id,
                    'status': 'RUNNING',
                    'query': query
                }
            start_time = time.time()
            while time.time() - start_time < max_wait_time:
                response = self.athena_client.get_query_execution(QueryExecutionId=execution_id)
                status = response['QueryExecution']['Status']['State']
                if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                    return {
                        'execution_id': execution_id,
                        'status': status,
                        'query': query,
                        'response': response
                    }
                time.sleep(2)
            return {
                'execution_id': execution_id,
                'status': 'TIMEOUT',
                'query': query
            }
        except Exception as e:
            logger.error(f"Erro na query Athena: {e}")
            raise
