# core/output_manager.py
import time
import json
import logging
import pandas as pd
import boto3
from typing import List, Dict, Any, Optional, Union
from botocore.exceptions import ClientError
import re
from datetime import datetime

logger = logging.getLogger(__name__)

DEFAULT_DATABASE = 'workspace_db'
DEFAULT_WORKGROUP = 'analytics-workgroup-v3'
DEFAULT_REGION = 'sa-east-1'
MAX_RETRY_ATHENA = 3

class OutputManager:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Inicializa o OutputManager para processar resultados do AIProcessor
        e gerenciar outputs em tabelas AWS.
        
        Args:
            config: Configura√ß√µes opcionais contendo:
                - database: Nome do database (default: workspace_db)
                - workgroup: Nome do workgroup (default: analytics-workgroup-v3)
                - region: Regi√£o AWS (default: sa-east-1)
        """
        if config is None:
            config = {}
            
        self.database = config.get('database', DEFAULT_DATABASE)
        self.workgroup = config.get('workgroup', DEFAULT_WORKGROUP)
        self.region = config.get('region', DEFAULT_REGION)
        
        # Inicializar clientes AWS
        try:
            self.athena_client = boto3.client('athena', region_name=self.region)
            self.s3_client = boto3.client('s3', region_name=self.region)
            self.glue_client = boto3.client('glue', region_name=self.region)
        except Exception as e:
            logger.error(
                f"Erro ao inicializar clientes AWS: {e}",
                extra={'action': 'aws_client_init_error', 'error': str(e)}
            )
            raise
            
        logger.info(
            "OutputManager inicializado com sucesso",
            extra={
                'database': self.database,
                'workgroup': self.workgroup,
                'region': self.region,
                'action': 'output_manager_init'
            }
        )

    def _generate_operation_id(self) -> str:
        """Gera um ID √∫nico para opera√ß√µes"""
        return f"op_{int(time.time() * 1000)}_{id(self)}"

    def _validate_table_name(self, table_name: str) -> str:
        """Valida e sanitiza nome da tabela"""
        if not table_name or not isinstance(table_name, str):
            raise ValueError("Nome da tabela deve ser uma string n√£o vazia")
        
        # Remove caracteres especiais e converte para lowercase
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', table_name.lower())
        
        # Garante que come√ßa com letra ou underscore
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"table_{sanitized}"
            
        return sanitized

    def _validate_s3_uri(self, s3_uri: str) -> str:
        """Valida URI S3 e garante que termina com /"""
        if not s3_uri or not isinstance(s3_uri, str):
            raise ValueError("URI S3 deve ser uma string n√£o vazia")
        
        if not s3_uri.startswith('s3://'):
            raise ValueError("URI S3 deve come√ßar com 's3://'")
        
        # Garante que termina com /
        if not s3_uri.endswith('/'):
            s3_uri += '/'
            
        return s3_uri

    def _extract_json_columns(self, json_data: Dict[str, Any]) -> Dict[str, str]:
        """
        Extrai colunas de um objeto JSON e infere tipos b√°sicos
        
        Returns:
            Dict com nome_coluna: tipo_sql
        """
        columns = {}
        
        for key, value in json_data.items():
            # Sanitiza nome da coluna
            col_name = re.sub(r'[^a-zA-Z0-9_]', '_', str(key).lower())
            
            # Infere tipo baseado no valor
            if value is None:
                col_type = 'string'
            elif isinstance(value, bool):
                col_type = 'boolean'
            elif isinstance(value, int):
                col_type = 'bigint'
            elif isinstance(value, float):
                col_type = 'double'
            elif isinstance(value, (list, dict)):
                col_type = 'string'  # Ser√° serializado como JSON
            else:
                col_type = 'string'
                
            columns[col_name] = col_type
            
        return columns

    def process_ai_results(
        self, 
        ai_results: Union[Dict[str, Any], List[Dict[str, Any]]]
    ) -> pd.DataFrame:
        """
        Processa resultados do AIProcessor e converte para DataFrame
        
        Args:
            ai_results: Resultado(s) do AIProcessor - pode ser:
                - Resultado √∫nico (dict)
                - Lista de resultados (list)
                - Resultado de batch com 'results' (dict com key 'results')
        
        Returns:
            DataFrame com custom_id e colunas extra√≠das do content JSON
        """
        operation_id = self._generate_operation_id()
        start_time = time.time()
        
        logger.info(
            "üîÑ Iniciando processamento de resultados AI",
            extra={
                'operation_id': operation_id,
                'input_type': type(ai_results).__name__,
                'action': 'process_ai_results_start'
            }
        )
        
        try:
            # Normaliza input para lista de resultados
            if isinstance(ai_results, dict):
                if 'results' in ai_results:
                    # Resultado de batch
                    results_list = ai_results['results']
                else:
                    # Resultado √∫nico
                    results_list = [ai_results]
            elif isinstance(ai_results, list):
                results_list = ai_results
            else:
                raise ValueError(f"Tipo de input n√£o suportado: {type(ai_results)}")
            
            processed_data = []
            json_content_count = 0
            text_content_count = 0
            failed_count = 0
            
            for i, result in enumerate(results_list):
                if not isinstance(result, dict):
                    logger.warning(
                        f"Resultado {i} n√£o √© um dict, pulando",
                        extra={'operation_id': operation_id, 'index': i}
                    )
                    continue
                
                # Extrai informa√ß√µes b√°sicas
                custom_id = result.get('custom_id')
                content = result.get('content')
                success = result.get('success', False)
                
                row_data = {
                    'custom_id': custom_id,
                    'success': success,
                    'processing_time': result.get('processing_time'),
                    'tokens_used': result.get('tokens_used'),
                    'cost': result.get('cost'),
                    'error': result.get('error'),
                    'error_type': result.get('error_type')
                }
                
                if not success:
                    failed_count += 1
                    processed_data.append(row_data)
                    continue
                
                # Processa content baseado no tipo
                if content is None:
                    processed_data.append(row_data)
                elif isinstance(content, dict):
                    # Content √© JSON - expande as chaves como colunas
                    json_content_count += 1
                    for key, value in content.items():
                        # Serializa objetos complexos como JSON string
                        if isinstance(value, (dict, list)):
                            row_data[key] = json.dumps(value, ensure_ascii=False)
                        else:
                            row_data[key] = value
                    processed_data.append(row_data)
                elif isinstance(content, str):
                    # Tenta fazer parse como JSON
                    try:
                        parsed_content = json.loads(content)
                        if isinstance(parsed_content, dict):
                            json_content_count += 1
                            for key, value in parsed_content.items():
                                if isinstance(value, (dict, list)):
                                    row_data[key] = json.dumps(value, ensure_ascii=False)
                                else:
                                    row_data[key] = value
                        else:
                            text_content_count += 1
                            row_data['content_text'] = content
                    except json.JSONDecodeError:
                        # Content √© texto simples
                        text_content_count += 1
                        row_data['content_text'] = content
                    processed_data.append(row_data)
                else:
                    # Outros tipos - converte para string
                    text_content_count += 1
                    row_data['content_text'] = str(content)
                    processed_data.append(row_data)
            
            # Cria DataFrame
            df = pd.DataFrame(processed_data)
            
            # Reordena colunas colocando custom_id primeiro
            if 'custom_id' in df.columns:
                cols = ['custom_id'] + [col for col in df.columns if col != 'custom_id']
                df = df[cols]
            
            processing_time = time.time() - start_time
            
            logger.info(
                f"‚úÖ Processamento conclu√≠do - {len(df)} registros processados",
                extra={
                    'operation_id': operation_id,
                    'total_records': len(df),
                    'json_content_count': json_content_count,
                    'text_content_count': text_content_count,
                    'failed_count': failed_count,
                    'processing_time': round(processing_time, 2),
                    'columns_count': len(df.columns),
                    'action': 'process_ai_results_complete'
                }
            )
            
            return df
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(
                f"Erro no processamento de resultados AI: {e}",
                extra={
                    'operation_id': operation_id,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'processing_time': round(processing_time, 2),
                    'action': 'process_ai_results_error'
                }
            )
            raise

    def _execute_athena_query(
        self, 
        query: str, 
        wait_for_completion: bool = True,
        max_wait_time: int = 300
    ) -> Dict[str, Any]:
        """
        Executa query no Athena
        
        Args:
            query: Query SQL para executar
            wait_for_completion: Se deve aguardar conclus√£o
            max_wait_time: Tempo m√°ximo de espera em segundos
        
        Returns:
            Dict com informa√ß√µes da execu√ß√£o
        """
        try:
            # Configura par√¢metros da query
            query_params = {
                'QueryString': query,
                'WorkGroup': self.workgroup
            }
            
            # Inicia execu√ß√£o
            response = self.athena_client.start_query_execution(**query_params)
            execution_id = response['QueryExecutionId']
            
            logger.info(
                f"Query Athena iniciada: {execution_id}",
                extra={
                    'execution_id': execution_id,
                    'workgroup': self.workgroup,
                    'action': 'athena_query_started'
                }
            )
            
            if not wait_for_completion:
                return {
                    'execution_id': execution_id,
                    'status': 'RUNNING',
                    'query': query
                }
            
            # Aguarda conclus√£o
            start_time = time.time()
            while time.time() - start_time < max_wait_time:
                response = self.athena_client.get_query_execution(QueryExecutionId=execution_id)
                status = response['QueryExecution']['Status']['State']
                
                if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                    execution_time = time.time() - start_time
                    
                    if status == 'SUCCEEDED':
                        logger.info(
                            f"‚úÖ Query Athena conclu√≠da com sucesso em {execution_time:.1f}s",
                            extra={
                                'execution_id': execution_id,
                                'execution_time': round(execution_time, 2),
                                'action': 'athena_query_succeeded'
                            }
                        )
                    else:
                        error_reason = response['QueryExecution']['Status'].get('StateChangeReason', 'Erro desconhecido')
                        logger.error(
                            f"‚ùå Query Athena falhou: {error_reason}",
                            extra={
                                'execution_id': execution_id,
                                'status': status,
                                'error_reason': error_reason,
                                'action': 'athena_query_failed'
                            }
                        )
                    
                    return {
                        'execution_id': execution_id,
                        'status': status,
                        'execution_time': execution_time,
                        'query': query,
                        'response': response
                    }
                
                time.sleep(2)
            
            # Timeout
            logger.warning(
                f"‚è∞ Timeout na query Athena ap√≥s {max_wait_time}s",
                extra={
                    'execution_id': execution_id,
                    'max_wait_time': max_wait_time,
                    'action': 'athena_query_timeout'
                }
            )
            
            return {
                'execution_id': execution_id,
                'status': 'TIMEOUT',
                'execution_time': max_wait_time,
                'query': query
            }
            
        except Exception as e:
            logger.error(
                f"Erro na execu√ß√£o da query Athena: {e}",
                extra={
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'query': query[:200] + '...' if len(query) > 200 else query,
                    'action': 'athena_query_error'
                }
            )
            raise

    def _table_exists(self, table_name: str) -> bool:
        """Verifica se a tabela existe no Glue Catalog"""
        try:
            self.glue_client.get_table(
                DatabaseName=self.database,
                Name=table_name
            )
            return True
        except ClientError as e:
            if e.response['Error']['Code'] == 'EntityNotFoundException':
                return False
            raise

    def _get_table_schema(self, table_name: str) -> Dict[str, str]:
        """Obt√©m schema da tabela existente"""
        try:
            response = self.glue_client.get_table(
                DatabaseName=self.database,
                Name=table_name
            )
            
            schema = {}
            for column in response['Table']['StorageDescriptor']['Columns']:
                schema[column['Name']] = column['Type']
                
            return schema
        except Exception as e:
            logger.error(
                f"Erro ao obter schema da tabela {table_name}: {e}",
                extra={'table_name': table_name, 'error': str(e)}
            )
            raise

    def create_table_from_dataframe(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_location: str,
        file_format: str = 'PARQUET',
        overwrite: bool = False
    ) -> Dict[str, Any]:
        """
        Cria nova tabela a partir de DataFrame
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela a ser criada
            s3_location: URI S3 completa onde os dados da tabela ficar√£o armazenados
            file_format: Formato do arquivo (PARQUET, JSON, CSV)
            overwrite: Se True, sobrescreve tabela existente
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_location = self._validate_s3_uri(s3_location)
        
        logger.info(
            f"üèóÔ∏è Criando tabela {table_name} a partir de DataFrame",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'rows_count': len(df),
                'columns_count': len(df.columns),
                'file_format': file_format,
                'overwrite': overwrite,
                's3_location': s3_location,
                'action': 'create_table_start'
            }
        )
        
        try:
            # Verifica se tabela existe
            if self._table_exists(table_name):
                if not overwrite:
                    raise ValueError(f"Tabela {table_name} j√° existe. Use overwrite=True para sobrescrever.")
                else:
                    logger.info(f"Tabela {table_name} ser√° sobrescrita")
            
            # Gera schema baseado no DataFrame
            columns_ddl = []
            for col_name, dtype in df.dtypes.items():
                if pd.api.types.is_integer_dtype(dtype):
                    sql_type = 'bigint'
                elif pd.api.types.is_float_dtype(dtype):
                    sql_type = 'double'
                elif pd.api.types.is_bool_dtype(dtype):
                    sql_type = 'boolean'
                else:
                    sql_type = 'string'
                
                columns_ddl.append(f"`{col_name}` {sql_type}")
            
            # Monta DDL
            ddl = f"""
            CREATE TABLE IF NOT EXISTS `{self.database}`.`{table_name}` (
                {', '.join(columns_ddl)}
            )
            STORED AS {file_format}
            LOCATION '{s3_location}'
            """
            
            if overwrite:
                # Drop table se existe
                drop_ddl = f"DROP TABLE IF EXISTS `{self.database}`.`{table_name}`"
                self._execute_athena_query(drop_ddl)
            
            # Cria tabela
            result = self._execute_athena_query(ddl)
            
            logger.info(
                f"‚úÖ Tabela {table_name} criada com sucesso",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'execution_id': result.get('execution_id'),
                    's3_location': s3_location,
                    'action': 'create_table_complete'
                }
            )
            
            return {
                'table_name': table_name,
                'database': self.database,
                's3_location': s3_location,
                'columns_count': len(df.columns),
                'athena_execution': result,
                'operation_id': operation_id
            }
            
        except Exception as e:
            logger.error(
                f"Erro ao criar tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'create_table_error'
                }
            )
            raise

    def append_to_table(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_staging_location: str
    ) -> Dict[str, Any]:
        """
        Adiciona dados do DataFrame a uma tabela existente
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela existente
            s3_staging_location: URI S3 completa para staging dos dados
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_staging_location = self._validate_s3_uri(s3_staging_location)
        
        logger.info(
            f"üìù Adicionando dados √† tabela {table_name}",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'rows_count': len(df),
                's3_staging_location': s3_staging_location,
                'action': 'append_to_table_start'
            }
        )
        
        try:
            # Verifica se tabela existe
            if not self._table_exists(table_name):
                raise ValueError(f"Tabela {table_name} n√£o existe. Crie a tabela primeiro.")
            
            # TODO: Implementar upload do DataFrame para S3 e INSERT INTO
            # Por enquanto, retorna estrutura b√°sica
            
            logger.info(
                f"‚úÖ Dados adicionados √† tabela {table_name}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'rows_added': len(df),
                    'action': 'append_to_table_complete'
                }
            )
            
            return {
                'table_name': table_name,
                'database': self.database,
                'rows_added': len(df),
                'staging_location': s3_staging_location,
                'operation_id': operation_id
            }
            
        except Exception as e:
            logger.error(
                f"Erro ao adicionar dados √† tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'append_to_table_error'
                }
            )
            raise

    def merge_overwrite_by_custom_id(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_staging_location: str,
        custom_id_column: str = 'custom_id'
    ) -> Dict[str, Any]:
        """
        Faz merge/overwrite na tabela baseado no custom_id
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela existente
            s3_staging_location: URI S3 completa para staging dos dados
            custom_id_column: Nome da coluna para fazer o merge
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_staging_location = self._validate_s3_uri(s3_staging_location)
        
        logger.info(
            f"üîÑ Fazendo merge overwrite na tabela {table_name} por {custom_id_column}",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'merge_column': custom_id_column,
                'rows_count': len(df),
                's3_staging_location': s3_staging_location,
                'action': 'merge_overwrite_start'
            }
        )
        
        try:
            # Verifica se tabela existe
            if not self._table_exists(table_name):
                raise ValueError(f"Tabela {table_name} n√£o existe. Crie a tabela primeiro.")
            
            # Verifica se coluna de merge existe no DataFrame
            if custom_id_column not in df.columns:
                raise ValueError(f"Coluna {custom_id_column} n√£o encontrada no DataFrame")
            
            # TODO: Implementar l√≥gica de merge com ICEBERG ou estrat√©gia de DELETE + INSERT
            
            logger.info(
                f"‚úÖ Merge overwrite conclu√≠do na tabela {table_name}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'rows_processed': len(df),
                    'action': 'merge_overwrite_complete'
                }
            )
            
            return {
                'table_name': table_name,
                'database': self.database,
                'merge_column': custom_id_column,
                'rows_processed': len(df),
                'staging_location': s3_staging_location,
                'operation_id': operation_id
            }
            
        except Exception as e:
            logger.error(
                f"Erro no merge overwrite da tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'merge_overwrite_error'
                }
            )
            raise

    def upsert_table(
        self, 
        df: pd.DataFrame, 
        table_name: str,
        s3_location: str,
        custom_id_column: str = 'custom_id'
    ) -> Dict[str, Any]:
        """
        Faz upsert: merge se tabela existe, sen√£o cria nova tabela
        
        Args:
            df: DataFrame com os dados
            table_name: Nome da tabela
            s3_location: URI S3 completa para os dados da tabela
            custom_id_column: Nome da coluna para fazer o merge (se tabela existir)
        """
        operation_id = self._generate_operation_id()
        table_name = self._validate_table_name(table_name)
        s3_location = self._validate_s3_uri(s3_location)
        
        logger.info(
            f"üîÄ Fazendo upsert na tabela {table_name}",
            extra={
                'operation_id': operation_id,
                'table_name': table_name,
                'rows_count': len(df),
                's3_location': s3_location,
                'action': 'upsert_table_start'
            }
        )
        
        try:
            if self._table_exists(table_name):
                # Tabela existe - faz merge
                logger.info(f"Tabela {table_name} existe, fazendo merge")
                return self.merge_overwrite_by_custom_id(
                    df, table_name, s3_location, custom_id_column
                )
            else:
                # Tabela n√£o existe - cria nova
                logger.info(f"Tabela {table_name} n√£o existe, criando nova")
                return self.create_table_from_dataframe(df, table_name, s3_location)
                
        except Exception as e:
            logger.error(
                f"Erro no upsert da tabela {table_name}: {e}",
                extra={
                    'operation_id': operation_id,
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'upsert_table_error'
                }
            )
            raise

    def get_table_info(self, table_name: str) -> Dict[str, Any]:
        """
        Obt√©m informa√ß√µes detalhadas sobre uma tabela
        
        Args:
            table_name: Nome da tabela
        """
        table_name = self._validate_table_name(table_name)
        
        try:
            if not self._table_exists(table_name):
                return {
                    'exists': False,
                    'table_name': table_name,
                    'database': self.database
                }
            
            # Obt√©m metadados da tabela
            response = self.glue_client.get_table(
                DatabaseName=self.database,
                Name=table_name
            )
            
            table_info = response['Table']
            
            # Conta registros (query simples)
            count_query = f"SELECT COUNT(*) as row_count FROM `{self.database}`.`{table_name}`"
            count_result = self._execute_athena_query(count_query)
            
            return {
                'exists': True,
                'table_name': table_name,
                'database': self.database,
                'location': table_info['StorageDescriptor'].get('Location'),
                'input_format': table_info['StorageDescriptor'].get('InputFormat'),
                'output_format': table_info['StorageDescriptor'].get('OutputFormat'),
                'columns': [
                    {
                        'name': col['Name'],
                        'type': col['Type'],
                        'comment': col.get('Comment', '')
                    }
                    for col in table_info['StorageDescriptor']['Columns']
                ],
                'columns_count': len(table_info['StorageDescriptor']['Columns']),
                'created_time': table_info.get('CreateTime'),
                'updated_time': table_info.get('UpdateTime'),
                'count_query_execution': count_result
            }
            
        except Exception as e:
            logger.error(
                f"Erro ao obter informa√ß√µes da tabela {table_name}: {e}",
                extra={
                    'table_name': table_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'get_table_info_error'
                }
            )
            raise

    def show_table_info(self, table_name: str):
        """Exibe informa√ß√µes da tabela de forma formatada"""
        info = self.get_table_info(table_name)
        
        print("=" * 60)
        print(f"üìä INFORMA√á√ïES DA TABELA: {table_name}")
        print("=" * 60)
        
        if not info['exists']:
            print("‚ùå Tabela n√£o existe")
            return
        
        print(f"‚úÖ Tabela existe")
        print(f"üóÑÔ∏è Database: {info['database']}")
        print(f"üìç Localiza√ß√£o: {info['location']}")
        print(f"üìÅ Formato: {info['input_format']}")
        print(f"üóìÔ∏è Criada em: {info['created_time']}")
        print(f"üîÑ Atualizada em: {info['updated_time']}")
        print(f"üìä Colunas ({info['columns_count']}):")
        
        for col in info['columns']:
            comment = f" - {col['comment']}" if col['comment'] else ""
            print(f"   ‚Ä¢ {col['name']} ({col['type']}){comment}")
        
        print("=" * 60)

    def list_tables(self, pattern: Optional[str] = None) -> List[str]:
        """Lista tabelas no database"""
        try:
            response = self.glue_client.get_tables(DatabaseName=self.database)
            tables = [table['Name'] for table in response['TableList']]
            
            if pattern:
                import fnmatch
                tables = [t for t in tables if fnmatch.fnmatch(t, pattern)]
            
            return sorted(tables)
            
        except Exception as e:
            logger.error(
                f"Erro ao listar tabelas: {e}",
                extra={
                    'database': self.database,
                    'pattern': pattern,
                    'error': str(e),
                    'action': 'list_tables_error'
                }
            )
            raise

    def show_tables(self, pattern: Optional[str] = None):
        """Exibe lista de tabelas de forma formatada"""
        tables = self.list_tables(pattern)
        
        print("=" * 60)
        print(f"üìã TABELAS NO DATABASE: {self.database}")
        if pattern:
            print(f"üîç Filtro: {pattern}")
        print("=" * 60)
        
        if not tables:
            print("‚ùå Nenhuma tabela encontrada")
        else:
            for i, table in enumerate(tables, 1):
                print(f"{i:3d}. {table}")
        
        print(f"\nüìä Total: {len(tables)} tabelas")
        print("=" * 60)

    def execute_query(
        self, 
        query: str,
        wait_for_completion: bool = True,
        max_wait_time: int = 300
    ) -> Dict[str, Any]:
        """
        Executa uma query SQL customizada no Athena
        
        Args:
            query: Query SQL para executar
            wait_for_completion: Se deve aguardar conclus√£o
            max_wait_time: Tempo m√°ximo de espera em segundos
        """
        operation_id = self._generate_operation_id()
        
        logger.info(
            f"üîç Executando query customizada",
            extra={
                'operation_id': operation_id,
                'query_preview': query[:100] + '...' if len(query) > 100 else query,
                'action': 'execute_custom_query_start'
            }
        )
        
        try:
            result = self._execute_athena_query(query, wait_for_completion, max_wait_time)
            result['operation_id'] = operation_id
            
            logger.info(
                f"‚úÖ Query customizada executada",
                extra={
                    'operation_id': operation_id,
                    'execution_id': result.get('execution_id'),
                    'status': result.get('status'),
                    'action': 'execute_custom_query_complete'
                }
            )
            
            return result
            
        except Exception as e:
            logger.error(
                f"Erro na execu√ß√£o da query customizada: {e}",
                extra={
                    'operation_id': operation_id,
                    'error': str(e),
                    'error_type': type(e).__name__,
                    'action': 'execute_custom_query_error'
                }
            )
            raise
