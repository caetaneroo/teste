import json
import logging
import pandas as pd
import awswrangler as wr
import boto3
from typing import List, Dict, Any, Optional, Union
import re
import time

logger = logging.getLogger(__name__)

DEFAULT_DATABASE = 'workspace_db'
DEFAULT_WORKGROUP = 'analytics-workgroup-v3'
DEFAULT_REGION = 'sa-east-1'

class OutputManager:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        if config is None:
            config = {}
        self.database = config.get('database', DEFAULT_DATABASE)
        self.workgroup = config.get('workgroup', DEFAULT_WORKGROUP)
        self.region = config.get('region', DEFAULT_REGION)
        self.session = boto3.Session(region_name=self.region)

    def _validate_table_name(self, table_name: str) -> str:
        if not table_name or not isinstance(table_name, str):
            raise ValueError("Nome da tabela deve ser uma string nÃ£o vazia")
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', table_name.lower())
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"table_{sanitized}"
        return sanitized

    def _validate_column_name(self, column_name: str) -> str:
        if not column_name or not isinstance(column_name, str):
            raise ValueError("Nome da coluna deve ser uma string nÃ£o vazia")
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', column_name.lower())
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"col_{sanitized}"
        return sanitized

    def _validate_s3_path(self, s3_path: str) -> str:
        """Valida e normaliza o caminho S3"""
        if not s3_path or not isinstance(s3_path, str):
            raise ValueError("Caminho S3 deve ser uma string nÃ£o vazia")
        if not s3_path.startswith('s3://'):
            raise ValueError("Caminho S3 deve comeÃ§ar com 's3://'")
        return s3_path.rstrip('/') + '/'

    def _get_temp_path(self, s3_path: str) -> str:
        """Gera caminho temporÃ¡rio dentro do path da tabela"""
        base_path = s3_path.rstrip('/')
        return f"{base_path}/_temp_{int(time.time())}/"

    def process_ai_results(
        self, 
        ai_results: Union[Dict[str, Any], List[Dict[str, Any]]],
        id_column_name: str = 'id',
        include_content_text: bool = True
    ) -> pd.DataFrame:
        """
        Processa resultados do AIProcessor e retorna DataFrame.
        Apenas sucessos sÃ£o processados.
        """
        start_time = time.time()
        logger.info("ðŸ”„ Iniciando processamento dos resultados do AI Processor")
        
        id_column_name = self._validate_column_name(id_column_name)
        
        if isinstance(ai_results, dict):
            if 'results' in ai_results:
                results = ai_results['results']
            else:
                results = [ai_results]
        elif isinstance(ai_results, list):
            results = ai_results
        else:
            raise ValueError(f"Tipo de input nÃ£o suportado: {type(ai_results)}")

        processed = []
        success_count = 0
        result_columns = set()
        
        for r in results:
            if not r.get('success', False):
                continue
                
            success_count += 1
            row = {
                id_column_name: r.get('id'),
                'processing_time': r.get('processing_time'),
                'input_tokens': r.get('input_tokens'),
                'output_tokens': r.get('output_tokens'),
                'cached_tokens': r.get('cached_tokens'),
                'tokens_used': r.get('tokens_used'),
                'cost': r.get('cost')
            }
            
            content = r.get('content')
            if content is None:
                if include_content_text:
                    processed.append(row)
                continue
                
            if isinstance(content, dict):
                for k, v in content.items():
                    result_columns.add(k)
                    key = f'result_{k}'
                    if isinstance(v, (dict, list)):
                        row[key] = json.dumps(v, ensure_ascii=False)
                    else:
                        row[key] = v
                processed.append(row)
            elif isinstance(content, str):
                try:
                    parsed = json.loads(content)
                    if isinstance(parsed, dict):
                        for k, v in parsed.items():
                            result_columns.add(k)
                            key = f'result_{k}'
                            if isinstance(v, (dict, list)):
                                row[key] = json.dumps(v, ensure_ascii=False)
                            else:
                                row[key] = v
                        processed.append(row)
                    else:
                        if include_content_text:
                            row['content_text'] = content
                            processed.append(row)
                except json.JSONDecodeError:
                    if include_content_text:
                        row['content_text'] = content
                        processed.append(row)
            else:
                if include_content_text:
                    row['content_text'] = str(content)
                    processed.append(row)

        df = pd.DataFrame(processed)
        if id_column_name in df.columns:
            cols = [id_column_name] + [c for c in df.columns if c != id_column_name]
            df = df[cols]
        
        processing_time = time.time() - start_time
        result_cols_str = ', '.join(sorted(result_columns)) if result_columns else 'nenhuma'
        logger.info(f"âœ… AI Results processados | Sucessos: {success_count} | DataFrame: {len(df)} linhas | Colunas resultado: {result_cols_str} | Tempo: {processing_time:.2f}s")
        
        return df

    def create_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Cria tabela Iceberg usando funÃ§Ãµes nativas do awswrangler.
        """
        start_time = time.time()
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        temp_path = self._get_temp_path(s3_path)
        
        logger.info(f"ðŸ”„ Criando tabela Iceberg | Database: {database} | Tabela: {table_name} | Registros: {len(df)}")
        
        # Garante que o database existe
        wr.catalog.create_database(database, exist_ok=True, boto3_session=self.session)
        
        # Cria tabela Iceberg usando funÃ§Ã£o especÃ­fica
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            temp_path=temp_path,
            partition_cols=partition_cols,
            mode='overwrite',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… Tabela Iceberg criada | {database}.{table_name} | Registros: {len(df)} | S3: {s3_path} | Tempo: {processing_time:.2f}s")

    def append_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Adiciona dados em tabela Iceberg existente.
        """
        start_time = time.time()
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        temp_path = self._get_temp_path(s3_path)
        
        logger.info(f"ðŸ”„ Append em tabela Iceberg | Database: {database} | Tabela: {table_name} | Registros: {len(df)}")
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} nÃ£o existe. Use create_table primeiro.")
        
        # Append usando funÃ§Ã£o Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            temp_path=temp_path,
            partition_cols=partition_cols,
            mode='append',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… Append realizado | {database}.{table_name} | Registros adicionados: {len(df)} | Tempo: {processing_time:.2f}s")

    def upsert_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        merge_keys: Optional[List[str]] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Upsert usando merge nativo do Iceberg.
        """
        start_time = time.time()
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        temp_path = self._get_temp_path(s3_path)
        
        # Se a tabela nÃ£o existe, cria
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            logger.info(f"ðŸ”„ Tabela nÃ£o existe, criando | Database: {database} | Tabela: {table_name} | Registros: {len(df)}")
            self.create_table(df, table_name, s3_path, database, partition_cols)
            return
        
        logger.info(f"ðŸ”„ Upsert em tabela Iceberg | Database: {database} | Tabela: {table_name} | Registros: {len(df)}")
        
        # Define chaves de merge (usa 'id' como padrÃ£o se nÃ£o especificado)
        if merge_keys is None:
            merge_keys = ['id']
        
        # Verifica se as chaves de merge existem
        for key in merge_keys:
            if key not in df.columns:
                raise ValueError(f"Chave de merge '{key}' nÃ£o encontrada no DataFrame")
        
        # Upsert usando merge do Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            temp_path=temp_path,
            merge_cols=merge_keys,
            partition_cols=partition_cols,
            mode='upsert',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        processing_time = time.time() - start_time
        merge_keys_str = ', '.join(merge_keys)
        logger.info(f"âœ… Upsert realizado | {database}.{table_name} | Registros: {len(df)} | Chaves merge: {merge_keys_str} | Tempo: {processing_time:.2f}s")

    def merge_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        merge_keys: List[str],
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Merge usando capacidades nativas do Iceberg.
        """
        start_time = time.time()
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        temp_path = self._get_temp_path(s3_path)
        
        logger.info(f"ðŸ”„ Merge em tabela Iceberg | Database: {database} | Tabela: {table_name} | Registros: {len(df)}")
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} nÃ£o existe. Use create_table primeiro.")
        
        # Verifica se as chaves de merge existem
        for key in merge_keys:
            if key not in df.columns:
                raise ValueError(f"Chave de merge '{key}' nÃ£o encontrada no DataFrame")
        
        # Merge usando funÃ§Ã£o Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            temp_path=temp_path,
            merge_cols=merge_keys,
            partition_cols=partition_cols,
            mode='upsert',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        processing_time = time.time() - start_time
        merge_keys_str = ', '.join(merge_keys)
        logger.info(f"âœ… Merge realizado | {database}.{table_name} | Registros: {len(df)} | Chaves merge: {merge_keys_str} | Tempo: {processing_time:.2f}s")

    def overwrite_partitions(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> None:
        """
        Sobrescreve partiÃ§Ãµes especÃ­ficas da tabela Iceberg.
        """
        start_time = time.time()
        table_name = self._validate_table_name(table_name)
        s3_path = self._validate_s3_path(s3_path)
        database = database or self.database
        temp_path = self._get_temp_path(s3_path)
        
        logger.info(f"ðŸ”„ Overwrite partitions | Database: {database} | Tabela: {table_name} | Registros: {len(df)}")
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} nÃ£o existe. Use create_table primeiro.")
        
        # Sobrescreve partiÃ§Ãµes usando funÃ§Ã£o Iceberg
        wr.athena.to_iceberg(
            df=df,
            database=database,
            table=table_name,
            table_location=s3_path,
            temp_path=temp_path,
            partition_cols=partition_cols,
            mode='overwrite_partitions',
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… PartiÃ§Ãµes sobrescritas | {database}.{table_name} | Registros: {len(df)} | Tempo: {processing_time:.2f}s")

    def delete_table(
        self,
        table_name: str,
        database: Optional[str] = None,
        delete_s3_data: bool = False
    ) -> None:
        """
        Remove tabela Iceberg do catÃ¡logo e opcionalmente os dados do S3.
        """
        start_time = time.time()
        table_name = self._validate_table_name(table_name)
        database = database or self.database
        
        logger.info(f"ðŸ”„ Removendo tabela | Database: {database} | Tabela: {table_name} | Limpar S3: {delete_s3_data}")
        
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            logger.warning(f"âš ï¸ Tabela {database}.{table_name} nÃ£o existe")
            return
        
        # Remove a tabela do catÃ¡logo
        wr.catalog.delete_table_if_exists(
            database=database,
            table=table_name,
            boto3_session=self.session
        )
        
        processing_time = time.time() - start_time
        
        # Opcionalmente remove os dados do S3
        if delete_s3_data:
            try:
                table_location = wr.catalog.get_table_location(
                    database=database,
                    table=table_name,
                    boto3_session=self.session
                )
                wr.s3.delete_objects(path=table_location, boto3_session=self.session)
                logger.info(f"âœ… Tabela e dados S3 removidos | {database}.{table_name} | Tempo: {processing_time:.2f}s")
            except Exception:
                logger.info(f"âœ… Tabela removida do catÃ¡logo | {database}.{table_name} | Tempo: {processing_time:.2f}s")
        else:
            logger.info(f"âœ… Tabela removida do catÃ¡logo | {database}.{table_name} | Tempo: {processing_time:.2f}s")

    def read_table(
        self,
        table_name: str,
        database: Optional[str] = None,
        columns: Optional[List[str]] = None,
        filters: Optional[List[List[str]]] = None
    ) -> pd.DataFrame:
        """
        LÃª dados de uma tabela Iceberg.
        """
        start_time = time.time()
        table_name = self._validate_table_name(table_name)
        database = database or self.database
        
        logger.info(f"ðŸ”„ Lendo tabela Iceberg | Database: {database} | Tabela: {table_name}")
        
        # Verifica se a tabela existe
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            raise ValueError(f"Tabela {database}.{table_name} nÃ£o existe")
        
        # LÃª a tabela usando funÃ§Ã£o Iceberg
        df = wr.athena.read_sql_table(
            table=table_name,
            database=database,
            columns=columns,
            filters=filters,
            boto3_session=self.session,
            workgroup=self.workgroup
        )
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… Tabela lida | {database}.{table_name} | Registros: {len(df)} | Tempo: {processing_time:.2f}s")
        return df
