import time
import json
import logging
import pandas as pd
import awswrangler as wr
import boto3
from typing import List, Dict, Any, Optional, Union
import re

logger = logging.getLogger(__name__)

DEFAULT_DATABASE = 'workspace_db'
DEFAULT_WORKGROUP = 'analytics-workgroup-v3'
DEFAULT_REGION = 'sa-east-1'

class OutputManager:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        if config is None:
            config = {}
        self.database = config.get('database', DEFAULT_DATABASE)
        self.workgroup = config.get('workgroup', DEFAULT_WORKGROUP)
        self.region = config.get('region', DEFAULT_REGION)
        self.session = boto3.Session(region_name=self.region)

    def _validate_table_name(self, table_name: str) -> str:
        if not table_name or not isinstance(table_name, str):
            raise ValueError("Nome da tabela deve ser uma string não vazia")
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', table_name.lower())
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"table_{sanitized}"
        return sanitized

    def _validate_column_name(self, column_name: str) -> str:
        if not column_name or not isinstance(column_name, str):
            raise ValueError("Nome da coluna deve ser uma string não vazia")
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', column_name.lower())
        if not re.match(r'^[a-zA-Z_]', sanitized):
            sanitized = f"col_{sanitized}"
        return sanitized

    def process_ai_results(
        self, 
        ai_results: Union[Dict[str, Any], List[Dict[str, Any]]],
        id_column_name: str = 'id',
        include_content_text: bool = True
    ) -> pd.DataFrame:
        """
        Processa resultados do AIProcessor e retorna DataFrame.
        Apenas sucessos. Colunas padrão: id, processing_time, input_tokens, output_tokens, cached_tokens, tokens_used, cost.
        Chaves do JSON de resposta da IA são prefixadas com result_.
        """
        id_column_name = self._validate_column_name(id_column_name)
        if isinstance(ai_results, dict):
            if 'results' in ai_results:
                results = ai_results['results']
            else:
                results = [ai_results]
        elif isinstance(ai_results, list):
            results = ai_results
        else:
            raise ValueError(f"Tipo de input não suportado: {type(ai_results)}")

        processed = []
        for r in results:
            if not r.get('success', False):
                continue
            row = {
                id_column_name: r.get('id'),
                'processing_time': r.get('processing_time'),
                'input_tokens': r.get('input_tokens'),
                'output_tokens': r.get('output_tokens'),
                'cached_tokens': r.get('cached_tokens'),
                'tokens_used': r.get('tokens_used'),
                'cost': r.get('cost')
            }
            content = r.get('content')
            if content is None:
                if include_content_text:
                    processed.append(row)
                continue
            if isinstance(content, dict):
                for k, v in content.items():
                    key = f'result_{k}'
                    if isinstance(v, (dict, list)):
                        row[key] = json.dumps(v, ensure_ascii=False)
                    else:
                        row[key] = v
                processed.append(row)
            elif isinstance(content, str):
                try:
                    parsed = json.loads(content)
                    if isinstance(parsed, dict):
                        for k, v in parsed.items():
                            key = f'result_{k}'
                            if isinstance(v, (dict, list)):
                                row[key] = json.dumps(v, ensure_ascii=False)
                            else:
                                row[key] = v
                        processed.append(row)
                    else:
                        if include_content_text:
                            row['content_text'] = content
                            processed.append(row)
                except json.JSONDecodeError:
                    if include_content_text:
                        row['content_text'] = content
                        processed.append(row)
            else:
                if include_content_text:
                    row['content_text'] = str(content)
                    processed.append(row)

        df = pd.DataFrame(processed)
        if id_column_name in df.columns:
            cols = [id_column_name] + [c for c in df.columns if c != id_column_name]
            df = df[cols]
        return df

    def create_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Cria tabela Iceberg sobrescrevendo sempre.
        """
        table_name = self._validate_table_name(table_name)
        s3_path = s3_path if s3_path.endswith('/') else s3_path + '/'
        database = database or self.database
        wr.catalog.create_database(database, exist_ok=True, boto3_session=self.session)
        wr.s3.to_parquet(
            df=df,
            path=s3_path,
            dataset=True,
            database=database,
            table=table_name,
            mode='overwrite',
            partition_cols=partition_cols,
            table_type='ICEBERG',
            boto3_session=self.session
        )
        logger.info(f"✅ Tabela {table_name} criada/atualizada em {s3_path} (Iceberg, overwrite)")
        return {'database': database, 'table': table_name, 's3_path': s3_path, 'mode': 'overwrite'}

    def append_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        partition_cols: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Adiciona dados em tabela Iceberg existente.
        """
        table_name = self._validate_table_name(table_name)
        s3_path = s3_path if s3_path.endswith('/') else s3_path + '/'
        database = database or self.database
        wr.s3.to_parquet(
            df=df,
            path=s3_path,
            dataset=True,
            database=database,
            table=table_name,
            mode='append',
            partition_cols=partition_cols,
            table_type='ICEBERG',
            boto3_session=self.session
        )
        logger.info(f"✅ Append realizado em {table_name}")
        return {'database': database, 'table': table_name, 's3_path': s3_path, 'mode': 'append'}

    def upsert_table(
        self,
        df: pd.DataFrame,
        table_name: str,
        s3_path: str,
        database: Optional[str] = None,
        id_column: str = 'id',
        partition_cols: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Upsert real: cria se não existe, senão faz overwrite_partitions (merge pelo particionamento).
        """
        table_name = self._validate_table_name(table_name)
        s3_path = s3_path if s3_path.endswith('/') else s3_path + '/'
        database = database or self.database
        if not wr.catalog.does_table_exist(database=database, table=table_name, boto3_session=self.session):
            return self.create_table(df, table_name, s3_path, database, partition_cols)
        if id_column not in df.columns:
            raise ValueError(f"id_column '{id_column}' not found in dataframe")
        wr.s3.to_parquet(
            df=df,
            path=s3_path,
            dataset=True,
            database=database,
            table=table_name,
            mode='overwrite_partitions',
            partition_cols=partition_cols or [id_column],
            table_type='ICEBERG',
            boto3_session=self.session
        )
        logger.info(f"✅ Upsert realizado em {table_name}")
        return {'database': database, 'table': table_name, 's3_path': s3_path, 'mode': 'upsert'}
